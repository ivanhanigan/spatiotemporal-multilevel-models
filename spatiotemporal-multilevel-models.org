#+TITLE: Spatiotemporal multilevel modelling
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----
* to add a video
<iframe style="border: none;" height="400" width="600" src="./GMT20181019-002609_CAR-webinar_1920x1200.mp4"></iframe>


* COMMENT showme DEPRECATED
#+name:showme
#+begin_src R :session *R* :tangle no :exports none :eval no
#### name:showme ####
setwd("~/Dropbox/projects/spatiotemporal-multilevel-models/")
browseURL("index.html")
#+end_src

#+RESULTS: showme
: 0

* Version Statement
#+begin_src R :session *R* :exports none
  commit_msg <- "Add ML modelling"
  commit_msg <- as.data.frame(c(as.character(Sys.Date()), commit_msg))
  commit_msg
#+end_src

#+RESULTS:
|       2016-05-07 |
| Add ML modelling |





* COMMENT Notes for blog
** Update on reflections from Bob Haining's Lecture
[[http://ivanhanigan.github.io/2013/04/reflections-bob-haining/][Earlier this year]] Prof Bob Haining from the Geography Department Cambridge visited and gave us a great lecture on spatial regression.

This Tuesday at the [[http://gis-forum.github.io][GIS Forum]] we were lucky to be joined by statistician Phil Kokic from CSIRO who had heard we'd be discussing spatial autocorrelation (Phil is my PhD supervisor). Here are some quick notes I made:

*** CART Tree analysis that addresses the (potential)spatial autocorrelation problem
We started off the discussion with an assessment of the approach described in this post [[http://thebiobucket.blogspot.com.au/2012/03/classification-trees-allowing-for.html][Classification Trees and Spatial Autocorrelation]].

I've been thinking more and more about decision trees/CART/random forest methods for selecting a subset of relevant variables (and interations) for use in GLM or GAM model construction.  In a perfect world I'd have data on the main predictor I wanted to model and enough data about all the relevant other predictors (especially confounding or modifying variables) to ensure I get a 'well behaved model'. But with all the data around and so many potentially plausible relationships one might choose to include we need a way to narrow down these to just include the most important covariates, confounders and interactions.  CART or some variation on it seems a good way to do this, but is prone to the potential problem of spatially correlated errors too.

The idea from that blog post is:

"compute the classification tree, calculate residuals and use it for a Mantel-test and Mantel correlograms.
The Mantel correlograms test differrences in dissimilarities of
the residuals across several spatial distances and thus enable you to detect lag-distances where possible spatial autocorrelation vanishes.
...If encounter autocorrelation... try to use subsamples of the data avoiding resampling within the lag-distance.."

I think the workflow would be to

- fit the classification tree (Question: best to use all the data or with a sample like using cross-validation)
- get the residuals and visually assess the lagged distances plot provided by the Mantel correlogram.  Decide on a threshold (Question: is there an objective way to do this?).
- Sample from the data and select out from this sample only data from pairs with distances greater than the threshold (have to keep one out of each close pair or else we'd only be getting data from the sparsely sampled parts of our study region).

We all agreed this sounded OK, but only avoids the problem of spatial autocorrelation (and loses data).

*** Modeling with control for spatial autocorrelation
So we all agreed we'd prefer if our model can control for spatial autocorrelation.  I confessed that I'd always found the GeoBUGS tutorial and other tutorials about Bayesian methods for this very difficult and would really like a "Simple" way to make the problem go away.  So first we briefly reviewed Prof Hainings 3 equations again:

NOTE: THE FOLLOWING IDEAS WORK BEST FOR AREAL DATA.

*** The Spatial Error Model

$Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \eta_{i}$

Where:

$\eta_{i}$ = Spatially autocorrelated errors.


*** The Spatial Lag Model

$Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \rho(Neighbours Y_{ij}) + e_{i}$

Where:

$\rho_(Neighbours Y_{ij})$ = is an additional explanatory variable which is the value of the dependent variable in neighbouring areas. 

*** Spatially Lagged Independent Variable(s)

$Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \beta_{2L} X_{2ij} + e_{i}$

Where:

$\beta_{2L} X_{2ij}$ = is the independent variable X2 that is spatially lagged.


*** Discussion
- Phil agreed with Bob that the spatial error model is the best, spatial lag model is OK and spatially lagged covariates not so great.
- For spatial error model fitting Phil suggested looking at R packages spBayes and spTimer
- I pointed out that I am mostly interested in "spatially structured time-series models" rather than spatial models at a single point in time.  By this I mean that we have several neighbouring areal units observed over a period of time.  In this framework the general methods of time series modelling are used to control for temporal autocorrelation.  However this makes the methods of spatial error and spatial lag models tricky because the spatial autocorrelation needs to be assessed at many points in time.
- I asked that if spatial lag is OK (and it seems easier to fit into the time-series model framework) how can I check to know if it has done the trick?  If this were purely a spatial model we could check for spatial autocorrelation in the residuals just as they described in the CART blog above, but here we have many maps we could make (one every time point), and our spatial autocorrelation measure would surely vary a lot over time.  SO would a simple way just be to asses the effect on the Standard Error on beta1 (our primary interest) and if it is bigger but still significant we can be reassured that our result isn't affected? Or perhaps we should assess the beta on the lagged variable, for instance is a significant p-value on the lagged Beta an indication that it is capturing the unmeasured spatial associations represented by the neighbourhood variable?  
- If it hadn't done the trick Nerida pointed out this might be because the Neighbourhoods are actually not appropriately represented by the first order neighbours and therefore more neighbours could be included, like moving out several concentric circles to wider and wider neighbourhoods
- Nasser and Phil pointed out that the lagged variable (the outcome in the neighbours) includes an element of the exposure variables, and said that it would be difficult to 'unpack' what that part of the model meant.
- so it looks like there is no simple answer and spatial error model is still preferred.

** md
#+name:reflections-bob-haining-update-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-09-25-reflections-bob-haining-update.md :exports none :eval no :padline no
  ---
  name: reflections-bob-haining-update
  layout: post
  title: reflections-bob-haining-update
  date: 2013-09-25
  categories:
  - spatial dependence
  ---
  
  <!-- <?xml version="1.0" encoding="utf-8"?> -->
  <!-- <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" -->
  <!--                "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"> -->
  <!-- <html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"> -->
  <head>
  <!-- <title>spatiotemporal </title> -->
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
  <meta name="title" content="spatiotemporal "/>
  <meta name="generator" content="Org-mode"/>
  <meta name="generated" content="2013-09-25T14:46+1000"/>
  <meta name="author" content="Ivan Hanigan"/>
  <meta name="description" content=""/>
  <meta name="keywords" content=""/>
  <style type="text/css">
   <!--/*--><![CDATA[/*><!--*/
    html { font-family: Times, serif; font-size: 12pt; }
    .title  { text-align: center; }
    .todo   { color: red; }
    .done   { color: green; }
    .tag    { background-color: #add8e6; font-weight:normal }
    .target { }
    .timestamp { color: #bebebe; }
    .timestamp-kwd { color: #5f9ea0; }
    .right  {margin-left:auto; margin-right:0px;  text-align:right;}
    .left   {margin-left:0px;  margin-right:auto; text-align:left;}
    .center {margin-left:auto; margin-right:auto; text-align:center;}
    p.verse { margin-left: 3% }
    pre {
      border: 1pt solid #AEBDCC;
      background-color: #F3F5F7;
      padding: 5pt;
      font-family: courier, monospace;
          font-size: 90%;
          overflow:auto;
    }
    table { border-collapse: collapse; }
    td, th { vertical-align: top;  }
    th.right  { text-align:center;  }
    th.left   { text-align:center;   }
    th.center { text-align:center; }
    td.right  { text-align:right;  }
    td.left   { text-align:left;   }
    td.center { text-align:center; }
    dt { font-weight: bold; }
    div.figure { padding: 0.5em; }
    div.figure p { text-align: center; }
    div.inlinetask {
      padding:10px;
      border:2px solid gray;
      margin:10px;
      background: #ffffcc;
    }
    textarea { overflow-x: auto; }
    .linenr { font-size:smaller }
    .code-highlighted {background-color:#ffff00;}
    .org-info-js_info-navigation { border-style:none; }
    #org-info-js_console-label { font-size:10px; font-weight:bold;
                                 white-space:nowrap; }
    .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                   font-weight:bold; }
    /*]]>*/-->
  </style>
  <script type="text/javascript">
  /*
  @licstart  The following is the entire license notice for the
  JavaScript code in this tag.
  
  Copyright (C) 2012-2013 Free Software Foundation, Inc.
  
  The JavaScript code in this tag is free software: you can
  redistribute it and/or modify it under the terms of the GNU
  General Public License (GNU GPL) as published by the Free Software
  Foundation, either version 3 of the License, or (at your option)
  any later version.  The code is distributed WITHOUT ANY WARRANTY;
  without even the implied warranty of MERCHANTABILITY or FITNESS
  FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.
  
  As additional permission under GNU GPL version 3 section 7, you
  may distribute non-source (e.g., minimized or compacted) forms of
  that code without the copy of the GNU GPL normally required by
  section 4, provided you include this license notice and a URL
  through which recipients can access the Corresponding Source.
  
  
  @licend  The above is the entire license notice
  for the JavaScript code in this tag.
  ,*/
  <!--/*--><![CDATA[/*><!--*/
   function CodeHighlightOn(elem, id)
   {
     var target = document.getElementById(id);
     if(null != target) {
       elem.cacheClassElem = elem.className;
       elem.cacheClassTarget = target.className;
       target.className = "code-highlighted";
       elem.className   = "code-highlighted";
     }
   }
   function CodeHighlightOff(elem, id)
   {
     var target = document.getElementById(id);
     if(elem.cacheClassElem)
       elem.className = elem.cacheClassElem;
     if(elem.cacheClassTarget)
       target.className = elem.cacheClassTarget;
   }
  /*]]>*///-->
  </script>
  <script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js">
  /**
   ,*
   ,* @source: http://orgmode.org/mathjax/MathJax.js
   ,*
   ,* @licstart  The following is the entire license notice for the
   ,*  JavaScript code in http://orgmode.org/mathjax/MathJax.js.
   ,*
   ,* Copyright (C) 2012-2013  MathJax
   ,*
   ,* Licensed under the Apache License, Version 2.0 (the "License");
   ,* you may not use this file except in compliance with the License.
   ,* You may obtain a copy of the License at
   ,*
   ,*     http://www.apache.org/licenses/LICENSE-2.0
   ,*
   ,* Unless required by applicable law or agreed to in writing, software
   ,* distributed under the License is distributed on an "AS IS" BASIS,
   ,* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   ,* See the License for the specific language governing permissions and
   ,* limitations under the License.
   ,*
   ,* @licend  The above is the entire license notice
   ,* for the JavaScript code in http://orgmode.org/mathjax/MathJax.js.
   ,*
   ,*/
  
  /*
  @licstart  The following is the entire license notice for the
  JavaScript code below.
  
  Copyright (C) 2012-2013 Free Software Foundation, Inc.
  
  The JavaScript code below is free software: you can
  redistribute it and/or modify it under the terms of the GNU
  General Public License (GNU GPL) as published by the Free Software
  Foundation, either version 3 of the License, or (at your option)
  any later version.  The code is distributed WITHOUT ANY WARRANTY;
  without even the implied warranty of MERCHANTABILITY or FITNESS
  FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.
  
  As additional permission under GNU GPL version 3 section 7, you
  may distribute non-source (e.g., minimized or compacted) forms of
  that code without the copy of the GNU GPL normally required by
  section 4, provided you include this license notice and a URL
  through which recipients can access the Corresponding Source.
  
  
  @licend  The above is the entire license notice
  for the JavaScript code below.
  ,*/
  <!--/*--><![CDATA[/*><!--*/
      MathJax.Hub.Config({
          // Only one of the two following lines, depending on user settings
          // First allows browser-native MathML display, second forces HTML/CSS
          //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
              jax: ["input/TeX", "output/HTML-CSS"],
          extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                       "TeX/noUndefined.js"],
          tex2jax: {
              inlineMath: [ ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
              skipTags: ["script","noscript","style","textarea","pre","code"],
              ignoreClass: "tex2jax_ignore",
              processEscapes: false,
              processEnvironments: true,
              preview: "TeX"
          },
          showProcessingMessages: true,
          displayAlign: "center",
          displayIndent: "2em",
  
          "HTML-CSS": {
               scale: 100,
               availableFonts: ["STIX","TeX"],
               preferredFont: "TeX",
               webFont: "TeX",
               imageFont: "TeX",
               showMathMenu: true,
          },
          MMLorHTML: {
               prefer: {
                   MSIE:    "MML",
                   Firefox: "MML",
                   Opera:   "HTML",
                   other:   "HTML"
               }
          }
      });
  /*]]>*///-->
  </script>
  </head>
  <body>
  
  <div id="preamble">
  
  </div>
  
  <div id="content">
  <!-- <h1 class="title">spatiotemporal </h1> -->
  
  
  <div id="table-of-contents">
  <h2>Table of Contents</h2>
  <div id="text-table-of-contents">
  <ul>
  <li><a href="#sec-1">1 Update on reflections from Bob Haining's Lecture</a>
  <ul>
  <li><a href="#sec-1-1">1.1 CART Tree analysis that addresses the (potential)spatial autocorrelation problem</a></li>
  <li><a href="#sec-1-2">1.2 Modeling with control for spatial autocorrelation</a></li>
  <li><a href="#sec-1-3">1.3 The Spatial Error Model</a></li>
  <li><a href="#sec-1-4">1.4 The Spatial Lag Model</a></li>
  <li><a href="#sec-1-5">1.5 Spatially Lagged Independent Variable(s)</a></li>
  <li><a href="#sec-1-6">1.6 Discussion</a></li>
  </ul>
  </li>
  </ul>
  </div>
  </div>
  
  <div id="outline-container-1" class="outline-3">
  <h3 id="sec-1"><span class="section-number-3">1</span> Update on reflections from Bob Haining's Lecture</h3>
  <div class="outline-text-3" id="text-1">
  
  <p><a href="http://ivanhanigan.github.io/2013/04/reflections-bob-haining/">Earlier this year</a> Prof Bob Haining from the Geography Department Cambridge visited and gave us a great lecture on spatial regression.
  </p>
  <p>
  This Tuesday at the <a href="http://gis-forum.github.io">GIS Forum</a> we were lucky to be joined by statistician Phil Kokic from CSIRO who had heard we'd be discussing spatial autocorrelation (Phil is my PhD supervisor). Here are some quick notes I made:
  </p>
  
  </div>
  
  <div id="outline-container-1-1" class="outline-4">
  <h4 id="sec-1-1"><span class="section-number-4">1.1</span> CART Tree analysis that addresses the (potential)spatial autocorrelation problem</h4>
  <div class="outline-text-4" id="text-1-1">
  
  <p>We started off the discussion with an assessment of the approach described in this post <a href="http://thebiobucket.blogspot.com.au/2012/03/classification-trees-allowing-for.html">Classification Trees and Spatial Autocorrelation</a>.
  </p>
  <p>
  I've been thinking more and more about decision trees/CART/random forest methods for selecting a subset of relevant variables (and interations) for use in GLM or GAM model construction.  In a perfect world I'd have data on the main predictor I wanted to model and enough data about all the relevant other predictors (especially confounding or modifying variables) to ensure I get a 'well behaved model'. But with all the data around and so many potentially plausible relationships one might choose to include we need a way to narrow down these to just include the most important covariates, confounders and interactions.  CART or some variation on it seems a good way to do this, but is prone to the potential problem of spatially correlated errors too.
  </p>
  <p>
  The idea from that blog post is:
  </p>
  <p>
  "compute the classification tree, calculate residuals and use it for a Mantel-test and Mantel correlograms.
  The Mantel correlograms test differrences in dissimilarities of
  the residuals across several spatial distances and thus enable you to detect lag-distances where possible spatial autocorrelation vanishes.
  &hellip;If encounter autocorrelation&hellip; try to use subsamples of the data avoiding resampling within the lag-distance.."
  </p>
  <p>
  I think the workflow would be to
  </p>
  <ul>
  <li>fit the classification tree (Question: best to use all the data or with a sample like using cross-validation)
  </li>
  <li>get the residuals and visually assess the lagged distances plot provided by the Mantel correlogram.  Decide on a threshold (Question: is there an objective way to do this?).
  </li>
  <li>Sample from the data and select out from this sample only data from pairs with distances greater than the threshold (have to keep one out of each close pair or else we'd only be getting data from the sparsely sampled parts of our study region).
  </li>
  </ul>
  
  
  <p>
  We all agreed this sounded OK, but only avoids the problem of spatial autocorrelation (and loses data).
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-2" class="outline-4">
  <h4 id="sec-1-2"><span class="section-number-4">1.2</span> Modeling with control for spatial autocorrelation</h4>
  <div class="outline-text-4" id="text-1-2">
  
  <p>So we all agreed we'd prefer if our model can control for spatial autocorrelation.  I confessed that I'd always found the GeoBUGS tutorial and other tutorials about Bayesian methods for this very difficult and would really like a "Simple" way to make the problem go away.  So first we briefly reviewed Prof Hainings 3 equations again:
  </p>
  <p>
  NOTE: THE FOLLOWING IDEAS WORK BEST FOR AREAL DATA.
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-3" class="outline-4">
  <h4 id="sec-1-3"><span class="section-number-4">1.3</span> The Spatial Error Model</h4>
  <div class="outline-text-4" id="text-1-3">
  
  
  
  
  \(Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \eta_{i}\)
  
  <p>
  Where:
  </p>
  <p>
  \(\eta_{i}\) = Spatially autocorrelated errors.
  </p>
  
  </div>
  
  </div>
  
  <div id="outline-container-1-4" class="outline-4">
  <h4 id="sec-1-4"><span class="section-number-4">1.4</span> The Spatial Lag Model</h4>
  <div class="outline-text-4" id="text-1-4">
  
  
  
  
  \(Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \rho(Neighbours Y_{ij}) + e_{i}\)
  
  <p>
  Where:
  </p>
  <p>
  \(\rho_(Neighbours Y_{ij})\) = is an additional explanatory variable which is the value of the dependent variable in neighbouring areas. 
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-5" class="outline-4">
  <h4 id="sec-1-5"><span class="section-number-4">1.5</span> Spatially Lagged Independent Variable(s)</h4>
  <div class="outline-text-4" id="text-1-5">
  
  
  
  
  \(Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \beta_{2L} X_{2ij} + e_{i}\)
  
  <p>
  Where:
  </p>
  <p>
  \(\beta_{2L} X_{2ij}\) = is the independent variable X2 that is spatially lagged.
  </p>
  
  </div>
  
  </div>
  
  <div id="outline-container-1-6" class="outline-4">
  <h4 id="sec-1-6"><span class="section-number-4">1.6</span> Discussion</h4>
  <div class="outline-text-4" id="text-1-6">
  
  <ul>
  <li>Phil agreed with Bob that the spatial error model is the best, spatial lag model is OK and spatially lagged covariates not so great.
  </li>
  <li>For spatial error model fitting Phil suggested looking at R packages spBayes and spTimer.
  </li>
  <li>I pointed out that I am mostly interested in "spatially structured time-series models" rather than spatial models at a single point in time.  By this I mean that we have several neighbouring areal units observed over a period of time.  In this framework the general methods of time series modelling are used to control for temporal autocorrelation.  However this makes the methods of spatial error and spatial lag models tricky because the spatial autocorrelation needs to be assessed at many points in time.
  </li>
  <li>I asked that if spatial lag is OK (and it seems easier to fit into the time-series model framework) how can I check to know if it has done the trick?  If this were purely a spatial model we could check for spatial autocorrelation in the residuals just as they described in the CART blog above, but here we have many maps we could make (one every time point), and our spatial autocorrelation measure would surely vary a lot over time.  SO would a simple way just be to asses the effect on the Standard Error on beta1 (our primary interest) and if it is bigger but still significant we can be reassured that our result isn't affected? Or perhaps we should assess the beta on the lagged variable, for instance is a significant p-value on the lagged Beta an indication that it is capturing the unmeasured spatial associations represented by the neighbourhood variable?  
  </li>
  <li>If it hadn't done the trick Nerida pointed out this might be because the Neighbourhoods are actually not appropriately represented by the first order neighbours and therefore more neighbours could be included, like moving out several concentric circles to wider and wider neighbourhoods
  </li>
  <li>Nasser and Phil pointed out that the lagged variable (the outcome in the neighbours) includes an element of the exposure variables, and said that it would be difficult to 'unpack' what that part of the model meant.
  </li>
  <li>so it looks like there is no simple answer and spatial error model is still preferred.
  </li>
  </ul>
  
  
  </div>
  </div>
  </div>
  </div>
  
  </body>
  </html>
  
#+end_src



** 2013-09-26-spatially-structured-timeseries-vs-spatiotemporal-modelling
** Spatially Structured Timeseries Vs Spatiotemporal Modelling
In my last post about [[http://ivanhanigan.github.io/2013/09/reflections-bob-haining-update][spatiotemporal regression modelling]] I mentioned that I am mostly interested in "spatially structured time-series models" rather than spatial models at a single point in time. By this I mean that we have several neighbouring areal units observed over a period of time. In this framework the general methods of time series modelling are used to control for temporal autocorrelation. However this makes the methods of spatial error and spatial lag models tricky because the spatial autocorrelation needs to be assessed at many points in time.

I want to expand more on this topic because I want to be clear that the organisation of the material I am aiming to bring to this notebook topic is not aimed at purely spatial regression models [[https://geodacenter.asu.edu/spatial-lag-and][(there is a lot of material and tools out there already for that)]].  I am trying with these notes to document my learning steps toward integrating spatial methods with time-series methods to allow me to practice (and understand) spatiotemporal regression modelling.

*** Spatially Structured Time Series
In [[http://www.pnas.org/content/early/2012/08/08/1112965109.full.pdf+html][my most successful previous attempt to conduct a spatiotemporal analysis of Suicide and Droughts]] I built on my knowledge of time-series regression models from single-city air pollution studies where the whole city is the unit of analysis and the temporal variation is modelled with controlling techniques for temporal autocorrelation.  These techniques are also valid for multi-city studies because it is pretty safe to assume the cities are all independent at each time point.  I structured my study by Eleven large zones (Census Statistical Divisions) of NSW and assumed each of these would vary over time independent of each other, and I fitted a zone-specific time trend and cycle. This is what I call "spatially structured time-series" modelling.  

I justify using this model in this case because aggregating up to these very large regions will diminish the possibility of spatial autocorrelation and because Droughts vary over large spatial zones too, we will not suffer from exposure misclassificaiton bias.

So this model is a simple time-series regression (with trend and seasonality) and an additional term for spatial Zone.

\begin{eqnarray*}
        log({\color{red} O_{ijk}})  & = & s({\color{red}ExposureVariable})  + {\color{blue} OtherExplanators}  \\
        & &   + AgeGroup_{i} + Sex_{j} \\
        & &   + {\color{blue} SpatialZone_{k}}  \\
        & &  + sin(Time \times 2 \times \pi) + cos(Time \times 2 \times \pi) \\
        & &  + Trend \\
        & &   + offset({\color{blue} log(Pop_{ijk})})\\
\end{eqnarray*}

Where:\\

- ${\color{red}O_{ijk}}$ = Outcome (counts) by Age$_{i}$, Sex$_{j}$ and SpatialZone$_{k}$ \\
- {\color{red}ExposureVariable} = Data with {\color{red}Restrictive Intellectual Property~(IP)} \\
- {\color{blue}OtherExplanators} = Other {\color{blue}Less Restricted}  Explanatory variables \\
- s( ) = penalized regression splines \\
- ${\color{blue} SpatialZone_{k}}$  = {\color{blue} Less Restricted} data representing the $SpatialZone_{k}$  \\
- Trend = Longterm smooth trend(s) \\
- ${\color{blue}Pop_{ijk}}$ = interpolated Census populations, by time in each group\\

*** TODO Spatiotemporal modelling
In contrast to the above model for modelling exposures that have fine resolution spatial variation (such as air pollution) the exposure misclassification effect of aggregating up to very large spatial zones will conteract the benefits of avoiding spatially autocorrelated errors and this might be unacceptable for certain research questions.  Therefore it is important to move toward a spatiotemporal regression model that replaces the $SpatialZone_{k}$ term with a more spatial error or spatial lag approach.
** md
#+name:spatially-structured-timeseries-vs-spatiotemporal-modelling-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-09-26-spatially-structured-timeseries-vs-spatiotemporal-modelling.md :exports none :eval no :padline no
---
name: spatially-structured-timeseries-vs-spatiotemporal-modelling
layout: post
title: spatially-structured-timeseries-vs-spatiotemporal-modelling
date: 2013-09-26
categories:
- spatial dependence
---
    
<head>
<title>Spatiotemporal Regression Modelling</title>
<meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1"/>
<meta name="title" content="Spatiotemporal Regression Modelling"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2013-09-26T10:18+1000"/>
<meta name="author" content="Ivan Hanigan"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js">
/**
 *
 * @source: http://orgmode.org/mathjax/MathJax.js
 *
 * @licstart  The following is the entire license notice for the
 *  JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 * Copyright (C) 2012-2013  MathJax
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * @licend  The above is the entire license notice
 * for the JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 */

/*
@licstart  The following is the entire license notice for the
JavaScript code below.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code below is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code below.
*/
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">Spatiotemporal Regression Modelling</h1>


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 Spatially Structured Timeseries Vs Spatiotemporal Modelling</a>
<ul>
<li><a href="#sec-1-1">1.1 Spatially Structured Time Series</a></li>
<li><a href="#sec-1-2">1.2 Spatiotemporal modelling</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-3">
<h3 id="sec-1"><span class="section-number-3">1</span> Spatially Structured Timeseries Vs Spatiotemporal Modelling</h3>
<div class="outline-text-3" id="text-1">

<p>In my last post about <a href="http://ivanhanigan.github.io/2013/09/reflections-bob-haining-update">spatiotemporal regression modelling</a> I mentioned that I am mostly interested in "spatially structured time-series models" rather than spatial models at a single point in time. By this I mean that we have several neighbouring areal units observed over a period of time. In this framework the general methods of time series modelling are used to control for temporal autocorrelation. However this makes the methods of spatial error and spatial lag models tricky because the spatial autocorrelation needs to be assessed at many points in time.
</p>
<p>
I want to expand more on this topic because I want to be clear that the organisation of the material I am aiming to bring to this notebook topic is not aimed at purely spatial regression models <a href="https://geodacenter.asu.edu/spatial-lag-and">(there is a lot of material and tools out there already for that)</a>.  I am trying with these notes to document my learning steps toward integrating spatial methods with time-series methods to allow me to practice (and understand) spatiotemporal regression modelling.
</p>

</div>

<div id="outline-container-1-1" class="outline-4">
<h4 id="sec-1-1"><span class="section-number-4">1.1</span> Spatially Structured Time Series</h4>
<div class="outline-text-4" id="text-1-1">

<p>In <a href="http://www.pnas.org/content/early/2012/08/08/1112965109.full.pdf+html">my most successful previous attempt to conduct a spatiotemporal analysis of Suicide and Droughts</a> I built on my knowledge of time-series regression models from single-city air pollution studies where the whole city is the unit of analysis and the temporal variation is modelled with controlling techniques for temporal autocorrelation.  These techniques are also valid for multi-city studies because it is pretty safe to assume the cities are all independent at each time point.  I structured my study by Eleven large zones (Census Statistical Divisions) of NSW and assumed each of these would vary over time independent of each other, and I fitted a zone-specific time trend and cycle. This is what I call "spatially structured time-series" modelling.  
</p>
<p>
I justify using this model in this case because aggregating up to these very large regions will diminish the possibility of spatial autocorrelation and because Droughts vary over large spatial zones too, we will not suffer from exposure misclassificaiton bias.
</p>
<p>
So this model is a simple time-series regression (with trend and seasonality) and an additional term for spatial Zone.
</p>


\begin{eqnarray*}
        log({\color{red} O_{ijk}})  & = & s({\color{red}ExposureVariable})  + {\color{blue} OtherExplanators}  \\
        & &   + AgeGroup_{i} + Sex_{j} \\
        & &   + {\color{blue} SpatialZone_{k}}  \\
        & &  + sin(Time \times 2 \times \pi) + cos(Time \times 2 \times \pi) \\
        & &  + Trend \\
        & &   + offset({\color{blue} log(Pop_{ijk})})\\
\end{eqnarray*}

<p>
Where:<br/>
</p>
<ul>
<li>\({\color{red}O_{ijk}}\) = Outcome (counts) by Age\(_{i}\), Sex\(_{j}\) and SpatialZone\(_{k}\) <br/>
</li>
<li>{\color{red}ExposureVariable} = Data with {\color{red}Restrictive Intellectual Property~(IP)} <br/>
</li>
<li>{\color{blue}OtherExplanators} = Other {\color{blue}Less Restricted}  Explanatory variables <br/>
</li>
<li>s( ) = penalized regression splines <br/>
</li>
<li>\({\color{blue} SpatialZone_{k}}\)  = {\color{blue} Less Restricted} data representing the \(SpatialZone_{k}\)  <br/>
</li>
<li>Trend = Longterm smooth trend(s) <br/>
</li>
<li>\({\color{blue}Pop_{ijk}}\) = interpolated Census populations, by time in each group<br/>
</li>
</ul>


</div>

</div>

<div id="outline-container-1-2" class="outline-4">
<h4 id="sec-1-2"><span class="section-number-4">1.2</span> <span class="todo TODO">TODO</span> Spatiotemporal modelling</h4>
<div class="outline-text-4" id="text-1-2">

<p>In contrast to the above model for modelling exposures that have fine resolution spatial variation (such as air pollution) the exposure misclassification effect of aggregating up to very large spatial zones will conteract the benefits of avoiding spatially autocorrelated errors and this might be unacceptable for certain research questions.  Therefore it is important to move toward a spatiotemporal regression model that replaces the \(SpatialZone_{k}\) term with a more spatial error or spatial lag approach.
</p></div>
</div>
</div>
</div>

</body>
</html>
#+end_src

** 2013-10-10-simple-example-using-nmmaps
#+name:simple-example-using-nmmaps-header
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-10-10-simple-example-using-nmmaps.md :exports none :eval no :padline no
  ---
  name: 2013-10-10-simple-example-using-nmmaps
  layout: post
  title: simple-example-using-nmmaps
  date: 2013-10-10
  categories:
  - spatial dependence
  ---
  
  I will use the NMMAPSlite datasets for a simple example of what I am trying to do.
    

  <!-- begin_src R :session *R* :tangle NMMAPS-example/NMMAPS-example-code.r :exports none :eval no -->
   
  #### Code: get nmmaps data
      # func
      if(!require(NMMAPSlite)) install.packages('NMMAPSlite');require(NMMAPSlite)
      require(mgcv)
      require(splines)
  
      ######################################################
      # load  
      setwd('data')
      initDB('data/NMMAPS') # this requires that we connect to the web,
                            # so lets get local copies
      setwd('..')
      cities <- getMetaData('cities')
      head(cities)
      citieslist <- cities$cityname
      # write out a few cities for access later
      for(city_i in citieslist[sample(1:nrow(cities), 9)])
      {
       city <- subset(cities, cityname == city_i)$city
       data <- readCity(city)
       write.table(data, file.path('data', paste(city_i, '.csv',sep='')),
       row.names = F, sep = ',')
      }
      # these are all tiny, go some big ones
      for(city_i in c('New York', 'Los Angeles', 'Madison', 'Boston'))
      {
       city <- subset(cities, cityname == city_i)$city
       data <- readCity(city)
       write.table(data, file.path('data', paste(city_i, '.csv',sep='')),
       row.names = F, sep = ',')
      }
  
      ######################################################
      # now we can use these locally
      dir("data")
      city <- "Chicago"
      data <- read.csv(sprintf("data/%s.csv", city), header=T)
      str(data)
      data$yy <- substr(data$date,1,4)
      data$date <- as.Date(data$date)
      ######################################################
      # check
      par(mfrow=c(2,1), mar=c(4,4,3,1))
      with(subset(data[,c(1,15:25)], agecat == '75p'),
        plot(date, tmax)
       )
      with(subset(data[,c(1,4,15:25)], agecat == '75p'),
              plot(date, cvd, type ='l', col = 'grey')
              )
      with(subset(data[,c(1,4,15:25)], agecat == '75p'),
              lines(lowess(date, cvd, f = 0.015))
              )
      # I am worried about that outlier
      data$date[which(data$cvd > 100)]
      # [1] "1995-07-15" "1995-07-16"
       
      ######################################################
      # do standard NMMAPS timeseries poisson GAM model
      numYears<-length(names(table(data$yy)))
      df <- subset(data, agecat == '75p')
      df$time <- as.numeric(df$date)
      fit <- gam(cvd ~ s(pm10tmean) + s(tmax) + s(dptp) + s(time, k= 7*numYears, fx=T), data = df, family = poisson)
      # plot of response functions
      par(mfrow=c(2,2))
      plot(fit)
      dev.off()
       
      ######################################################
      # some diagnostics
      summary(fit)
      # note the R-sq.(adj) =   0.21
      gam.check(fit)
      # note the lack of a leverage plot.  for that we need glm
       
      ######################################################
      # do same model as glm
      fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = poisson)
      # plot responses
      par(mfrow=c(2,2))
      termplot(fit2, se =T)
      dev.off()
       
      # plot prediction
      df$predictedCvd <- predict(fit2, df, 'response')
      # baseline is given by the intercept
      fit3 <- glm(cvd ~ 1, data = df, family = poisson)
      df$baseline <-  predict(fit3, df, 'response')
      with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
       plot(date, cvd, type ='l', col = 'grey')
              )
      with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
              lines(date,predictedCvd)
              )
      with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
       lines(date,baseline)
              )
      ######################################################
      # some diagnostics
      # need to load a function to calculate poisson adjusted R squared
      # original S code from
      # The formula for pseudo-R^2 is taken from G. S. Maddalla,
      # Limited-dependent and Qualitative Variables in Econometrics, Cambridge:Cambridge Univ. Press, 1983. page 40, equation 2.50.
      RsquaredGlm <- function(o) {
       n <- length(o$residuals)
       ll <- logLik(o)[1]
       ll_0 <- logLik(update(o,~1))[1]
       R2 <- (1 - exp(((-2*ll) - (-2*ll_0))/n))/(1 - exp( - (-2*ll_0)/n))
       names(R2) <- 'pseudo.Rsquared'
       R2
       }
      RsquaredGlm(fit2)
      # 0.51
      # the difference is presumably due to the arguments about how to account for unexplainable variance in the poisson distribution?
       
      # significance of spline terms
      drop1(fit2, test='Chisq')
      # also note AIC. best model includes all of these terms
      # BIC can be computed instead (but still labelled AIC) using
      drop1(fit2, test='Chisq', k = log(nrow(data)))
       
      # diagnostic plots
      par(mfrow=c(2,2))
      plot(fit2)
      dev.off()
      # note high leverage plus residuals points are labelled
      # leverage doesn't seem to be too high though which is good
      # NB the numbers refer to the row.names attribute which still refer to the original dataset, not this subset
      df[row.names(df) %in% c(9354,9356),]$date
      # as suspected [1] "1995-07-15" "1995-07-16"
       
      ######################################################
      # so lets re run without these obs
      df2 <- df[!row.names(df) %in% c(9354,9356),]
      # to avoid duplicating code just re run fit2, replacing data=df with df2
      # tmax still significant but not so extreme
      # check diagnostic plots again
      par(mfrow=c(2,2))
      plot(fit2)
      dev.off()
      # looks like a well behaved model now.
       
      # if we were still worried about any high leverage values we could identify these with
      df3 <- na.omit(df2[,c('cvd','pm10tmean','tmax','dptp','time')])
      df3$hatvalue <- hatvalues(fit2)
      df3$res <- residuals(fit2, 'pearson')
      with(df3, plot(hatvalue, res))
      # this is the same as the fourth default glm diagnostic plot, which they label x-axis as leverage
      summary(df3$hatvalue)
      # gives us an idea of the distribution of hat values
      # decide on a threshold and look at it
      hatThreshold <- 0.1
      with(subset(df3, hatvalue > hatThreshold), points(hatvalue, res, col = 'red', pch = 16))
      abline(0,0)
      segments(hatThreshold,-2,hatThreshold,15)
      dev.off()
       
      fit3 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = subset(df3, hatvalue < hatThreshold), family = poisson)
      par(mfrow=c(2,2))
      termplot(fit3, se = T)
      # same same
      plot(fit3)
      # no better
       
      # or we could go nuts with a whole number of ways of estimating influence
      # check all influential observations
      infl <- influence.measures(fit2)
      # which observations 'are' influential
      inflk <- which(apply(infl$is.inf, 1, any))
      length(inflk)
       
       
      ######################################################
      # now what about serial autocorrelation in the residuals?
       
      par(mfrow = c(2,1))
      with(df3, acf(res))
      with(df3, pacf(res))
      dev.off()
       
       
       
      ######################################################
      # just check for overdispersion
      fit <- gam(cvd ~ s(pm10tmean) + s(tmax) + s(dptp) + s(time, k= 7*numYears, fx=T), data = df, family = quasipoisson)
      summary(fit)
      # note the Scale est. = 1.1627
      # alternatively check the glm
      fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = quasipoisson)
      summary(fit2)
      # (Dispersion parameter for quasipoisson family taken to be 1.222640)
      # this is probably near enough to support a standard poisson model...
       
      # if we have overdispersion we can use QAIC (A quasi- mode does not have a likelihood and so does not have an AIC,  by definition)
      # we can use the poisson model and calculate the overdispersion
      fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = poisson)
      1- pchisq(deviance(fit2), df.residual(fit2))
       
      # QAIC, c is the variance inflation factor, the ratio of the residual deviance of the global (most complicated) model to the residual degrees of freedom
      c=deviance(fit2)/df.residual(fit2)
      QAIC.1=-2*logLik(fit2)/c + 2*(length(coef(fit2)) + 1)
      QAIC.1
       
      # Actually lets use QAICc which is more conservative about parameters,
      QAICc.1=-2*logLik(fit2)/c + 2*(length(coef(fit2)) + 1) + 2*(length(coef(fit2)) + 1)*(length(coef(fit2)) + 1 + 1)/(nrow(na.omit(df[,c('cvd','pm10tmean','tmax','dptp','time')]))- (length(coef(fit2))+1)-1)
      QAICc.1
       
       
      ######################################################
      # the following is old work, some may be interesting
      # such as the use of sinusoidal wave instead of smooth function of time
       
       
      # # sine wave
      # timevar <- as.data.frame(names(table(df$date)))
      # index <- 1:length(names(table(df$date)))
      # timevar$time2 <- index / (length(index) / (length(index)/365.25))
      # names(timevar) <- c('date','timevar')
      # timevar$date <- as.Date(timevar$date)
      # df <- merge(df,timevar)
       
      # fit <- gam(cvd ~ s(tmax) + s(dptp) + sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
      # summary(fit)
      # par(mfrow=c(3,2))
      # plot(fit, all.terms = T)
      # dev.off()
       
      # # now just explore the season fit
      # fit <- gam(cvd ~ sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
      # yhat <- predict(fit)
      # head(yhat)
       
      # with(df, plot(date,cvd,type = 'l',col='grey', ylim = c(15,55)))
      # lines(df[,'date'],exp(yhat),col='red')
       
       
      # # drop1(fit, test= 'Chisq')
       
      # # drop1 only works in glm?
      # # fit with weather variables, use degrees of freedom estimated by gam
      # fit <- glm(cvd ~ ns(tmax,8) + ns(dptp,2) + sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
      # drop1(fit, test= 'Chisq')
      # # use plot.glm for diagnostics
      # par(mfrow=c(2,2))
      # plot(fit)
      # par(mfrow=c(3,2))
      # termplot(fit, se=T)
      # dev.off()
       
      # # cyclic spline, overlay on prior sinusoidal
      # with(df, plot(date,cvd,type = 'l',col='grey', ylim = c(0,55)))
      # lines(df[,'date'],exp(yhat),col='red')
       
      # df$daynum <- as.numeric(format(df$date, "%j"))
      # df[360:370,c('date','daynum')]
      # fit <- gam(cvd ~ s(daynum, k=3, fx=T, bs = 'cp') +  s(time, k = numYears, fx = T), data = df, family = poisson)
      # yhat2 <- predict(fit)
      # head(yhat2)
       
      # lines(df[,'date'],exp(yhat2),col='blue')
       
       
      # par(mfrow=c(1,2))
      # plot(fit)
       
       
      # # fit weather with season
      # fit <- gam(cvd ~ s(tmax) + s(dptp) +
        # s(daynum, k=3, fx=T, bs = 'cp') +  s(time, k = numYears, fx = T), data = df, family = poisson)
      # par(mfrow=c(2,2))
      # plot(fit)
       
      # summary(fit)
  
#+end_src
** COMMENT 2013-10-16-spatially-structured-time-series-with-nmmaps
#+name:spatially-structured-time-series-with-nmmaps-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-10-16-spatially-structured-time-series-with-nmmaps.md :exports none :eval no :padline no
  ---
  name: spatially-structured-time-series-with-nmmaps
  layout: post
  title: spatially-structured-time-series-with-nmmaps
  date: 2013-10-16
  categories:
  - spatial dependence
  ---
  
  I will use the NMMAPSlite datasets for a simple example of what I
  describe as "Spatially Structured Timeseries" as opposed to
  "Spatio-Temporal" which I think more explicitly includes spatial
  structure in the model.  [See This Report](http://ivanhanigan.github.io/spatiotemporal-regression-models/) for all the gory details.
  
  # R Codes
  
  <!-- <?xml version="1.0" encoding="utf-8"?> -->
  <!-- <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" -->
  <!--                "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"> -->
  <!-- <html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"> -->
  <head>
  <title>Spatiotemporal Regression Modelling</title>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
  <meta name="title" content="Spatiotemporal Regression Modelling"/>
  <meta name="generator" content="Org-mode"/>
  <meta name="generated" content="2013-10-16T15:17+1100"/>
  <meta name="author" content="Ivan Hanigan"/>
  <meta name="description" content=""/>
  <meta name="keywords" content=""/>
  <style type="text/css">
   <!--/*--><![CDATA[/*><!--*/
    html { font-family: Times, serif; font-size: 12pt; }
    .title  { text-align: center; }
    .todo   { color: red; }
    .done   { color: green; }
    .tag    { background-color: #add8e6; font-weight:normal }
    .target { }
    .timestamp { color: #bebebe; }
    .timestamp-kwd { color: #5f9ea0; }
    .right  {margin-left:auto; margin-right:0px;  text-align:right;}
    .left   {margin-left:0px;  margin-right:auto; text-align:left;}
    .center {margin-left:auto; margin-right:auto; text-align:center;}
    p.verse { margin-left: 3% }
    pre {
      border: 1pt solid #AEBDCC;
      background-color: #F3F5F7;
      padding: 5pt;
      font-family: courier, monospace;
          font-size: 90%;
          overflow:auto;
    }
    table { border-collapse: collapse; }
    td, th { vertical-align: top;  }
    th.right  { text-align:center;  }
    th.left   { text-align:center;   }
    th.center { text-align:center; }
    td.right  { text-align:right;  }
    td.left   { text-align:left;   }
    td.center { text-align:center; }
    dt { font-weight: bold; }
    div.figure { padding: 0.5em; }
    div.figure p { text-align: center; }
    div.inlinetask {
      padding:10px;
      border:2px solid gray;
      margin:10px;
      background: #ffffcc;
    }
    textarea { overflow-x: auto; }
    .linenr { font-size:smaller }
    .code-highlighted {background-color:#ffff00;}
    .org-info-js_info-navigation { border-style:none; }
    #org-info-js_console-label { font-size:10px; font-weight:bold;
                                 white-space:nowrap; }
    .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                   font-weight:bold; }
    /*]]>*/-->
  </style>
  <script type="text/javascript">
  /*
  @licstart  The following is the entire license notice for the
  JavaScript code in this tag.
  
  Copyright (C) 2012-2013 Free Software Foundation, Inc.
  
  The JavaScript code in this tag is free software: you can
  redistribute it and/or modify it under the terms of the GNU
  General Public License (GNU GPL) as published by the Free Software
  Foundation, either version 3 of the License, or (at your option)
  any later version.  The code is distributed WITHOUT ANY WARRANTY;
  without even the implied warranty of MERCHANTABILITY or FITNESS
  FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.
  
  As additional permission under GNU GPL version 3 section 7, you
  may distribute non-source (e.g., minimized or compacted) forms of
  that code without the copy of the GNU GPL normally required by
  section 4, provided you include this license notice and a URL
  through which recipients can access the Corresponding Source.
  
  
  @licend  The above is the entire license notice
  for the JavaScript code in this tag.
  ,*/
  <!--/*--><![CDATA[/*><!--*/
   function CodeHighlightOn(elem, id)
   {
     var target = document.getElementById(id);
     if(null != target) {
       elem.cacheClassElem = elem.className;
       elem.cacheClassTarget = target.className;
       target.className = "code-highlighted";
       elem.className   = "code-highlighted";
     }
   }
   function CodeHighlightOff(elem, id)
   {
     var target = document.getElementById(id);
     if(elem.cacheClassElem)
       elem.className = elem.cacheClassElem;
     if(elem.cacheClassTarget)
       target.className = elem.cacheClassTarget;
   }
  /*]]>*///-->
  </script>
  
  </head>
  <body>
  
  <div id="preamble">
  
  </div>
  
  <div id="content">
  <h1 class="title">Spatiotemporal Regression Modelling</h1>
  
  
  <div id="table-of-contents">
  <h2>Table of Contents</h2>
  <div id="text-table-of-contents">
  <ul>
  <li><a href="#sec-1">1 Core Model</a></li>
  <li><a href="#sec-2">2 Core Model Plots</a></li>
  </ul>
  </div>
  </div>
  
  <div id="outline-container-1" class="outline-4">
  <h4 id="sec-1"><span class="section-number-4">1</span> Core Model</h4>
  <div class="outline-text-4" id="text-1">
  
  
  
  
  <pre class="src src-R"><span style="color: #5F7F5F;">################################################################</span>
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">name:core</span>
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">func</span>
  setwd(<span style="color: #CC9393;">"~/projects/spatiotemporal-regression-models/NMMAPS-example"</span>)
  <span style="color: #BFEBBF; font-weight: bold;">require</span>(mgcv)
  <span style="color: #BFEBBF; font-weight: bold;">require</span>(splines)
  
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">load</span>
  analyte <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> read.csv(<span style="color: #CC9393;">"analyte.csv"</span>)
  
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">clean</span>
  analyte$yy <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> substr(analyte$date,1,4)
  numYears<span style="color: #BFEBBF; font-weight: bold;">&lt;-</span>length(names(table(analyte$yy)))
  analyte$date <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> as.Date(analyte$date)
  analyte$time <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> as.numeric(analyte$date)
  analyte$agecat <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> factor(analyte$agecat,
                            levels = c(<span style="color: #CC9393;">"under65"</span>,
                                <span style="color: #CC9393;">"65to74"</span>, <span style="color: #CC9393;">"75p"</span>),
                            ordered = <span style="color: #7CB8BB;">TRUE</span>
                            )
  
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">do</span>
  fit <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> gam(cvd ~ s(tmax) + s(dptp) +
             city + agecat +
             s(time, k= 7*numYears, fx=T) +
             offset(log(pop)),
             data = analyte, family = poisson
             )
  
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">plot of response functions</span>
  png(<span style="color: #CC9393;">"images/nmmaps-eg-core.png"</span>, width = 1000, height = 750, res = 150)
  par(mfrow=c(2,3))
  plot(fit, all.terms = <span style="color: #7CB8BB;">TRUE</span>)
  dev.off()
  
  
  </pre>
  
  </div>
  
  </div>
  
  <div id="outline-container-2" class="outline-4">
  <h4 id="sec-2"><span class="section-number-4">2</span> Core Model Plots</h4>
  <div class="outline-text-4" id="text-2">
  
  <p><img src="/images/nmmaps-eg-core.png"  alt="/images/nmmaps-eg-core.png" />
  </p></div>
  </div>
  </div>
  
  </body>
  </html>
  
  
#+end_src
# ![nmmaps-eg-core.png](/images/nmmaps-eg-core.png)
** 2013-10-28-spatial-lag-and-timeseries-model-with-nmmaps
#+name:spatial-lag-and-timeseries-model-with-nmmaps-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-10-28-spatial-lag-and-timeseries-model-with-nmmaps.md :exports none :eval no :padline no
---
name: spatial-lag-and-timeseries-model-with-nmmaps
layout: post
title: spatial-lag-and-timeseries-model-with-nmmaps
date: 2013-10-28
categories:
- spatial dependence
---

### UPDATE

- I have renamed the repo.  See [/2016/05/spatial-lag-and-timeseries-model-with-nmmaps-UPDATE](/2016/05/spatial-lag-and-timeseries-model-with-nmmaps-UPDATE)

### OLD POST

- Today I chatted with Phil Kokic at CSIRO Mathematics, Informatics and Statistics about the way the spatially lagged neighbours variable absorbs any residual spatial correlation in the errors
- We agreed that this is a pretty minimal attempt, not as good as a spatial error model but pretty easy to implement
- I've hacked together some very very ugly code to construct the lagged variable
- [http://ivanhanigan.github.io/spatiotemporal-regression-models/#sec-3](http://ivanhanigan.github.io/spatiotemporal-regression-models/#sec-3)
- There may be errors, it's been a long day, but I won't have a chance to check back on this till next week so I thought I'd put it out there as is.

#+end_src

** 2013-10-31-notes-on-spatial-stats-meeting-with-sarunya-sujaritpong
#+name:notes-on-meeting-with-sarunya-sujaritpong-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-10-31-notes-on-spatial-stats-meeting-with-sarunya-sujaritpong.md :exports none :eval no :padline no
---
name: notes-on-spatial-stats-meeting-with-sarunya-sujaritpong
layout: post
title: notes-on-spatial-stats-meeting-with-sarunya-sujaritpong
date: 2013-10-31
categories:
- spatial dependence
---

- Yesterday I met with Sarunya Sujaritpong a PhD student working with [spatially structured time-series models as described previously](http://ivanhanigan.github.io/2013/10/spatially-structured-time-series-with-nmmaps/)
- Her supervisor Keith Dear has given me a lot of good stats advice in the past and one bit I keep thinking about is that a time series model can be fit for multiple spatial areal units of the same city and residual spatial auto-correlation in the errors should not be too much of a concern
- The problem would be if you get strong spatial autocorrelation of the residuals this indicates that the assumption of independent errors is violated and you will have tighter confidence intervals around the coefficients of interest than is really the case, inflating the signficance estimated for the relative risk  
- The beta coefficient itself shouldn't be affected.
- So long as biostatisticians like Keith are comfortable with not addressing this issue in spatially structured time-series that is great but I see people are [starting to include this in their models](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0043360) 
- To date I've mostly seen it done in spatial (cross sectional) data analysis, not spatial times-series
- I'm preparing for the day when I might need to address this for a reviewer and would like to know what to do about it in case that happens
- So I asked Sarunya for a discussion about her research

### Sarunya's model is essentially like this
#### R Code:
    fit <- glm(deaths ~ pollutant1 + pollutant2 + pollutant ... +
           ns(temp, df) + ns(humidity, df) +
           ns(time, df = 7*numYears) +
           SLA * ns(time, df = 2),
           data = analyte, family = poisson
           )
<p></p>
- SLA is Statistical Local Area (now called SA2, like a suburb)
- Sarunya explained that the research question was if the magnitude of the coeff on pollutant1 differed between this model and the old style of model where an entire city is used as the unit of analysis per day and exposure estimates are calculated as averages across several monitoring stations in the city
- turns out that this comparison is still valid EVEN IF THE STANDARD ERROR IS BIASED DUE TO RESIDUAL SPATIAL AUTOCORRELATION
- therefore this study avoids the issue by it's intentional design to compare betas not se
- Interestingly Sarunya explained that the stats theory suggests that even if the exposure precision is increased (exposure misclassification bias is decreased) the se on the beta will not be affected.
- this is fascinating in itself, but a separate issue for another post

### Conclusions
- So it looks like the extent a study might need to consider the issue of potential residual spatial autocorrelation depends alot on what questions are being asked and what inferences will be attempted from the results
- if the aim of the study is to estimate the magnitude AND CONFIDENCE INTERVALS of an exposure's relative risk (especially some novel exposure such as interstellar space dust across the suburbs) then the issue might become important to address.

THANKS Sarunya!
#+end_src
** 2016-05-22-spatial-lag-and-timeseries-model-with-nmmaps-UPDATE

#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2016-05-22-spatial-lag-and-timeseries-model-with-nmmaps-UPDATE.md :exports none :eval no :padline no
---
name: spatial-lag-and-timeseries-model-with-nmmaps-UPDATE
layout: post
title: spatial-lag-and-timeseries-model-with-nmmaps-UPDATE
date: 2016-05-22
categories:
- spatial dependence
---

- Re my old post: [/2013/10/spatial-lag-and-timeseries-model-with-nmmaps](/2013/10/spatial-lag-and-timeseries-model-with-nmmaps)
- Today I updated my repo from just looking at spatiotemporal regression to now also look at multilevel (aka mixed-effects/random-effects) models
-  The new site is based on the minimal theme by orderedlist: [http://ivanhanigan.github.io/spatiotemporal-multilevel-models](http://ivanhanigan.github.io/spatiotemporal-multilevel-models)
- That also means I've had to move some of my old codes to the now location
- [http://ivanhanigan.github.io/spatiotemporal-multilevel-models/spatiotemporal-multilevel-models.html](http://ivanhanigan.github.io/spatiotemporal-multilevel-models/spatiotemporal-multilevel-models.html)

#+end_src

* bookdown instructions
- create files in -raw
- build the book (rstudio is easy)
cd spatiotemporal-multilevel-models-book/
cp -R ../spatiotemporal-multilevel-models-raw/_book/* ./

_config.yml  general-regression-versus-mixed-effects-models.html  index.html  introduction-1.html  introduction-to-r.html  libs  LICENSE  _main_files  README.md  search_index.json  _site

- git push to gh-pages branch
* Jekyll website
* README and index
** txt
#+name:readme
#+begin_src R :session *R* :tangle no :exports none :eval no :padline no

This is an Open Notebook (Licence CC-BY-4.0) with some material from other sites (attributed).

In this notebook we will introduce what multi-level data and models are and why they are useful in epidemiology contexts. 

A secure online Rstudio server (in a web browser) is available at [https://rstudio.coesra.org.au](https://rstudio.coesra.org.au) hosted by Uni Queensland's [Collaborative Environment for Ecosystem Science Research and Analysis - CoESRA](http://www.coesra.org.au).

We begin with the general linear model framework GLM and then move to mixed-effects models. It is important to realise that these models are called different things in different disciplines: (generalized) linear mixed (effects) models (GLMM), hierarchical (generalized) linear models, etc. The terminology for the model parameters is equally diverse, usually including the terms random effects and fixed effects. So we focus on the key concepts.

We will cover what is a Random Effect and how it differs from a Fixed effect. Some example syntax in R and Stata will show how to get a handle on models that have random intercepts and additionally, random slopes.

We will spend a little time talking about how to partition variation and get estimates of the random and fixed effects. An important element will be our discussion of similarities between mixed-effects models with basic regression. There will also be brief discussion of extending the regression to non-gaussian responses (GLMERs).

We will discuss how to interpret these results in a population health context.


#+end_src

** README.md
#+HEADERS: :tangle README.md :noweb tangle :padline no
#+BEGIN_SRC markdown
---
layout: default
title: README
---


<<readme>>


#+end_src
** index.md
#+HEADERS: :tangle index.md :noweb tangle :padline no
#+BEGIN_SRC markdown
---
layout: default
title: README
---


<<readme>>


#+end_src

* TBA
#+begin_src R :session *R* :tangle TBA.md :exports none :eval no :padline no
---
layout: default
title: To be arranged
---

This page is not written yet

#+end_src
* 1 Introduction to R
SNIP

<iframe style="border: none;" height="400" width="600" src="./images/20190312_data_inventory.mp4"></iframe>

** introR
*** R codes basics
#+name:introRbasics
#+begin_src R :session *R* :tangle 1_intro_to_R/intro_R_basics.R :exports none :eval no :padline no
## A BRIEF INTRODUCTION TO R ##
# prepared by ChristyGeromboux
#-------------------------------------
# We have seen a brief introduction to RStudio. Now let's use it.

## RUNNING CODE ##
#-------------------------------------
# You can run a single line of code by using Alt-Enter, or
# you can select multiple rows and run them in the same way.
# Commented code won't be executed (# at the begining of the line or '' over multiple lines)


## LIBRARIES ##
#-------------------------------------
# R comes with a base set of libraries preloaded. Let's see them.
search()
# Most other libraries/packages are available on the CRAN website, but can 
# be imported using RStudio.


## GETTING HELP ##
#----------------------------------

# R has very good built in documentation - try ? or help() function  
help(plot)
?mean

# R also has built in examples in the documentation
example(mean)

# R offers a great inbuilt tutorial called swirl
library("swirl")
swirl()
?swirl_options
help(swirl)

# There is also heaps of information on the internet


## INBUILT DATAFRAMES ##
#----------------------------------

# R comes with an inbuilt data frame that you can experiment with - mtcars
# Other datasets can be accessed using the datasets package
library(datasets)

# Calling the data() function will tell you more
data()

# More info can be found here https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html for more


## ASSIGNING AND DELETING VARIABLES ##
#-------------------------------------

# One of the most common things you will want to do is store values in variables
# this is done in two almost equivalent ways, but the first is the recommended way
x <- 5.2
y = "cat"

# You can see the value of the variable by using Alt-enter on the variable name
# or you can look in the environment tab.
x
y

## VARIABLE NAME RULES
#-------------------------------------
# Variables are case sensitive "Cat" is not the same as "cat". 
# When chosing varible names they can be a combination of letters, 
# digits, period (.) and underscore (_).
# They must start with a letter or a period. If they start with a period, 
# they cannot be followed by a digit.
# Reserved words in R cannot be used as identifiers. 
# e.g. for, while, Inf, FALSE, if, etc. For a full list and details
?reserved

# Variables can be deleted using the remove() function
remove(y)
y


## NA, NAN and NULL
#-------------------------------------

# Variables in R can be empty or null. These are treated differently to:
#    0   - the number 0
#    ""  - the empty string
# NA is a place holder for a missing value. It has a length of 1. 
# NAN stands for "Not A Number" - e.g. 0/0
# NULL represents the null object - it can be returned by functions to 
# indicate that the value is unassigned. It has a length of 0.


## R OBJECTS
#-------------------------------------

# Everything in R is an object
# All objects have a class, and other attributed depending on their class


# ATOMIC CLASSES
#-------------------------------------
# There only 5 'atomic' classes

# numerics (double floating point real numbers)
num <- 2
num <- 3.5
num <- 0.4e10

# characters strings
text <- "Hello world"

# logicals (aka booleans) - these are stored as FALSE = 0 and TRUE = 1
i_love_r <- T # or TRUE
i_love_brusselsprouts <- FALSE # or F

# complex/imaginary numbers in the form a + bi
cmplx <- 3+7i
cmplx

# integers
int <- 3L  # Note the L here just denotes an integer. 


## CASTING BETWEEN TYPES
#------------------------------------- 

class(int)
class(num)
x <- as.integer(num)
class(x)

class(num)
x <- as.character(num)
class(x)
x  # When you print the variable x it now has it has quotes

# Sometimes casting happens silently
# integer + numeric = numeric
x <- int + num
class(x)

# imaginary + numeric = imaginary
x <- cmplx + num
class(x)
x

# numeric + boolean = numeric! False = 0, True = 1
x <- num + i_love_r
class(x)
x

# sometimes it will give errors
x <- text + int



## NON ATOMIC CLASSES ##
#-------------------------------------

# There are plenty of objects in R that have non-atomic classes - i.e. they are
# made up of one or more objects. For example:


## VECTORS 
#-------------------------------------
# Vectors consist of zero or more objects of the same class
# A "numeric" vector
x1 <- c(26, 10, 4, 7, 41, 19)
x1

# A "character" vector of country names
x1 <- c("Peru", "Italy", "Cuba", "Ghana")
x1


## LISTS
#-------------------------------------
# these are vectors that have elements of different classes
x<- list(5, "rabbits", FALSE)
x

## MATRICIES
#-------------------------------------
# all the elements in a matrix need to be of the same class
M = matrix(   # Note here that = can be used in a similar fashion to <- 
  c(2, 4, 3, 1, 5, 7), # the data elements  
  nrow=2,              # number of rows 
  ncol=3,              # number of columns 
  byrow = TRUE)        # fill matrix by rows 
M

## DATAFRAMES
#-------------------------------------
# Like matricies, only the elements can be of different classes.
# This is useful for data that consists of variables of different types

## Rasters
## Shapefiles
## Many others 

## BASIC OPERATIONS IN R
#-------------------------------------
# We can use basic operations on variables in R


# COMPARISON OPERATORS
#-------------------------------------

#  equal: ==
x<-2
y<-3
x==y

#  not equal: !=
x!=y

#  greater/less than: > <
x < y

#  greater/less than or equal: >= <=
x<=y


## INFINITY
#-------------------------------------
# R has a built in concept for infinity
x<-1/Inf


## LOGICAL OPERATORS
#-------------------------------------
# AND & Returns TRUE if both comparisons return TRUE.
x <- 1:10    # This creates a numeric vector of the numbers 1-10. Well come to this later.
x
y <- 10:1
y
x > y & x > 5    # This can be read as  (x > y) AND (x > 5)

# OR | Returns TRUE where at least one comparison returns TRUE.
x == y | x != y  # This can be read as (x equal to y) OR (x not equal to y)

# NOT ! The ’!’ sign returns the negation (opposite) of a logical vector.
!x > y  # This can be read as NOT (x > y) - which from maths we know is the same as x <= y

# These return a logical vector of TRUE or FALSE and can be useful for filtering


## CONTROL STRUCTURES IN R
#-------------------------------------
# Like all programming languages R offers various ways to control the 
# flow of execution

## IF / ELSE STATEMENTS
#-------------------------------------
x <- 5
y <- 10
if (x > y) {
  print("x is bigger than")
} else {
  print("x is less than or equal to y")
}

## FOR LOOP
#-------------------------------------
# Iterates through code for a fixed number of times
x<-NA
for (i in 1:27) {
  x<-paste(x," NA")
  if (i == 27) {
    x<-paste(x," BATMAN!")
  }
} 
print(x)

## WHILE LOOP
#-------------------------------------
# Iterates through code while a condition is met. 
# Useful when we don't know how many loops to do - e.g. convergence
current_time<-Sys.time()
current_time
end_time <- current_time+ 5 # 5 seconds
end_time
while (current_time < end_time) {
  print(paste("The time is: ",current_time))
  Sys.sleep(1) # wait for one second
  current_time<-Sys.time()
}

## REPEAT LOOP
#-------------------------------------
# Executes an infinite loop

## NEXT STATEMENT
#-------------------------------------
# Skip to the next iteration of the loop
# print the numbers 1 divided by the integers -5 to 5
for (i in -5:5) {
  if (i==0){  # can't divide by 0
    next()
  }
  print(1/i)
}

## BREAK STATEMENT
#-------------------------------------
# Break the exectution of a loop
i <- 1
convergence_point=0
x<-10
while (x!=convergence_point){
  x <- x + sample(-1:10,1) # add a random number between -1 and 10
  print(paste(i,": ",x,sep = ""));
  i <- i+1;
  if (i>100){
    print("no convergence after 100 iterations")
    break()
  }
  }


## VECTORISED OPERATIONS
#-------------------------------------
# R offers simple ways to add/subtract/muliply/divide 
# vectors by other vecors or scalars
x<-c(1:4)
y<-c(5:8)
x
y

x + y
x * y

a<-10
a * x

# this works well when you are adding or multiplying vectors where the
# number elements in one is a multiple of the number of element in the other
z <- c(10, 100)
x + z
x * z

# What happens here though
z <- c(10, 100, 1000)
x + z  # It works, but note the warning message

x <- seq(-50, 50, by=10)
x
x / 10

x <- -10:10
y <- x %% 2 # here %% = modulus
y


## SUBSETTING IN R
#-------------------------------------
# Non-atomic objects in R can be subset in three basic ways - 
# Single bracket operators []
# this always returns an object the same class as the original object being subset
# double bracket operators [[]]
# this returns a single element from a list or dataframe
# $ operator
# this returns an object of a list or dataframe by name
# this is similar to [[]]
my_list<-list(-1:-5,
              6L,
              5e-4,
              c("cat","dog","mouse"),
              5+3i,
              "hello world",
              TRUE)
my_list
class(my_list)
x<-my_list[2]
x
class(x)  # we started with a list so this should return a list with one element
y<-my_list[[2]]
y
class(y)

# you can also give names to the elements of your list 
# and you can use the $ operator to refer to them
x<- list(num=5, pet="rabbit", eats_meat=FALSE)
my_pet<-x$pet
class(my_pet)

# Note this is not allowed
a<-"pet"
x$a


# SUBSETTING USING LOGICAL VECTORS
#-------------------------------------

x<-c(-10:10)
even_x<- (x %% 2)==0 # here %% = modulus 
even_x # this is a logical vector representing if the values of x are divisible by 2
z<-x[even_x]
z
odd_x<-x[!even_x]
odd_x

# two ways to get the positive numbers 
pos_x <- x[x>0]
pos_x
pos_no <- x>0 # this gives a logical vector
pos_no
pos_x <- x[pos_no]
pos_x


## VIEWING DATA
#-------------------------------------

# R offers a number of ways to summarise and view data
# the str() function gives the structure of an object
str(mtcars)

# the class() function we have seen
class(mtcars)

# the head() and tail() functions allow us to look at the top and bottom rows
head(mtcars)
tail(mtcars, 10)

# the table() function gives the frequency of any given variable
table(mtcars$cyl)

# the dim() function gives the dimentions of the variable
# for matricies and dataframes the dimensions are given as number of rows
# by number of columns
dim(mtcars)

# the summary() function gives 
summary(mtcars)

# the attributes will give more information on the object depending on what it is
attributes(mtcars)



# PLOTTING DATA
#-------------------------------------
# One of the great things about R is how easy it is to plot data

# Read in the dataframe into a variable
cw<-ChickWeight

# Let's take a quick look at the data
head(ChickWeight)
str(ChickWeight)
summary(ChickWeight)

# Now let's create some plots
plot(cw$Diet,cw$weight)

plot(cw$Diet,cw$weight
     , xlab="diet" # x label
     , ylab="weight in grams" # y label
     , main="Chick weight by diet") # title

# just diet 1 over time
diet_1<-cw[cw$Diet==1,]
plot(diet_1$Time,diet_1$weight
     , type="p" # the type of graph see help(plot)
     , xlab="time in days" # x label
     , ylab="weight in grams" # y label
     , main= "Chick weight over time for diet 1"
)

# with some more sophisticated libraries we can get prettier plots
library(ggplot2)
ggplot(cw, aes(Time, weight, colour=Diet)) +
  geom_point() +
  facet_wrap(~ Diet) +
  theme(legend.position = "bottom")

ggplot(cw, aes(Time, weight,
               group=Diet, colour=Diet)) +
  ggtitle("Average chick weight by diet over time")+
  stat_summary(fun.y="mean", geom="line")


## PLOTTING SHAPEFILES
#-------------------------------------
# R can also be used to plot complex spatial data such as polygons
require(rgdal)
project_dir<-getwd()
setwd("POA2016_shapefile")
dat<-readOGR(dsn = ".", layer = "POA_2016_AUST")
str(dat)
head(dat@data)
plot(dat)



## OTHER THINGS YOU CAN DO ##
#-------------------------------------
# This is really just the tip of the iceburg. R can do so much more. e.g.

#  write your own functions
#  execute code in other files
#  connect to databases
#  scrape web pages
#  create web pages or other documents (static or dynamic)
#  create a large variety of data visualisations
#  almost anything you want!

# Please have a go! It is fun.
#+end_src
*** R codes
#+name:introR
#+begin_src R :session *R* :tangle 1_intro_to_R/intro_spatiotemporal_multilevel_models.R :exports none :eval no
  ## Intro to R ##
  # objects
  # e.g. numbers
  a <- 1
  a
  # or characters
  a <- "abc"
  a
  # and vectors
  x <- 1:100
  x
  # math functions and simulating from a range of distributions
  y <- x^2 + sample(rnorm(10000, 0, 1000), 100)
  # and plotting
  plot(x, y)

  # some packages are shipped with the base installation
  library(mgcv)
  # with statistical methods such as generalised additive models and
  # penalised splines
  fit <- gam(y ~ s(x))
  summary(fit)
  plot(fit)

  # there are many other packages on CRAN we can use tests to see if we
  # need to install packages on this computer
  if(!require(season)){
    install.packages("season")
  }; library(season)
  # sometimes with well documented help files
  ??season

  # these can also contain data
  data(CVDdaily)
  ?CVDdaily
  # str means structure
  str(CVDdaily)
  summary(CVDdaily)

  # sometimes you specify different plot types
  plot(CVDdaily$date, CVDdaily$cvd, type='l')
  # other times R knows based on the type of data
  plot(CVDdaily$dow, CVDdaily$cvd)

  # generalised linear models with non-gausian responses
  fit2 <- glm(cvd ~ tmpd + dow, data = CVDdaily, family = "poisson")
  summary(fit2)

  # partial residual plots are fun
  termplot(fit2, terms = 1, se = T)

  # and so are parametric smoothing splines
  library(splines)
  fit3 <- glm(cvd ~ ns(tmpd, df = 4) + dow,
              data = CVDdaily, family = "poisson")
  summary(fit3)

  # partial residual plots are fun
  termplot(fit3, terms = 1, se = T)

  # and post estimation statistics 
  aic_table <- data.frame(aic2 = AIC(fit2), aic3 = AIC(fit3))
  aic_min <- min(aic_table)
  (aic_table - aic_min)
  # model fit3 is substantially better than fit2

  # and diagnostics like cooks distance and leverage
  par(mfrow = c(2,2))
  plot(fit3)

  # and R can read most file formats
  library(foreign)
  # or even stata13/14 files 
  library(readstata13)
  infile <- dir(file.path(system.file(package = "readstata13"), "extdata"), full.names = T, pattern = ".dta")
  infile
  dat <- read.dta13(infile)
  str(dat)

#+end_src

*** monthly-gam
#+name:monthly-gam
#+begin_src R :session *R* :tangle no :exports none :eval no
  'name:monthly-gam'
  if(!require(season)){install.packages("season")}
  library(season)
  if(!require(sqldf)){install.packages("sqldf")}
  library(sqldf)
  library(mgcv)
  data(CVDdaily)
  ?CVDdaily
  str(CVDdaily)
  summary(CVDdaily)
  plot(CVDdaily$date, CVDdaily$cvd, type='l')
  CVDdaily$yy <- as.numeric(substr(as.character(CVDdaily$date), 1,4))
  head(CVDdaily)
  ?aggregate
  dat <- sqldf("select yy, month, sum(cvd) as cvd, avg(o3mean) as o3
   from CVDdaily
  group by yy, month
  order by yy, month")
  dat$time  <- 1:nrow(dat)
  head(dat)
  hist(dat$cvd)
  fit <- gam(cvd ~ o3 + s(month, bs = 'cc', k = 4, fx=T) + s(time, k = 6, fx=T), data = dat,
             family = poisson(link=log))
  summary(fit)
  # edf reports k minus one for s(), looks like k - 2 for cyclic basis
  # bs='cc'
  png("~/season03.png")
  par(mfrow=c(2,2))
  plot(fit, all.terms = T)
  dev.off()
  
#+end_src

** introRmd
*** intro-to-r
#+name:intro-to-r
#+HEADERS: :tangle intro-to-r.md :noweb tangle :padline no
#+BEGIN_SRC markdown
---
layout: default
title: Introduction to R
---


Download the script developed by Christy [here](./1_intro_to_R/intro_R_basics.R)


{% highlight r %}
<<introRbasics>>
{% endhighlight %}

#+end_src

    
* 2 general-regression-vs-mixed-effects 
** COMMENT compare
#+begin_src R :session *R* :tangle general-regression-vs-mixed-effects.md :exports none :eval no :padline no
---
layout: default
title: General regression versus mixed-effects models
---

We will cover what is a Random Effect and how it differs from a Fixed effect. Some example syntax in R (on a secure website portal we have configured) and Stata will show how to get a handle on models that have random intercepts and additionally, random slopes.

We will spend a little time talking about how to partition variation and get estimates of the random and fixed effects. An important element will be our discussion of similarities between mixed-effects models with basic regression. There will also be brief discussion of extending the regression to non-gaussian responses (GLMERs).

## Suggested workflow

### intercepts_and_slopes_in_general_regression.R

### intercepts_and_slopes_in_mixed_effects.R

### general_regression_vs_mixed_effects.R

### Check out make-simulated-data.R to see how the data were constructed

#+end_src

** Terminology

When discussing multilevel modelling in this workshop we will avoid using the names 'fixed effects' and 'random effects'. Instead we use the more verbose phrase: intercepts/coefficients that are common across groups and intercepts/coefficients that vary by group.  This is because, as noted in the introduction to Gelman and Hill 2007 (recommended reading) it is very problematic that these terms are used in imprecise and confusing ways that vary across disciplines. This concern also appears in the work of the rstanarm package developers and in the vignette 'Estimating Generalized Linear Models with Group-Specific Terms with rstanarm' who state:

'Models with this structure are referred to by many names: multilevel models, (generalized) linear mixed (effects) models (GLMM), hierarchical (generalized) linear models, etc. The terminology for the model parameters is equally diverse... [and commonly used terms] are not only misleading but also defined differently across the various fields in which these models are applied.'

** COMMENT Notes
*** Data structure
I was interested to read in Gelman and Hill (Data analysis using regression and multilevel /hierarchical models), that they specify breaking multilevel data into tables that reflect the nested structure of variables in the study rather than a long format with repetition of the values of group variables (Fig 11.2-11.3 attached). I found it interesting because the text struck me as a recommendation for _housing_ data this way rather than as a pre-analysis, data manipulation step. The authors message was (whether you agree with it or not): two tables (reflecting the appropriate nested data) are easier for the user to: a) understand the structure of the data; and b) analyse the data.  

** Introduce the linear model and lmer
** COMMENT Code
THis is made originally in index2.org, statistical modelling, ** 2015-09-12-effect-modification-and-mediation, after brambor2006
*** COMMENT make_simulated_data
#+name:make-simulated-data
#+begin_src R :session *R* :tangle multi-level-basics/code/make_simulated_data.R :exports none :eval no
  #### name:make_simulated_data ####
  setwd("multi-level-basics")
  x  <- seq(0, 6, by = 0.05)
  set.seed(42)
  y <- -0.5 + sample(rnorm(length(x),0,0.05))
  set.seed(123)
  y2 <- (-0.33 + 0.1*x) + sample(rnorm(length(x),0,0.05))
  set.seed(1)
  y3 <- (-0.5 + 0.1*x) + sample(rnorm(length(x),0,0.05))
  
  dat <- data.frame(y = y, x, z = 'group1')
  dat <- rbind(dat, data.frame(y = y2, x, z = 'group2'))
  dat <- rbind(dat, data.frame(y = y3, x, z = 'group3'))
  str(dat)
  write.csv(dat, "data/simulated_data.csv", row.names = F)
  
#+end_src
*** make-simulated-data md
#+HEADERS: :tangle make-simulated-data.md :noweb tangle :padline no
#+BEGIN_SRC markdown
---
layout: default
title: Make simulated data
---

We simulate some data

{% highlight r %}
<<make-simulated-data>>
{% endhighlight %}



#+END_SRC

*** COMMENT brambor
#+name: brambor
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:after_brambor ####
  
  x  <- seq(0, 6, by = 0.05)
  
  set.seed(42)
  y <- -0.5 + sample(rnorm(length(x),0,0.05))
  set.seed(42)
  y2 <- (-0.33 + 0.1*x) + sample(rnorm(length(x),0,0.05))
  
  
  #pdf("effect_modification.pdf")#,  width = 1000, height = 550)
  par(mar = c(3,3,2,1))
  plot(1, type="n", xlim=c(-0.3,6), ylim=c(-1,0.5), xlab="", ylab="", axes = F)
  axis(1, labels = F); axis(2, labels = F)
  mtext("Y", 2, 1 , at = 0, las = 2)
  mtext("X", 1, 1 , at = 3)
  title(expression(paste("Regression model Y = ", beta[0] + beta[1],"X + ", beta[2], "Z + ", beta[3], "XZ + ", epsilon )))
  #, 3, 0, at = 1, cex = .85)
  #segments(-3, -0.5, 3, -0.5, lwd=2)    
  points(x,y, pch = 1, cex = .5)
  fit <- lm(y~x)
  summary(fit)
  abline(fit)
  
  
  #curve(exp=-.008*x + .137*2  + .100*2*x,  from=-3, to=3, lwd=2, add=TRUE)
  points(x, y2, pch = 3, cex = .5)
  fit2  <- lm(y2~x)
  summary(fit2)
  abline(fit2)
  
  text(x = 2.8, y = 0.2, expression(paste("Y = (", beta[0] + beta[2],") + (", beta[1] + beta [3],")X when Z = 1")))
  text(x = 1.9, y = 0.1, expression(paste("Slope = ", beta[1] + beta[3])))
  
  #text(x = -3.4, y = -0.4, '{', srt = 0, cex = 3)
  text(x = -0.3, y = -.75, expression(beta[0]))
  text(x = -0.3, y = -0.4, expression(beta[2]))
  b0 <- coefficients(fit)[1]
  b2 <- coefficients(fit2)[1]
  text(x = 3.57, y = -0.7,  expression(paste("Y = ", beta[0] + beta[1] ,"X when Z = 0")))
  text(x = 3, y = -0.8, expression(paste("Slope = ", beta[1])))
  
  segments(0, b0, 0, b2, lty = 1, col = 'grey', lwd = 6)
  segments(-.9, b2, 0, b2, lty = 3)
  segments(-.9, b0, 0, b0, lty = 3)
  #mtext(expression(beta[2]),2,2, at = -0.4, las = 2)
  #text(x = -3.4, y = -.75, '{', srt = 0, cex = 3)
  
  segments(0, b2, 0, -1, lty = 3)
  #mtext(expression(beta[0]),2,2, at = -0.75, las = 2)
  #dev.off()
  #browseURL("effect_modification.pdf")
#+end_src

#+RESULTS: after_brambor

*** COMMENT after_brambor
#+name: after_brambor
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name: multilevel model with varying intercepts and varying slopes ####
  library(lme4)
  dat <- read.csv("simulated_data.csv")
  # step one: exploratory work
  str(dat)
  summary(dat)
  plot(dat$x, dat$y)
  plot(dat$z, dat$y)
  plot(density(dat$y))
  
  fit0 <- lm(y ~ x, data = dat)
  summary(fit0)
  fit0cf <- coefficients(fit0)
  with(dat, plot(x, y, col = z))
  
  abline(fit0cf[1], fit0cf[2])
  
  # step 2: we suspect a group level effect! begin to build multilevel model(s)
  # first lets set up a nice plot
  my_plot <- function(dat){
  
    par(mar = c(3,3,2,1))
    plot(1, type="n", xlim=c(-0.3,6), ylim=c(-1,0.5), xlab="", ylab="", axes = F)
    axis(1, labels = F); axis(2, labels = F)
    mtext("Y", 2, 1 , at = 0, las = 2)
    mtext("X", 1, 1 , at = 3)
  
    with(subset(dat, z == "group1"), points(x, y,  pch = 1, cex = .5))
    with(subset(dat, z == "group2"), points(x, y, pch = 3, cex = .5))
    with(subset(dat, z == "group3"), points(x, y, pch = 4, cex = 1.5, col = 'darkgreen'))
  
  }
  
  my_plot(dat)
  #dev.off()
  
  #  model 1: OLS with unvarying intercept and slope (fixed)
  fit <- lm(y~x, data = dat)
  summary(fit)
  abline(fit, lwd = 2, lty = 2)
  
  # model 2: varying intercept (random) and unvarying slope (fixed)
  my_plot(dat)
  fit2  <- lm(y ~ x + z, data = dat)
  summary(fit2)
  # we know this is put together in a linear equation thus:
  title(expression(paste("Regression model Y = ", beta[0] + beta[1],"X + ", beta[2], "Z + ", epsilon )))
  
  # so create a coeffs object
  fit2cf <- coefficients(fit2)
  
  # now we can extract the betas
  b0   <- fit2cf[1]
  b1   <- fit2cf[2]
  b2.1 <- fit2cf[3]
  b2.2 <- fit2cf[4]
  
  # and show the linear fits
  abline(b0, b1)
  abline(b0+b2.1, b1)
  abline(b0+b2.2, b1)
  
  # TODO maybe stop to talk about the tests of 'if group makes a difference?' here, which justify going the next
  
  #######################################################################################
  #### model 3: LM approach to varying intercept (random) and varying slope (random) ####
  #######################################################################################
  # clear out the plot
  my_plot(dat)
  fit3  <- lm(y ~ x * z, data = dat)
  summary(fit3)
  
  # this is a different equation
  title(expression(paste("Regression model Y = ", beta[0] + beta[1],"X + ", beta[2], "Z + ", beta[3], "XZ + ", epsilon )))
  
  # there is a trick to make this less work with getting se around multiplicative interaction terms... for lesson 3!
  # x1 <- ifelse(dat$z == 'group1', x, 0)
  # x2 <- ifelse(dat$z == 'group2', x, 0)
  # x3 <- ifelse(dat$z == 'group3', x, 0)
  #
  # fit3 <- lm(y ~ z + x1 + x2 + x3, data = dat)
  # summary(fit3)
  fit3cf <- coefficients(fit3)
  b0   <- fit3cf[1]
  b1   <- fit3cf[2]
  b2.1 <- fit3cf[3]
  b2.2 <- fit3cf[4]
  b3.1 <- fit3cf[5]
  b3.2 <- fit3cf[6]
  
  abline(b0, b1)
  abline(b0+b2.1, b1+b3.1)
  abline(b0+b2.2, b1+b3.2, col = 'darkgreen')
  
  # at this point may expect a question about change in X = estimated change in Y for each
  
  ######################################################################################################
  #### model 5 is linear mixed effects regression (lmer) with varying intercept and unvarying slope ####
  ######################################################################################################
  my_plot(dat)
  # all the tutes I read said to start with a 'null' random intercepts model
  fitnull <- lmer(y ~ (1|z), data = dat)
  summary(fitnull)
  random <- ranef(fitnull)
  fixed <- fixef(fitnull)
  gamma1 <- fixed[1] + random[[1]][1,1]  # this is beta0 + mu0 where Z = group1
  gamma2 <- fixed[1] + random[[1]][2,1]  # this is beta0 + mu0 where Z = group2
  gamma3 <- fixed[1] + random[[1]][3,1]  # this is beta0 + mu0 where Z = group3
  
  abline(gamma1, 0, col = 'red', lty = 'dashed')
  abline(gamma2, 0, col = 'red', lty = 'dashed')
  abline(gamma3, 0, col = 'red', lty = 'dashed')
  
  # versus the LM version
  fitnull1 <- lm(y ~ 1 + z, data = dat)
  summary(fitnull1)
  
  
  # but we know we have slopes so lets get going
  my_plot(dat)
  fit5 <- lmer(y ~ x + (1|z), data = dat)
  summary(fit5)
  
  title(expression(paste("Regression model Y = ", beta[0] + beta[1],"X + (", mu[0], "Z + ", epsilon, ")")))
  
  # we can extract the components thus
  random <- ranef(fit5)
  random
  fixed <- fixef(fit5)
  fixed
  
  gamma1 <- fixed[1] + random[[1]][1,1]  # this is beta0 + mu0 where Z = group1
  gamma2 <- fixed[1] + random[[1]][2,1]  # this is beta0 + mu0 where Z = group2
  gamma3 <- fixed[1] + random[[1]][3,1]  # this is beta0 + mu0 where Z = group3
  beta_x <- unlist(fixed)[2]
  
  abline(gamma1, beta_x, col = 'red', lty = 'dashed')
  abline(gamma2, beta_x, col = 'red', lty = 'dashed')
  abline(gamma3, beta_x, col = 'red', lty = 'dashed')
  
  # model 6  is lmer with varying intercept and varying slope
  my_plot(dat)
  fit6 <- lmer(y ~ x + (1 + x |z), data = dat)
  summary(fit6)
  
  title(expression(paste("Regression model Y = ", beta[0] + beta[1],"X + (", mu[0], "Z + ", mu[1], "X + ", epsilon, ")")))
  
  random <- ranef(fit6)
  random
  fixed <- fixef(fit6)
  fixed
  
  gamma1 <- fixed[1] + random[[1]][1,1]
  gamma2 <- fixed[1] + random[[1]][2,1]
  gamma3 <- fixed[1] + random[[1]][3,1]
  beta_x1 <- fixed[2] + random[[1]][1,2]
  beta_x2 <- fixed[2] + random[[1]][2,2]
  beta_x3 <- fixed[2] + random[[1]][2,2]
  
  abline(gamma1, beta_x1, col = 'red', lty = 'dashed')
  abline(gamma2, beta_x2, col = 'red', lty = 'dashed')
  abline(gamma3, beta_x3, col = 'red', lty = 'dashed')
  
#+end_src

* 3 schools
** introRm
#+HEADERS: :tangle schools.md :noweb tangle :padline no
#+BEGIN_SRC markdown
---
layout: default
title: Schools
---

- 3-1-centre-for-MLM-Tut-1
- 3-2-Aust-School-PA-OWOB

#+end_src

 
* 4 spatial-data-analysis 
Intercepts and coefficients that vary by group
** COMMENT spatial-data-analysis
#+begin_src R :session *R* :tangle spatial-data-analysis.md :exports none :eval no :padline no
---
layout: default
title: Spatial data analysis
---

We look at intercepts and coefficients that vary by geographic groups.

In this example we explore the Ross River virus infections from Western Australia.

#+end_src

** Ross River virus and urban vs rural mosquito habitat 
*** COMMENT Notes made after hacking in the origi vally repo
See do_model_checking_glmer


#########################
# model 4 is multilevel
** Standardised rates
/home/ivan_hanigan/projects/RatesTheory

* 5 your own data
* 6 Spatiotemporal data analysis
** Spatially structured timeseries 
I will use the NMMAPSlite datasets for a simple example of what I describe as "Spatially Structured Timeseries" as opposed to "Spatio-Temporal" which I think more explicitly includes spatial structure in the model.
** Outcome Data
*** Original Data
# NB The original data are not available anymore
#+name:md
#+begin_src R :tangle no :exports reports :eval no :tangle NMMAPS-example/NMMAPS-original-data.r 
"
http://cran.r-project.org/web/packages/NMMAPSlite/index.html
Package ‘NMMAPSlite’ was removed from the CRAN repository.
Formerly available versions can be obtained from the archive.
Archived on 2013-05-11 at the request of the maintainer.

The Archived versions do not seem to work either.

That is such a shame, lucky I saved some of the data using the following code:
"
#+end_src




#+begin_src R :session *R* :tangle NMMAPS-example/NMMAPS-original-data.r :exports reports :eval no
   
      #### Code: get nmmaps data
      # func
      if(!require(NMMAPSlite)) install.packages('NMMAPSlite');require(NMMAPSlite)

      ######################################################
      # load  
      setwd('data')
      initDB('data/NMMAPS') # this requires that we connect to the web,
                            # so lets get local copies
      setwd('..')
      cities <- getMetaData('cities')
      head(cities)
      citieslist <- cities$cityname
      # write out a few cities for access later
      for(city_i in citieslist[sample(1:nrow(cities), 9)])
      {
       city <- subset(cities, cityname == city_i)$city
       data <- readCity(city)
       write.table(data, file.path('data', paste(city_i, '.csv',sep='')),
       row.names = F, sep = ',')
      }
      # these are all tiny, go some big ones
      for(city_i in c('New York', 'Los Angeles', 'Madison', 'Boston'))
      {
       city <- subset(cities, cityname == city_i)$city
       data <- readCity(city)
       write.table(data, file.path('data', paste(city_i, '.csv',sep='')),
       row.names = F, sep = ',')
      }
#+end_src
*** Pooled Dataset
#+name:Pooled Dataset
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:Pooled Dataset
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")

  flist <- dir("data", full.names=T)
  
  flist <-    flist[which(basename(flist) %in%
                          c("Baton Rouge.csv",
                            "Los Angeles.csv",
                            "Tucson.csv",
                            "Denver.csv")
                          )
                    ]
  flist
  for(f_i in 1:length(flist))
      {
          #f_i <- 2
          fi <- flist[f_i]
          df <- read.csv(fi)
          df <- df[,c("city","date", "agecat",
                      "cvd", "resp", "tmax",
                      "tmin", "dptp")]
          # str(df)
          write.table(df,
                      "outcome.csv", sep = ",",
                      row.names = F, append = f_i > 1,
                      col.names = f_i ==1
                      )
      }
  
#+end_src

** Exposure Data
** Zones
*** Map Shapefile Code
#+name:zones
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:zones
  # func
  setwd("~/Dropbox/projects/spatiotemporal-regression-models/NMMAPS-example")
  #require(devtools)
  #install_github("gisviz", "ivanhanigan")
  require(gisviz)
  
  # load
  flist <- dir("data")
  flist
  # do    
  ## geocode
  flist <- gsub(".csv", "", flist)    
  flist_geo <- gGeoCode2(flist)
  flist_geo
  
  ## plot
  ## png("images/nmmaps-eg-cities.png")
  ## plotMyMap(
  ##     flist_geo[,c("long","lat")],
  ##     xl = c(-130,-60), yl = c(25,50)
  ##     )
  ## text(flist_geo$long, flist_geo$lat, flist_geo$address, pos=3)
  ## dev.off()
     
  # load city names
  flist2 <- dir("data")
  flist2
  city_codes <- matrix(NA, nrow = 0, ncol = 2)
  for(fi in 1:length(flist2))
      {
          # fi <- 1
          fname <- flist2[fi]
          print(fi); print(fname);
          df <- read.csv(
              file.path("data", fname),
              stringsAsFactors = F, nrow = 1)
          city_codes <- rbind(city_codes,
              c(gsub(".csv","",fname), df$city)
              )
      }
  city_codes <- as.data.frame(city_codes)
  names(city_codes) <- c("address","city")
  city_codes 
  flist_geo2 <- merge(flist_geo, city_codes, by = "address")
  flist_geo2
  
  ## make shapefile
  epsg <- make_EPSG()
  prj_code <- epsg[grep("WGS 84$", epsg$note),]
  prj_code
  shp <- SpatialPointsDataFrame(cbind(flist_geo2$long,flist_geo2$lat),flist_geo2,
                                proj4string = CRS(
                                    epsg$prj4[which(epsg$code ==prj_code$code)]
                                    )
                                )
  writeOGR(shp, 'cities.shp', 'cities', driver='ESRI Shapefile')
  
#+end_src
*** Map Shapefile 2
#+name:Map Shapefile 2
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:Map Shapefile 2 ####
  dir("data")
  flist_geo <- readOGR("data", "cities")
  png("images/nmmaps-eg-cities.png")
  plotMyMap(flist_geo,
      xl = c(-130,-60), yl = c(25,50)
      )
  text(flist_geo$long, flist_geo$lat, flist_geo$address, pos=3)
  dev.off()
  
#+end_src

*** Map Shapefile Output
[[file:NMMAPS-example/images/nmmaps-eg-cities.png]]

** Population Data
*** Population data 
**** A Code Skeleton
The process for getting the population data from the websites is a bit of a pain, with repeated copy and paste operations.  To make sure this is recorded I set up a skeleton to paste into.
**** TODO better to use gsub() to remove the ","
#+name:population-data
#+begin_src R :session *shell* :tangle no :exports reports :eval no
  ################################################################
  # name:population-data
  
  # func
  setwd("~/projects/spatiotemporal-regression-models")
  require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  cityname <- "SELECTED CITY"
  
  # do
  # save url
  # xxx
  population_input <- read.table(textConnection(
  "age: population
  xxx
  "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src


**** Baton Rouge
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval yes
  ################################################################
  # name:population-data
  # first city, remove population data.frame
  rm(population)
  
  # func
  setwd("~/projects/spatiotemporal-regression-models")
  require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  cityname <- "Baton Rouge"
  
  # do
  # save url
  # http://www.city-data.com/us-cities/The-South/Baton-Rouge-Population-Profile.html
  population_input <- read.table(textConnection(
  "age: population
  Population Under 5 years: 15502
  Population 5 to 9 years: 15609
  Population 10 to 14 years: 15248
  Population 15 to 19 years: 21954
  Population 20 to 24 years: 27230
  Population 25 to 34 years: 31719
  Population 35 to 44 years: 30343
  Population 45 to 54 years: 27166
  Population 55 to 59 years: 9495
  Population 60 to 64 years: 7490
  Population 65 to 74 years: 13312
  Population 75 to 84 years: 9611
  Population 85 years and over: 3139
  "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src

**** Los Angeles
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:population-data
  
  # func
  #setwd("~/projects/spatiotemporal-regression-models")
  #require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  cityname <- "Los Angeles"
  
  # do
  # save url
  # http://www.city-data.com/us-cities/The-West/Los-Angeles-Population-Profile.html
  population_input <- read.table(textConnection(
  "age: population
  Population under 5 years old: 285976
  Population 5 to 9 years old: 297837
  Population 10 to 14 years old: 255604
  Population 15 to 19 years old: 251632
  Population 20 to 24 years old: 299906
  Population 25 to 34 years old: 674098
  Population 35 to 44 years old: 584036
  Population 45 to 54 years old: 428974
  Population 55 to 59 years old: 143965
  Population 60 to 64 years old: 115663
  Population 65 to 74 years old: 18711
  Population 75 to 84 years old: 125829
  Population 85 years and over: 44189
  "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src


**** Tucson
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:population-data
  
  # func
  #setwd("~/projects/spatiotemporal-regression-models")
  #require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  cityname <- "Tucson"
  
  # do
  # save url
  # http://www.city-data.com/us-cities/The-West/Tucson-Population-Profile.html
  population_input <- read.table(textConnection(
  "age: population
  Population under 5 years old: 35201
  Population 5 to 9 years old: 34189
  Population 10 to 14 years old: 31939
  Population 15 to 19 years old: 38170
  Population 20 to 24 years old: 47428
  Population 25 to 34 years old: 76394
  Population 35 to 44 years old: 72289
  Population 45 to 54 years old: 57608
  Population 55 to 59 years old: 19597
  Population 60 to 64 years old: 16056
  Population 65 to 74 years old: 29117
  Population 75 to 84 years old: 21394
  Population 85 years and older: 7317
  "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src

**** Denver
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:population-data
  
  # func
  #setwd("~/projects/spatiotemporal-regression-models")
  #require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  cityname <- "Denver"
  
  # do
  # save url
  # http://www.city-data.com/us-cities/The-West/Denver-Population-Profile.html
  
  population_input <- read.table(textConnection(
  "age: population
  Population under 5 years old: 37769
  Population 5 to 9 years old: 34473
  Population 10 to 14 years old: 31315
  Population 15 to 19 years old: 32259
  Population 20 to 24 years old: 45534
  Population 25 to 34 years old: 113676
  Population 35 to 44 years old: 86420
  Population 45 to 54 years old: 71000
  Population 55 to 59 years old: 22573
  Population 60 to 64 years old: 17191
  Population 65 to 74 years old: 30643
  Population 75 to 84 years old: 23369
  Population 85 years and over: 8414
  "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src
**** Louisville
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:population-data
  
  # func
  setwd("~/projects/spatiotemporal-regression-models")
  require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  
  cityname <- "Louisville"
  
  # do
  # save url
  #http://www.city-data.com/us-cities/The-South/Louisville-Population-Profile.html
  
  ## NB error in table, 75-84 was missing.  used same from baton rouge
  population_input <- read.table(textConnection(
    "age: population
    Population under 5 years old: 16926
    Population 5 to 9 years old: 17359
    Population 10 to 14 years old: 16627
    Population 15 to 19 years old: 17362
    Population 20 to 24 years old: 18923
    Population 25 to 34 years old: 37541
    Population 35 to 44 years old: 40354
    Population 45 to 54 years old: 33755
    Population 55 to 59 years old: 10716
    Population 60 to 64 years old: 9211
    Population 65 to 74 years old: 18577
    Population 75 to 84 years: 9611
    Population 85 years and older: 5075
    "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src


**** Save Population Dataset
#+name:save-population
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:save-population
  write.csv(population, "population.csv",
            row.names = FALSE
            )
#+end_src

*** Population Summary 
#+name:Population Summary
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:Population Summary
  # func
  require(plyr)
  
  # load
  population <- read.csv("population.csv")
  str(population)
  population <- population[,c("city", "agecat", "population")]
  # do
  population_summary <- ddply(population,
                             .variables = c("city",
                                 "agecat"),
                             .fun = summarise,
                             sum(population)
                             )
  
  names(population_summary) <- c("city","agecat","pop")
  
  write.csv(population_summary,
            "population_summary.csv",
            row.names = FALSE
            )
#+end_src

** Merge

#+name:merge
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:merge
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
  
  # load
  outcome <- read.csv("outcome.csv")
  str(outcome)
  outcome$date <- as.Date(outcome$date)
  
  population <- read.csv("population_summary.csv")
  str(population)
  population
  
  # do
  analyte <- merge(outcome, population, by = c("city", "agecat"))
  analyte <- arrange(analyte, city, date, agecat)
  # check
  subset(analyte, date == as.Date("1990-01-01"))
  
  # save
  write.csv(analyte, "analyte.csv", row.names = FALSE)
#+end_src

** Data Checking
*** Check Assumption of Proportional Hazards
#+begin_src R :session *R* :tangle NMMAPS-example/case-study-1-data-checking.r :exports reports :eval no
  # Checking the proportional hazards assumption for Indirect Age Standardisation
  
  # Background
  ## Indirect SMRs from different index/study populations are not strictly comparable
  ## because they are calculated using different weighting schemes that
  ## depend upon the age structures of the index/study populations
  ## (see http://www.statsdirect.com/webhelp/#rates/smr.htm).
  
  ## Indirect SMRs can be compared if you make the assumption that the
  ## ratio of rates between index and reference populations is constant;
  ## this is similar to the assumption of proportional hazards in Cox
  ## regression (Armitage and Berry, 1994).
  
  ## So we need to check if the rate ratio of the study population(s)
  ## compared to the standard population varies substantially with age.
  ## If not the proportional hazards assumption holds for the standard
  ## rates compared with the observed rates and the Indirect SMRs are
  ## comparable.  To do this check we calculate the Annualised Age Specific
  ## Rates for our study areas and for our standard for several years at
  ## periodic timepoints across the study period, and then calculate the
  ## ratio of these at each timepoint we could reassure our selves that
  ## this assumption holds.
  
  ## An additional issue arises when there are non-negligible differences
  ## in the age distributions of the study population(s) and the standard
  ## population. In this situation, indirect standardisation produces
  ## biased results due to residual confounding by age
  
  ## Also see Australian Institute of Health and Welfare. (2011). Principles on
  ## the use of direct age-standardisation in administrative data
  ## collections For measuring the gap between Indigenous and
  ## non-Indigenous Australians. Data linkage series. Cat. no. CSI 12.
  ## Canberra: AIHW. Retrieved from
  ## http://www.aihw.gov.au/publication-detail/?id=10737420133
  
  ################################################################
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
  require(plyr)
  
  # load
  analyte <- read.csv("analyte.csv")
  
  # clean
  head(analyte)
  analyte$yy  <- substr(analyte$date, 1, 4)
    
  # do
  ## first we want to see if the age specific rates vary across the
  ## study sites
  disease_colname <- "cvd"
  pop_colname <- "pop"
  by_cols  <- c("city", "agecat", "yy")
  
  stdysites  <- ddply(analyte, by_cols,
                      function(df) return(
                        c(observed = sum(df[,disease_colname]),
                          pop = mean(df[,pop_colname]),
                          crude.rate = sum(df[,disease_colname])/mean(df[,pop_colname])
                          )
                        )
                      )
  ## check
  head(stdysites)
  subset(stdysites, yy == 1987 & city == "batr")
  
  ## define the subset we will use just the deaths and pop in 2000
  stdy <- subset(stdysites, yy == 2000)
  stdy
  
  ## now define the standard population as the entire country
  standard  <- ddply(stdy, c("agecat"),
                       function(df) return(
                         c(observed = sum(df[,"observed"]),
                           pop = sum(df[,"pop"]),
                           crude.rate = sum(df[,"observed"])/
                             sum(df[,"pop"])
                           )
                         )
                      )
  standard
  
  ## Merge the studysites and the standard
  stdyByStnd  <- merge(stdy, standard, by = "agecat")
  stdyByStnd
  
  ## plot the rate ratios
  png("images/ratio-stdy-by-stnd.png")
  mp <- barplot(stdyByStnd$crude.rate.x/stdyByStnd$crude.rate.y)
  text(mp, par("usr")[3],
       labels = paste(stdyByStnd$agecat,stdyByStnd$city),
       srt = 45, adj = c(1.1,1.1), xpd = TRUE
       )
  abline(1,0)
  dev.off()
  
  ## we can see that LA has an issue with the 65to74 agecat
  
  ## Second we will check if ratio of the proportions in each population
  ## agecat vary between study sites and the standard
  totals <- ddply(stdyByStnd, c("city"), summarise,
                  sum(pop.x)
                  )
  totals <- merge(stdyByStnd[,c("city","agecat","pop.x")], totals)
  totals$pop.wt  <- totals[,3] / totals[,4]
  totals <- arrange(totals, city, agecat)
  totals
  
  totalsStnd <- ddply(stdyByStnd, c("agecat"), summarise,
                  sum(pop.x)
                  )
  totalsStnd$totalPop <- sum(totalsStnd[,2])
  totalsStnd$pop.wt.total <- totalsStnd[,2]/totalsStnd[,3]
  totalsStnd
  
  ## merge these so we can look at the ratios
  totals2 <- merge(totals, totalsStnd, by = "agecat")
  totals2 <- arrange(totals2, agecat, city)
  
  ## now plot the ratios
  png("images/ratio-stdy-by-stnd-pops.png")
  mp <- barplot(totals2$pop.wt/totals2$pop.wt.total)
  text(mp, par("usr")[3],
       labels = paste(totals2$agecat,totals2$city),
       srt = 45, adj = c(1.1,1.1), xpd = TRUE
       )
  abline(1,0)
  dev.off()
  
  ## and we can see that there are far fewer 65to74 aged persons in LA
  ## than expected.
  
  
#+end_src

*** Incidence Rate Ratios Plot
[[file:NMMAPS-example/images/ratio-stdy-by-stnd.png]]
*** Population Rate Ratios Plot
[[file:NMMAPS-example/images/ratio-stdy-by-stnd-pops.png]]
    
** Exploratory Data Analyses
*** Time-series Plots Code
#+name:eda-tsplots
#+begin_src R :session *R* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:eda-tsplots
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
    
  # load
  flist <- dir("data")
  fname <- flist[7]; print(fname)
  df <- read.csv(file.path("data", fname))
  
  # clean
  str(df)
  summary(df$tmax); summary(df$dptp)
  
  # do
  ## we will only consider cities with long periods temp and humidity observed
  png("images/nmmaps-eg-dateranges.png", width = 1000, height=500, res = 150)
  par(mfrow=c(6,5), mar=c(0,3,3,0), cex=.25)
  for(fi in 1:length(flist))
      {
          #fi <- 4
          fname <- flist[fi]
          df <- read.csv(file.path("data", fname))
          print(fi); print(fname);
          with(df, plot(as.Date(date), tmax, type = "l"))
          title(paste(fname, "tmax"))
          with(df, plot(as.Date(date), dptp, type = "l"))
          title(paste(fname, "dptp"))
      }
  dev.off()
  
#+end_src
*** Time-series Plots Output
[[file:NMMAPS-example/images/nmmaps-eg-dateranges.png]]

** Main Analyses
*** Core Model
#+name:core
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-do.r :exports reports :eval no
  ################################################################
  # name:core
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
  require(mgcv)
  require(splines)
  
  # load
  analyte <- read.csv("analyte.csv")
  
  # clean
  analyte$yy <- substr(analyte$date,1,4)
  numYears<-length(names(table(analyte$yy)))
  analyte$date <- as.Date(analyte$date)
  analyte$time <- as.numeric(analyte$date)
  analyte$agecat <- factor(analyte$agecat,
                            levels = c("under65",
                                "65to74", "75p"),
                            ordered = TRUE
                            )
  
  # do
  fit <- gam(cvd ~ s(tmax) + s(dptp) +
             city + agecat +
             s(time, k= 7*numYears, fx=T) +
             offset(log(pop)),
             data = analyte, family = poisson
             )
  
  # plot of response functions
  png("images/nmmaps-eg-core.png", width = 1000, height = 750, res = 150)
  par(mfrow=c(2,3))
  plot(fit, all.terms = TRUE)
  dev.off()
  
  
#+end_src
*** Core Model Plots
[[file:NMMAPS-example/images/nmmaps-eg-core.png]]

*** Model Selection
#+begin_src R :session *R* :tangle NMMAPS-example/case-study-1-model-selection.r :exports reports :eval no
     
  # The following codes are just a dump of stuff I've found useful in the past.
  # This is all time-series methods.
  # TODO replace the city-specific model with a pooled analysis with city specific trend.
  
  # func
  require(mgcv)
  require(splines)
    
  ######################################################
  # load  
  dir("data")
  city <- "Chicago"
  data <- read.csv(sprintf("data/%s.csv", city), header=T)
  str(data)
  data$yy <- substr(data$date,1,4)
  data$date <- as.Date(data$date)
  ######################################################
  # check
  par(mfrow=c(2,1), mar=c(4,4,3,1))
  with(subset(data[,c(1,15:25)], agecat == '75p'),
    plot(date, tmax)
   )
  with(subset(data[,c(1,4,15:25)], agecat == '75p'),
          plot(date, cvd, type ='l', col = 'grey')
          )
  with(subset(data[,c(1,4,15:25)], agecat == '75p'),
          lines(lowess(date, cvd, f = 0.015))
          )
  # I am worried about that outlier
  data$date[which(data$cvd > 100)]
  # [1] "1995-07-15" "1995-07-16"
   
  ######################################################
  # do standard NMMAPS timeseries poisson GAM model
  numYears<-length(names(table(data$yy)))
  df <- subset(data, agecat == '75p')
  df$time <- as.numeric(df$date)
  fit <- gam(cvd ~ s(pm10tmean) + s(tmax) + s(dptp) + s(time, k= 7*numYears, fx=T), data = df, family = poisson)
  # plot of response functions
  par(mfrow=c(2,2))
  plot(fit)
  dev.off()
   
  ######################################################
  # some diagnostics
  summary(fit)
  # note the R-sq.(adj) =   0.21
  gam.check(fit)
  # note the lack of a leverage plot.  for that we need glm
   
  ######################################################
  # do same model as glm
  fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = poisson)
  # plot responses
  par(mfrow=c(2,2))
  termplot(fit2, se =T)
  dev.off()
   
  # plot prediction
  df$predictedCvd <- predict(fit2, df, 'response')
  # baseline is given by the intercept
  fit3 <- glm(cvd ~ 1, data = df, family = poisson)
  df$baseline <-  predict(fit3, df, 'response')
  with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
   plot(date, cvd, type ='l', col = 'grey')
          )
  with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
          lines(date,predictedCvd)
          )
  with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
   lines(date,baseline)
          )
  ######################################################
  # some diagnostics
  # need to load a function to calculate poisson adjusted R squared
  # original S code from
  # The formula for pseudo-R^2 is taken from G. S. Maddalla,
  # Limited-dependent and Qualitative Variables in Econometrics, Cambridge:Cambridge Univ. Press, 1983. page 40, equation 2.50.
  RsquaredGlm <- function(o) {
   n <- length(o$residuals)
   ll <- logLik(o)[1]
   ll_0 <- logLik(update(o,~1))[1]
   R2 <- (1 - exp(((-2*ll) - (-2*ll_0))/n))/(1 - exp( - (-2*ll_0)/n))
   names(R2) <- 'pseudo.Rsquared'
   R2
   }
  RsquaredGlm(fit2)
  # 0.51
  # the difference is presumably due to the arguments about how to account for unexplainable variance in the poisson distribution?
   
  # significance of spline terms
  drop1(fit2, test='Chisq')
  # also note AIC. best model includes all of these terms
  # BIC can be computed instead (but still labelled AIC) using
  drop1(fit2, test='Chisq', k = log(nrow(data)))
   
  # diagnostic plots
  par(mfrow=c(2,2))
  plot(fit2)
  dev.off()
  # note high leverage plus residuals points are labelled
  # leverage doesn't seem to be too high though which is good
  # NB the numbers refer to the row.names attribute which still refer to the original dataset, not this subset
  df[row.names(df) %in% c(9354,9356),]$date
  # as suspected [1] "1995-07-15" "1995-07-16"
   
  ######################################################
  # so lets re run without these obs
  df2 <- df[!row.names(df) %in% c(9354,9356),]
  # to avoid duplicating code just re run fit2, replacing data=df with df2
  # tmax still significant but not so extreme
  # check diagnostic plots again
  par(mfrow=c(2,2))
  plot(fit2)
  dev.off()
  # looks like a well behaved model now.
   
  # if we were still worried about any high leverage values we could identify these with
  df3 <- na.omit(df2[,c('cvd','pm10tmean','tmax','dptp','time')])
  df3$hatvalue <- hatvalues(fit2)
  df3$res <- residuals(fit2, 'pearson')
  with(df3, plot(hatvalue, res))
  # this is the same as the fourth default glm diagnostic plot, which they label x-axis as leverage
  summary(df3$hatvalue)
  # gives us an idea of the distribution of hat values
  # decide on a threshold and look at it
  hatThreshold <- 0.1
  with(subset(df3, hatvalue > hatThreshold), points(hatvalue, res, col = 'red', pch = 16))
  abline(0,0)
  segments(hatThreshold,-2,hatThreshold,15)
  dev.off()
   
  fit3 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = subset(df3, hatvalue < hatThreshold), family = poisson)
  par(mfrow=c(2,2))
  termplot(fit3, se = T)
  # same same
  plot(fit3)
  # no better
   
  # or we could go nuts with a whole number of ways of estimating influence
  # check all influential observations
  infl <- influence.measures(fit2)
  # which observations 'are' influential
  inflk <- which(apply(infl$is.inf, 1, any))
  length(inflk)
   
   
  ######################################################
  # now what about serial autocorrelation in the residuals?
   
  par(mfrow = c(2,1))
  with(df3, acf(res))
  with(df3, pacf(res))
  dev.off()
   
   
   
  ######################################################
  # just check for overdispersion
  fit <- gam(cvd ~ s(pm10tmean) + s(tmax) + s(dptp) + s(time, k= 7*numYears, fx=T), data = df, family = quasipoisson)
  summary(fit)
  # note the Scale est. = 1.1627
  # alternatively check the glm
  fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = quasipoisson)
  summary(fit2)
  # (Dispersion parameter for quasipoisson family taken to be 1.222640)
  # this is probably near enough to support a standard poisson model...
   
  # if we have overdispersion we can use QAIC (A quasi- mode does not have a likelihood and so does not have an AIC,  by definition)
  # we can use the poisson model and calculate the overdispersion
  fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = poisson)
  1- pchisq(deviance(fit2), df.residual(fit2))
   
  # QAIC, c is the variance inflation factor, the ratio of the residual deviance of the global (most complicated) model to the residual degrees of freedom
  c=deviance(fit2)/df.residual(fit2)
  QAIC.1=-2*logLik(fit2)/c + 2*(length(coef(fit2)) + 1)
  QAIC.1
   
  # Actually lets use QAICc which is more conservative about parameters,
  QAICc.1=-2*logLik(fit2)/c + 2*(length(coef(fit2)) + 1) + 2*(length(coef(fit2)) + 1)*(length(coef(fit2)) + 1 + 1)/(nrow(na.omit(df[,c('cvd','pm10tmean','tmax','dptp','time')]))- (length(coef(fit2))+1)-1)
  QAICc.1
   
   
  ######################################################
  # the following is old work, some may be interesting
  # such as the use of sinusoidal wave instead of smooth function of time
   
   
  # # sine wave
  # timevar <- as.data.frame(names(table(df$date)))
  # index <- 1:length(names(table(df$date)))
  # timevar$time2 <- index / (length(index) / (length(index)/365.25))
  # names(timevar) <- c('date','timevar')
  # timevar$date <- as.Date(timevar$date)
  # df <- merge(df,timevar)
   
  # fit <- gam(cvd ~ s(tmax) + s(dptp) + sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
  # summary(fit)
  # par(mfrow=c(3,2))
  # plot(fit, all.terms = T)
  # dev.off()
   
  # # now just explore the season fit
  # fit <- gam(cvd ~ sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
  # yhat <- predict(fit)
  # head(yhat)
   
  # with(df, plot(date,cvd,type = 'l',col='grey', ylim = c(15,55)))
  # lines(df[,'date'],exp(yhat),col='red')
   
   
  # # drop1(fit, test= 'Chisq')
   
  # # drop1 only works in glm?
  # # fit with weather variables, use degrees of freedom estimated by gam
  # fit <- glm(cvd ~ ns(tmax,8) + ns(dptp,2) + sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
  # drop1(fit, test= 'Chisq')
  # # use plot.glm for diagnostics
  # par(mfrow=c(2,2))
  # plot(fit)
  # par(mfrow=c(3,2))
  # termplot(fit, se=T)
  # dev.off()
   
  # # cyclic spline, overlay on prior sinusoidal
  # with(df, plot(date,cvd,type = 'l',col='grey', ylim = c(0,55)))
  # lines(df[,'date'],exp(yhat),col='red')
   
  # df$daynum <- as.numeric(format(df$date, "%j"))
  # df[360:370,c('date','daynum')]
  # fit <- gam(cvd ~ s(daynum, k=3, fx=T, bs = 'cp') +  s(time, k = numYears, fx = T), data = df, family = poisson)
  # yhat2 <- predict(fit)
  # head(yhat2)
   
  # lines(df[,'date'],exp(yhat2),col='blue')
   
   
  # par(mfrow=c(1,2))
  # plot(fit)
   
   
  # # fit weather with season
  # fit <- gam(cvd ~ s(tmax) + s(dptp) +
    # s(daynum, k=3, fx=T, bs = 'cp') +  s(time, k = numYears, fx = T), data = df, family = poisson)
  # par(mfrow=c(2,2))
  # plot(fit)
   
  # summary(fit)
    
#+end_src

** COMMENT SCP files across
#+name:scp-files
#+begin_src sh :session *shell* :tangle no :exports none :eval no
cd images
scp ivan_hanigan@130.56.102.53:projects/spatiotemporal-regression-models/NMMAPS-example/images/nmmaps-eg-core.png nmmaps-eg-core.png
cd ..
cd NMMAPS-example
scp ivan_hanigan@130.56.102.53:projects/spatiotemporal-regression-models/NMMAPS-example/analyte.csv analyte.csv
cd ..
#+end_src
** Spatial lag and timeseries 
I will use the same NMMAPSlite to show how I'd approach a simple "Spatio-Temporal" model.

*** TODO The following is a stub of an idea.  For further development
*** Data
**** Zones
***** Spatial Neighbours Code
#+name:spatwat
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-2-load.r :exports reports :eval no
  ################################################################
  # name:spatwat
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
  require(gisviz)
  
  # load
  dir()
  shp <- readOGR("cities.shp", "cities")
  
  # clean
  head(shp@data)
  
  # do
  ## I will use nearest neighbour within a distance threshold, but
  ## usually polygon datasets would use poly2nb
  nb <- dnearneigh(shp, d1 = 1, d2 = 1000)                      
  head(nb)
  shp[[1]][1]
  shp[[1]][nb[[1]]]
  # map
  nudge <- 10
  png("images/nmmaps-eg-neighbourhood.png")
  plotMyMap(
      shp@coords, xl = c(min(shp@coords[,1])-nudge, max(shp@coords[,1])+nudge),
      yl = c(min(shp@coords[,2])-nudge, max(shp@coords[,2])+nudge)
      )
  plot(nb, shp@coords, add=TRUE)
  text(shp@data$long, shp@data$lat, shp@data$address, pos=3)
  points(shp@coords[nb[[1]],], col = 'green', pch = 16)
  points(shp@data[1,c("long","lat")],col = 'blue', pch = 16)
  dev.off()
  
  
#+end_src

***** Spatial Neighbours Output
[[file:NMMAPS-example/images/nmmaps-eg-neighbourhood.png]]
**** Outcome
#+name:load-suicide
#+begin_src R :session *R* :tangle NMMAPS-example/case-study-2-load.r :exports reports :eval no
  #### Now do the Neighbourhood autoregressive lagged variable  ####
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
  require(gisviz)
  require(plyr)
  
  # load
  analyte <- read.csv("analyte.csv")
  shp <- readOGR("cities.shp", "cities")
  
  # clean
  analyte$yy <- substr(analyte$date,1,4)
  numYears<-length(names(table(analyte$yy)))
  analyte$date <- as.Date(analyte$date)
  analyte$time <- as.numeric(analyte$date)
  analyte$agecat <- factor(analyte$agecat,
                            levels = c("under65",
                                "65to74", "75p"),
                            ordered = TRUE
                            )
   
  names(analyte)
  analyte[1,]
  table(analyte$city)
  # do
  study <- analyte[, c("city","date", "agecat", "cvd", "pop")]
  subset(study, city == "tucs" & date == as.Date("1991-01-01"))
  
  nb <- dnearneigh(shp, d1 = 1, d2 = 1000)                      
  head(nb)
  head(shp@data)
  adj <- adjacency_df(NB = nb, shp = shp, zone_id = 'city')
  subset(adj, V1 == "tucs")
  
  neighbours <- merge(study, adj, by.x = "city", by.y = "V1")
  subset(neighbours, city == "tucs" & date == as.Date("1991-01-01"))
  
  xvars <- c("V2", "date","agecat")
  yvars <- c("city", "date", "agecat")
  neighbours <- merge(neighbours[,c(xvars, "city")],
                      analyte[,c(yvars, "cvd", "pop")],
                      by.x = xvars,
                      by.y = yvars)
  names(neighbours)
  subset(neighbours, city == "tucs" & date == as.Date("1991-01-01"))
  
  neighbours$asr  <- (neighbours$cvd / neighbours$pop) * 1000
  
  
  neighbours2  <- ddply(neighbours, c("city", "date", "agecat"), summarise,
                       NeighboursYij  = mean(asr)
                       )
  subset(neighbours2, city == "tucs" & date == as.Date("1991-01-01"))
  table(neighbours2$city)
  head(analyte)
  
  analyte  <- merge(analyte, neighbours2, by = c("city", "agecat", "date"))
  analyte <- arrange(analyte, city, date, agecat)
  head(analyte)
  
  
#+end_src


*** Analysis
**** Model with Spatial Lag
#+begin_src R :session *R* :tangle NMMAPS-example/case-study-2-do.r :exports reports :eval no
  # func
  require(splines)
  
  # load
  # assumes the prior code chunks have been run
  
  # do
  fit <- glm(cvd ~ ns(tmax, df = 3) + ns(dptp, df = 3) +
             NeighboursYij + 
             city + agecat +
             ns(time, df = 7*numYears) +
             offset(log(pop)),
             data = analyte, family = poisson
             )
  
  # plot of response functions
  png("images/nmmaps-eg-sp-lag.png", width = 1000, height = 750, res = 150)
  par(mfrow=c(2,3))
  termplot(fit, terms = attr(terms(fit),'term.labels') [c(1:2,6)], se = TRUE, ylim =c(-.2,.2), col.term = 'black', col.se = 'black')
  termplot(fit, terms = attr(terms(fit),'term.labels') [c(4,5)], se = TRUE, ylim =c(-3,3), col.term = 'black', col.se = 'black')
  termplot(fit, terms = attr(terms(fit),'term.labels') [3], se = TRUE, ylim =c(-1,1), col.term = 'black', col.se = 'black')
  dev.off()
  
  
#+end_src
**** Spatial Lag Model Plots
[[file:NMMAPS-example/images/nmmaps-eg-sp-lag.png]]

** Spatial error models
- TODO


* 7 Causal inference using multilevel models
** Statistical and causal models
Terminology "statistical and causal inference" or "probabilistic and causal reasoning"... 

Pearl, J., Glymour, M., & Jewell, N. P. (2016). Chapter 1: Preliminaries: Statistical and Causal Models. In Causal Inference in Statistics: A Primer. (pp. 1–34). West Sussex, United Kingdom: John Wiley & Sons. 

see page 5 of 37 
e.g. "there is no way to represent any causal information in contingency tables on which statistical inference is often based. There are, however, extra-statistical methods that can be used to express and interpret causal assumptions."

The approaches taken to address the limitation conform to
recommended statistical practices that address the
fundamental problem of causal inference:
  
We can not observe what happens to an individual after taking the treatment (at a particular point in time) and what happens to that same individual after not taking the treatment (at the same point in time) . . . . 

estimating causal effects requires one or some combination of the following: close substitutes for the [counterfactual] potential outcomes, randomization, or statistical adjustment. (Gelman and Hill 2007 p.171, also see their recommendations for Bayesian and Frequentist tools for causal inference using regression and multilevel models discussed in Chapters 9, 10 and 23)

  


* 8 The importance of scale
** paper
~/projects_ceraph/importance_of_scale
** chapter 
The problem will be lack of authorisation to share.
Therefore need to build clear descriptions, and maybe produce simulation for students?

