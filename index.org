#+TITLE:Spatiotemporal Regression Modelling
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----

* README
#+name:README
#+begin_src markdown :tangle README.md :exports reports :eval no
# Spatiotemporal Regression Modelling

This is an Open Notebook for my work on Spatiotemporal Regression Modelling tips and tricks.

Ivan Hanigan
#+end_src
** Version Statement
#+begin_src R :session *R* :exports none
  commit_msg <- "written R codes to scripts"
  commit_msg <- as.data.frame(c(as.character(Sys.Date()), commit_msg))
  commit_msg
#+end_src


|                 2013-10-16 |
| written R codes to scripts |




* COMMENT Notes
** Update on reflections from Bob Haining's Lecture
[[http://ivanhanigan.github.io/2013/04/reflections-bob-haining/][Earlier this year]] Prof Bob Haining from the Geography Department Cambridge visited and gave us a great lecture on spatial regression.

This Tuesday at the [[http://gis-forum.github.io][GIS Forum]] we were lucky to be joined by statistician Phil Kokic from CSIRO who had heard we'd be discussing spatial autocorrelation (Phil is my PhD supervisor). Here are some quick notes I made:

*** CART Tree analysis that addresses the (potential)spatial autocorrelation problem
We started off the discussion with an assessment of the approach described in this post [[http://thebiobucket.blogspot.com.au/2012/03/classification-trees-allowing-for.html][Classification Trees and Spatial Autocorrelation]].

I've been thinking more and more about decision trees/CART/random forest methods for selecting a subset of relevant variables (and interations) for use in GLM or GAM model construction.  In a perfect world I'd have data on the main predictor I wanted to model and enough data about all the relevant other predictors (especially confounding or modifying variables) to ensure I get a 'well behaved model'. But with all the data around and so many potentially plausible relationships one might choose to include we need a way to narrow down these to just include the most important covariates, confounders and interactions.  CART or some variation on it seems a good way to do this, but is prone to the potential problem of spatially correlated errors too.

The idea from that blog post is:

"compute the classification tree, calculate residuals and use it for a Mantel-test and Mantel correlograms.
The Mantel correlograms test differrences in dissimilarities of
the residuals across several spatial distances and thus enable you to detect lag-distances where possible spatial autocorrelation vanishes.
...If encounter autocorrelation... try to use subsamples of the data avoiding resampling within the lag-distance.."

I think the workflow would be to

- fit the classification tree (Question: best to use all the data or with a sample like using cross-validation)
- get the residuals and visually assess the lagged distances plot provided by the Mantel correlogram.  Decide on a threshold (Question: is there an objective way to do this?).
- Sample from the data and select out from this sample only data from pairs with distances greater than the threshold (have to keep one out of each close pair or else we'd only be getting data from the sparsely sampled parts of our study region).

We all agreed this sounded OK, but only avoids the problem of spatial autocorrelation (and loses data).

*** Modeling with control for spatial autocorrelation
So we all agreed we'd prefer if our model can control for spatial autocorrelation.  I confessed that I'd always found the GeoBUGS tutorial and other tutorials about Bayesian methods for this very difficult and would really like a "Simple" way to make the problem go away.  So first we briefly reviewed Prof Hainings 3 equations again:

NOTE: THE FOLLOWING IDEAS WORK BEST FOR AREAL DATA.

*** The Spatial Error Model

$Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \eta_{i}$

Where:

$\eta_{i}$ = Spatially autocorrelated errors.


*** The Spatial Lag Model

$Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \rho(Neighbours Y_{ij}) + e_{i}$

Where:

$\rho_(Neighbours Y_{ij})$ = is an additional explanatory variable which is the value of the dependent variable in neighbouring areas. 

*** Spatially Lagged Independent Variable(s)

$Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \beta_{2L} X_{2ij} + e_{i}$

Where:

$\beta_{2L} X_{2ij}$ = is the independent variable X2 that is spatially lagged.


*** Discussion
- Phil agreed with Bob that the spatial error model is the best, spatial lag model is OK and spatially lagged covariates not so great.
- For spatial error model fitting Phil suggested looking at R packages spBayes and spTimer
- I pointed out that I am mostly interested in "spatially structured time-series models" rather than spatial models at a single point in time.  By this I mean that we have several neighbouring areal units observed over a period of time.  In this framework the general methods of time series modelling are used to control for temporal autocorrelation.  However this makes the methods of spatial error and spatial lag models tricky because the spatial autocorrelation needs to be assessed at many points in time.
- I asked that if spatial lag is OK (and it seems easier to fit into the time-series model framework) how can I check to know if it has done the trick?  If this were purely a spatial model we could check for spatial autocorrelation in the residuals just as they described in the CART blog above, but here we have many maps we could make (one every time point), and our spatial autocorrelation measure would surely vary a lot over time.  SO would a simple way just be to asses the effect on the Standard Error on beta1 (our primary interest) and if it is bigger but still significant we can be reassured that our result isn't affected? Or perhaps we should assess the beta on the lagged variable, for instance is a significant p-value on the lagged Beta an indication that it is capturing the unmeasured spatial associations represented by the neighbourhood variable?  
- If it hadn't done the trick Nerida pointed out this might be because the Neighbourhoods are actually not appropriately represented by the first order neighbours and therefore more neighbours could be included, like moving out several concentric circles to wider and wider neighbourhoods
- Nasser and Phil pointed out that the lagged variable (the outcome in the neighbours) includes an element of the exposure variables, and said that it would be difficult to 'unpack' what that part of the model meant.
- so it looks like there is no simple answer and spatial error model is still preferred.

** md
#+name:reflections-bob-haining-update-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-09-25-reflections-bob-haining-update.md :exports none :eval no :padline no
  ---
  name: reflections-bob-haining-update
  layout: post
  title: reflections-bob-haining-update
  date: 2013-09-25
  categories:
  - spatial dependence
  ---
  
  <!-- <?xml version="1.0" encoding="utf-8"?> -->
  <!-- <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" -->
  <!--                "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"> -->
  <!-- <html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"> -->
  <head>
  <!-- <title>spatiotemporal </title> -->
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
  <meta name="title" content="spatiotemporal "/>
  <meta name="generator" content="Org-mode"/>
  <meta name="generated" content="2013-09-25T14:46+1000"/>
  <meta name="author" content="Ivan Hanigan"/>
  <meta name="description" content=""/>
  <meta name="keywords" content=""/>
  <style type="text/css">
   <!--/*--><![CDATA[/*><!--*/
    html { font-family: Times, serif; font-size: 12pt; }
    .title  { text-align: center; }
    .todo   { color: red; }
    .done   { color: green; }
    .tag    { background-color: #add8e6; font-weight:normal }
    .target { }
    .timestamp { color: #bebebe; }
    .timestamp-kwd { color: #5f9ea0; }
    .right  {margin-left:auto; margin-right:0px;  text-align:right;}
    .left   {margin-left:0px;  margin-right:auto; text-align:left;}
    .center {margin-left:auto; margin-right:auto; text-align:center;}
    p.verse { margin-left: 3% }
    pre {
      border: 1pt solid #AEBDCC;
      background-color: #F3F5F7;
      padding: 5pt;
      font-family: courier, monospace;
          font-size: 90%;
          overflow:auto;
    }
    table { border-collapse: collapse; }
    td, th { vertical-align: top;  }
    th.right  { text-align:center;  }
    th.left   { text-align:center;   }
    th.center { text-align:center; }
    td.right  { text-align:right;  }
    td.left   { text-align:left;   }
    td.center { text-align:center; }
    dt { font-weight: bold; }
    div.figure { padding: 0.5em; }
    div.figure p { text-align: center; }
    div.inlinetask {
      padding:10px;
      border:2px solid gray;
      margin:10px;
      background: #ffffcc;
    }
    textarea { overflow-x: auto; }
    .linenr { font-size:smaller }
    .code-highlighted {background-color:#ffff00;}
    .org-info-js_info-navigation { border-style:none; }
    #org-info-js_console-label { font-size:10px; font-weight:bold;
                                 white-space:nowrap; }
    .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                   font-weight:bold; }
    /*]]>*/-->
  </style>
  <script type="text/javascript">
  /*
  @licstart  The following is the entire license notice for the
  JavaScript code in this tag.
  
  Copyright (C) 2012-2013 Free Software Foundation, Inc.
  
  The JavaScript code in this tag is free software: you can
  redistribute it and/or modify it under the terms of the GNU
  General Public License (GNU GPL) as published by the Free Software
  Foundation, either version 3 of the License, or (at your option)
  any later version.  The code is distributed WITHOUT ANY WARRANTY;
  without even the implied warranty of MERCHANTABILITY or FITNESS
  FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.
  
  As additional permission under GNU GPL version 3 section 7, you
  may distribute non-source (e.g., minimized or compacted) forms of
  that code without the copy of the GNU GPL normally required by
  section 4, provided you include this license notice and a URL
  through which recipients can access the Corresponding Source.
  
  
  @licend  The above is the entire license notice
  for the JavaScript code in this tag.
  ,*/
  <!--/*--><![CDATA[/*><!--*/
   function CodeHighlightOn(elem, id)
   {
     var target = document.getElementById(id);
     if(null != target) {
       elem.cacheClassElem = elem.className;
       elem.cacheClassTarget = target.className;
       target.className = "code-highlighted";
       elem.className   = "code-highlighted";
     }
   }
   function CodeHighlightOff(elem, id)
   {
     var target = document.getElementById(id);
     if(elem.cacheClassElem)
       elem.className = elem.cacheClassElem;
     if(elem.cacheClassTarget)
       target.className = elem.cacheClassTarget;
   }
  /*]]>*///-->
  </script>
  <script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js">
  /**
   ,*
   ,* @source: http://orgmode.org/mathjax/MathJax.js
   ,*
   ,* @licstart  The following is the entire license notice for the
   ,*  JavaScript code in http://orgmode.org/mathjax/MathJax.js.
   ,*
   ,* Copyright (C) 2012-2013  MathJax
   ,*
   ,* Licensed under the Apache License, Version 2.0 (the "License");
   ,* you may not use this file except in compliance with the License.
   ,* You may obtain a copy of the License at
   ,*
   ,*     http://www.apache.org/licenses/LICENSE-2.0
   ,*
   ,* Unless required by applicable law or agreed to in writing, software
   ,* distributed under the License is distributed on an "AS IS" BASIS,
   ,* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   ,* See the License for the specific language governing permissions and
   ,* limitations under the License.
   ,*
   ,* @licend  The above is the entire license notice
   ,* for the JavaScript code in http://orgmode.org/mathjax/MathJax.js.
   ,*
   ,*/
  
  /*
  @licstart  The following is the entire license notice for the
  JavaScript code below.
  
  Copyright (C) 2012-2013 Free Software Foundation, Inc.
  
  The JavaScript code below is free software: you can
  redistribute it and/or modify it under the terms of the GNU
  General Public License (GNU GPL) as published by the Free Software
  Foundation, either version 3 of the License, or (at your option)
  any later version.  The code is distributed WITHOUT ANY WARRANTY;
  without even the implied warranty of MERCHANTABILITY or FITNESS
  FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.
  
  As additional permission under GNU GPL version 3 section 7, you
  may distribute non-source (e.g., minimized or compacted) forms of
  that code without the copy of the GNU GPL normally required by
  section 4, provided you include this license notice and a URL
  through which recipients can access the Corresponding Source.
  
  
  @licend  The above is the entire license notice
  for the JavaScript code below.
  ,*/
  <!--/*--><![CDATA[/*><!--*/
      MathJax.Hub.Config({
          // Only one of the two following lines, depending on user settings
          // First allows browser-native MathML display, second forces HTML/CSS
          //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
              jax: ["input/TeX", "output/HTML-CSS"],
          extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                       "TeX/noUndefined.js"],
          tex2jax: {
              inlineMath: [ ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
              skipTags: ["script","noscript","style","textarea","pre","code"],
              ignoreClass: "tex2jax_ignore",
              processEscapes: false,
              processEnvironments: true,
              preview: "TeX"
          },
          showProcessingMessages: true,
          displayAlign: "center",
          displayIndent: "2em",
  
          "HTML-CSS": {
               scale: 100,
               availableFonts: ["STIX","TeX"],
               preferredFont: "TeX",
               webFont: "TeX",
               imageFont: "TeX",
               showMathMenu: true,
          },
          MMLorHTML: {
               prefer: {
                   MSIE:    "MML",
                   Firefox: "MML",
                   Opera:   "HTML",
                   other:   "HTML"
               }
          }
      });
  /*]]>*///-->
  </script>
  </head>
  <body>
  
  <div id="preamble">
  
  </div>
  
  <div id="content">
  <!-- <h1 class="title">spatiotemporal </h1> -->
  
  
  <div id="table-of-contents">
  <h2>Table of Contents</h2>
  <div id="text-table-of-contents">
  <ul>
  <li><a href="#sec-1">1 Update on reflections from Bob Haining's Lecture</a>
  <ul>
  <li><a href="#sec-1-1">1.1 CART Tree analysis that addresses the (potential)spatial autocorrelation problem</a></li>
  <li><a href="#sec-1-2">1.2 Modeling with control for spatial autocorrelation</a></li>
  <li><a href="#sec-1-3">1.3 The Spatial Error Model</a></li>
  <li><a href="#sec-1-4">1.4 The Spatial Lag Model</a></li>
  <li><a href="#sec-1-5">1.5 Spatially Lagged Independent Variable(s)</a></li>
  <li><a href="#sec-1-6">1.6 Discussion</a></li>
  </ul>
  </li>
  </ul>
  </div>
  </div>
  
  <div id="outline-container-1" class="outline-3">
  <h3 id="sec-1"><span class="section-number-3">1</span> Update on reflections from Bob Haining's Lecture</h3>
  <div class="outline-text-3" id="text-1">
  
  <p><a href="http://ivanhanigan.github.io/2013/04/reflections-bob-haining/">Earlier this year</a> Prof Bob Haining from the Geography Department Cambridge visited and gave us a great lecture on spatial regression.
  </p>
  <p>
  This Tuesday at the <a href="http://gis-forum.github.io">GIS Forum</a> we were lucky to be joined by statistician Phil Kokic from CSIRO who had heard we'd be discussing spatial autocorrelation (Phil is my PhD supervisor). Here are some quick notes I made:
  </p>
  
  </div>
  
  <div id="outline-container-1-1" class="outline-4">
  <h4 id="sec-1-1"><span class="section-number-4">1.1</span> CART Tree analysis that addresses the (potential)spatial autocorrelation problem</h4>
  <div class="outline-text-4" id="text-1-1">
  
  <p>We started off the discussion with an assessment of the approach described in this post <a href="http://thebiobucket.blogspot.com.au/2012/03/classification-trees-allowing-for.html">Classification Trees and Spatial Autocorrelation</a>.
  </p>
  <p>
  I've been thinking more and more about decision trees/CART/random forest methods for selecting a subset of relevant variables (and interations) for use in GLM or GAM model construction.  In a perfect world I'd have data on the main predictor I wanted to model and enough data about all the relevant other predictors (especially confounding or modifying variables) to ensure I get a 'well behaved model'. But with all the data around and so many potentially plausible relationships one might choose to include we need a way to narrow down these to just include the most important covariates, confounders and interactions.  CART or some variation on it seems a good way to do this, but is prone to the potential problem of spatially correlated errors too.
  </p>
  <p>
  The idea from that blog post is:
  </p>
  <p>
  "compute the classification tree, calculate residuals and use it for a Mantel-test and Mantel correlograms.
  The Mantel correlograms test differrences in dissimilarities of
  the residuals across several spatial distances and thus enable you to detect lag-distances where possible spatial autocorrelation vanishes.
  &hellip;If encounter autocorrelation&hellip; try to use subsamples of the data avoiding resampling within the lag-distance.."
  </p>
  <p>
  I think the workflow would be to
  </p>
  <ul>
  <li>fit the classification tree (Question: best to use all the data or with a sample like using cross-validation)
  </li>
  <li>get the residuals and visually assess the lagged distances plot provided by the Mantel correlogram.  Decide on a threshold (Question: is there an objective way to do this?).
  </li>
  <li>Sample from the data and select out from this sample only data from pairs with distances greater than the threshold (have to keep one out of each close pair or else we'd only be getting data from the sparsely sampled parts of our study region).
  </li>
  </ul>
  
  
  <p>
  We all agreed this sounded OK, but only avoids the problem of spatial autocorrelation (and loses data).
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-2" class="outline-4">
  <h4 id="sec-1-2"><span class="section-number-4">1.2</span> Modeling with control for spatial autocorrelation</h4>
  <div class="outline-text-4" id="text-1-2">
  
  <p>So we all agreed we'd prefer if our model can control for spatial autocorrelation.  I confessed that I'd always found the GeoBUGS tutorial and other tutorials about Bayesian methods for this very difficult and would really like a "Simple" way to make the problem go away.  So first we briefly reviewed Prof Hainings 3 equations again:
  </p>
  <p>
  NOTE: THE FOLLOWING IDEAS WORK BEST FOR AREAL DATA.
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-3" class="outline-4">
  <h4 id="sec-1-3"><span class="section-number-4">1.3</span> The Spatial Error Model</h4>
  <div class="outline-text-4" id="text-1-3">
  
  
  
  
  \(Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \eta_{i}\)
  
  <p>
  Where:
  </p>
  <p>
  \(\eta_{i}\) = Spatially autocorrelated errors.
  </p>
  
  </div>
  
  </div>
  
  <div id="outline-container-1-4" class="outline-4">
  <h4 id="sec-1-4"><span class="section-number-4">1.4</span> The Spatial Lag Model</h4>
  <div class="outline-text-4" id="text-1-4">
  
  
  
  
  \(Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \rho(Neighbours Y_{ij}) + e_{i}\)
  
  <p>
  Where:
  </p>
  <p>
  \(\rho_(Neighbours Y_{ij})\) = is an additional explanatory variable which is the value of the dependent variable in neighbouring areas. 
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-5" class="outline-4">
  <h4 id="sec-1-5"><span class="section-number-4">1.5</span> Spatially Lagged Independent Variable(s)</h4>
  <div class="outline-text-4" id="text-1-5">
  
  
  
  
  \(Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \beta_{2L} X_{2ij} + e_{i}\)
  
  <p>
  Where:
  </p>
  <p>
  \(\beta_{2L} X_{2ij}\) = is the independent variable X2 that is spatially lagged.
  </p>
  
  </div>
  
  </div>
  
  <div id="outline-container-1-6" class="outline-4">
  <h4 id="sec-1-6"><span class="section-number-4">1.6</span> Discussion</h4>
  <div class="outline-text-4" id="text-1-6">
  
  <ul>
  <li>Phil agreed with Bob that the spatial error model is the best, spatial lag model is OK and spatially lagged covariates not so great.
  </li>
  <li>For spatial error model fitting Phil suggested looking at R packages spBayes and spTimer.
  </li>
  <li>I pointed out that I am mostly interested in "spatially structured time-series models" rather than spatial models at a single point in time.  By this I mean that we have several neighbouring areal units observed over a period of time.  In this framework the general methods of time series modelling are used to control for temporal autocorrelation.  However this makes the methods of spatial error and spatial lag models tricky because the spatial autocorrelation needs to be assessed at many points in time.
  </li>
  <li>I asked that if spatial lag is OK (and it seems easier to fit into the time-series model framework) how can I check to know if it has done the trick?  If this were purely a spatial model we could check for spatial autocorrelation in the residuals just as they described in the CART blog above, but here we have many maps we could make (one every time point), and our spatial autocorrelation measure would surely vary a lot over time.  SO would a simple way just be to asses the effect on the Standard Error on beta1 (our primary interest) and if it is bigger but still significant we can be reassured that our result isn't affected? Or perhaps we should assess the beta on the lagged variable, for instance is a significant p-value on the lagged Beta an indication that it is capturing the unmeasured spatial associations represented by the neighbourhood variable?  
  </li>
  <li>If it hadn't done the trick Nerida pointed out this might be because the Neighbourhoods are actually not appropriately represented by the first order neighbours and therefore more neighbours could be included, like moving out several concentric circles to wider and wider neighbourhoods
  </li>
  <li>Nasser and Phil pointed out that the lagged variable (the outcome in the neighbours) includes an element of the exposure variables, and said that it would be difficult to 'unpack' what that part of the model meant.
  </li>
  <li>so it looks like there is no simple answer and spatial error model is still preferred.
  </li>
  </ul>
  
  
  </div>
  </div>
  </div>
  </div>
  
  </body>
  </html>
  
#+end_src



** 2013-09-26-spatially-structured-timeseries-vs-spatiotemporal-modelling
** Spatially Structured Timeseries Vs Spatiotemporal Modelling
In my last post about [[http://ivanhanigan.github.io/2013/09/reflections-bob-haining-update][spatiotemporal regression modelling]] I mentioned that I am mostly interested in "spatially structured time-series models" rather than spatial models at a single point in time. By this I mean that we have several neighbouring areal units observed over a period of time. In this framework the general methods of time series modelling are used to control for temporal autocorrelation. However this makes the methods of spatial error and spatial lag models tricky because the spatial autocorrelation needs to be assessed at many points in time.

I want to expand more on this topic because I want to be clear that the organisation of the material I am aiming to bring to this notebook topic is not aimed at purely spatial regression models [[https://geodacenter.asu.edu/spatial-lag-and][(there is a lot of material and tools out there already for that)]].  I am trying with these notes to document my learning steps toward integrating spatial methods with time-series methods to allow me to practice (and understand) spatiotemporal regression modelling.

*** Spatially Structured Time Series
In [[http://www.pnas.org/content/early/2012/08/08/1112965109.full.pdf+html][my most successful previous attempt to conduct a spatiotemporal analysis of Suicide and Droughts]] I built on my knowledge of time-series regression models from single-city air pollution studies where the whole city is the unit of analysis and the temporal variation is modelled with controlling techniques for temporal autocorrelation.  These techniques are also valid for multi-city studies because it is pretty safe to assume the cities are all independent at each time point.  I structured my study by Eleven large zones (Census Statistical Divisions) of NSW and assumed each of these would vary over time independent of each other, and I fitted a zone-specific time trend and cycle. This is what I call "spatially structured time-series" modelling.  

I justify using this model in this case because aggregating up to these very large regions will diminish the possibility of spatial autocorrelation and because Droughts vary over large spatial zones too, we will not suffer from exposure misclassificaiton bias.

So this model is a simple time-series regression (with trend and seasonality) and an additional term for spatial Zone.

\begin{eqnarray*}
        log({\color{red} O_{ijk}})  & = & s({\color{red}ExposureVariable})  + {\color{blue} OtherExplanators}  \\
        & &   + AgeGroup_{i} + Sex_{j} \\
        & &   + {\color{blue} SpatialZone_{k}}  \\
        & &  + sin(Time \times 2 \times \pi) + cos(Time \times 2 \times \pi) \\
        & &  + Trend \\
        & &   + offset({\color{blue} log(Pop_{ijk})})\\
\end{eqnarray*}

Where:\\

- ${\color{red}O_{ijk}}$ = Outcome (counts) by Age$_{i}$, Sex$_{j}$ and SpatialZone$_{k}$ \\
- {\color{red}ExposureVariable} = Data with {\color{red}Restrictive Intellectual Property~(IP)} \\
- {\color{blue}OtherExplanators} = Other {\color{blue}Less Restricted}  Explanatory variables \\
- s( ) = penalized regression splines \\
- ${\color{blue} SpatialZone_{k}}$  = {\color{blue} Less Restricted} data representing the $SpatialZone_{k}$  \\
- Trend = Longterm smooth trend(s) \\
- ${\color{blue}Pop_{ijk}}$ = interpolated Census populations, by time in each group\\

*** TODO Spatiotemporal modelling
In contrast to the above model for modelling exposures that have fine resolution spatial variation (such as air pollution) the exposure misclassification effect of aggregating up to very large spatial zones will conteract the benefits of avoiding spatially autocorrelated errors and this might be unacceptable for certain research questions.  Therefore it is important to move toward a spatiotemporal regression model that replaces the $SpatialZone_{k}$ term with a more spatial error or spatial lag approach.
** md
#+name:spatially-structured-timeseries-vs-spatiotemporal-modelling-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-09-26-spatially-structured-timeseries-vs-spatiotemporal-modelling.md :exports none :eval no :padline no
---
name: spatially-structured-timeseries-vs-spatiotemporal-modelling
layout: post
title: spatially-structured-timeseries-vs-spatiotemporal-modelling
date: 2013-09-26
categories:
- spatial dependence
---
    
<head>
<title>Spatiotemporal Regression Modelling</title>
<meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1"/>
<meta name="title" content="Spatiotemporal Regression Modelling"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2013-09-26T10:18+1000"/>
<meta name="author" content="Ivan Hanigan"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js">
/**
 *
 * @source: http://orgmode.org/mathjax/MathJax.js
 *
 * @licstart  The following is the entire license notice for the
 *  JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 * Copyright (C) 2012-2013  MathJax
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * @licend  The above is the entire license notice
 * for the JavaScript code in http://orgmode.org/mathjax/MathJax.js.
 *
 */

/*
@licstart  The following is the entire license notice for the
JavaScript code below.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code below is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code below.
*/
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">Spatiotemporal Regression Modelling</h1>


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 Spatially Structured Timeseries Vs Spatiotemporal Modelling</a>
<ul>
<li><a href="#sec-1-1">1.1 Spatially Structured Time Series</a></li>
<li><a href="#sec-1-2">1.2 Spatiotemporal modelling</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-3">
<h3 id="sec-1"><span class="section-number-3">1</span> Spatially Structured Timeseries Vs Spatiotemporal Modelling</h3>
<div class="outline-text-3" id="text-1">

<p>In my last post about <a href="http://ivanhanigan.github.io/2013/09/reflections-bob-haining-update">spatiotemporal regression modelling</a> I mentioned that I am mostly interested in "spatially structured time-series models" rather than spatial models at a single point in time. By this I mean that we have several neighbouring areal units observed over a period of time. In this framework the general methods of time series modelling are used to control for temporal autocorrelation. However this makes the methods of spatial error and spatial lag models tricky because the spatial autocorrelation needs to be assessed at many points in time.
</p>
<p>
I want to expand more on this topic because I want to be clear that the organisation of the material I am aiming to bring to this notebook topic is not aimed at purely spatial regression models <a href="https://geodacenter.asu.edu/spatial-lag-and">(there is a lot of material and tools out there already for that)</a>.  I am trying with these notes to document my learning steps toward integrating spatial methods with time-series methods to allow me to practice (and understand) spatiotemporal regression modelling.
</p>

</div>

<div id="outline-container-1-1" class="outline-4">
<h4 id="sec-1-1"><span class="section-number-4">1.1</span> Spatially Structured Time Series</h4>
<div class="outline-text-4" id="text-1-1">

<p>In <a href="http://www.pnas.org/content/early/2012/08/08/1112965109.full.pdf+html">my most successful previous attempt to conduct a spatiotemporal analysis of Suicide and Droughts</a> I built on my knowledge of time-series regression models from single-city air pollution studies where the whole city is the unit of analysis and the temporal variation is modelled with controlling techniques for temporal autocorrelation.  These techniques are also valid for multi-city studies because it is pretty safe to assume the cities are all independent at each time point.  I structured my study by Eleven large zones (Census Statistical Divisions) of NSW and assumed each of these would vary over time independent of each other, and I fitted a zone-specific time trend and cycle. This is what I call "spatially structured time-series" modelling.  
</p>
<p>
I justify using this model in this case because aggregating up to these very large regions will diminish the possibility of spatial autocorrelation and because Droughts vary over large spatial zones too, we will not suffer from exposure misclassificaiton bias.
</p>
<p>
So this model is a simple time-series regression (with trend and seasonality) and an additional term for spatial Zone.
</p>


\begin{eqnarray*}
        log({\color{red} O_{ijk}})  & = & s({\color{red}ExposureVariable})  + {\color{blue} OtherExplanators}  \\
        & &   + AgeGroup_{i} + Sex_{j} \\
        & &   + {\color{blue} SpatialZone_{k}}  \\
        & &  + sin(Time \times 2 \times \pi) + cos(Time \times 2 \times \pi) \\
        & &  + Trend \\
        & &   + offset({\color{blue} log(Pop_{ijk})})\\
\end{eqnarray*}

<p>
Where:<br/>
</p>
<ul>
<li>\({\color{red}O_{ijk}}\) = Outcome (counts) by Age\(_{i}\), Sex\(_{j}\) and SpatialZone\(_{k}\) <br/>
</li>
<li>{\color{red}ExposureVariable} = Data with {\color{red}Restrictive Intellectual Property~(IP)} <br/>
</li>
<li>{\color{blue}OtherExplanators} = Other {\color{blue}Less Restricted}  Explanatory variables <br/>
</li>
<li>s( ) = penalized regression splines <br/>
</li>
<li>\({\color{blue} SpatialZone_{k}}\)  = {\color{blue} Less Restricted} data representing the \(SpatialZone_{k}\)  <br/>
</li>
<li>Trend = Longterm smooth trend(s) <br/>
</li>
<li>\({\color{blue}Pop_{ijk}}\) = interpolated Census populations, by time in each group<br/>
</li>
</ul>


</div>

</div>

<div id="outline-container-1-2" class="outline-4">
<h4 id="sec-1-2"><span class="section-number-4">1.2</span> <span class="todo TODO">TODO</span> Spatiotemporal modelling</h4>
<div class="outline-text-4" id="text-1-2">

<p>In contrast to the above model for modelling exposures that have fine resolution spatial variation (such as air pollution) the exposure misclassification effect of aggregating up to very large spatial zones will conteract the benefits of avoiding spatially autocorrelated errors and this might be unacceptable for certain research questions.  Therefore it is important to move toward a spatiotemporal regression model that replaces the \(SpatialZone_{k}\) term with a more spatial error or spatial lag approach.
</p></div>
</div>
</div>
</div>

</body>
</html>
#+end_src

** 2013-10-10-simple-example-using-nmmaps
#+name:simple-example-using-nmmaps-header
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-10-10-simple-example-using-nmmaps.md :exports none :eval no :padline no
  ---
  name: 2013-10-10-simple-example-using-nmmaps
  layout: post
  title: simple-example-using-nmmaps
  date: 2013-10-10
  categories:
  - spatial dependence
  ---
  
  I will use the NMMAPSlite datasets for a simple example of what I am trying to do.
    

  <!-- begin_src R :session *R* :tangle NMMAPS-example/NMMAPS-example-code.r :exports none :eval no -->
   
  #### Code: get nmmaps data
      # func
      if(!require(NMMAPSlite)) install.packages('NMMAPSlite');require(NMMAPSlite)
      require(mgcv)
      require(splines)
  
      ######################################################
      # load  
      setwd('data')
      initDB('data/NMMAPS') # this requires that we connect to the web,
                            # so lets get local copies
      setwd('..')
      cities <- getMetaData('cities')
      head(cities)
      citieslist <- cities$cityname
      # write out a few cities for access later
      for(city_i in citieslist[sample(1:nrow(cities), 9)])
      {
       city <- subset(cities, cityname == city_i)$city
       data <- readCity(city)
       write.table(data, file.path('data', paste(city_i, '.csv',sep='')),
       row.names = F, sep = ',')
      }
      # these are all tiny, go some big ones
      for(city_i in c('New York', 'Los Angeles', 'Madison', 'Boston'))
      {
       city <- subset(cities, cityname == city_i)$city
       data <- readCity(city)
       write.table(data, file.path('data', paste(city_i, '.csv',sep='')),
       row.names = F, sep = ',')
      }
  
      ######################################################
      # now we can use these locally
      dir("data")
      city <- "Chicago"
      data <- read.csv(sprintf("data/%s.csv", city), header=T)
      str(data)
      data$yy <- substr(data$date,1,4)
      data$date <- as.Date(data$date)
      ######################################################
      # check
      par(mfrow=c(2,1), mar=c(4,4,3,1))
      with(subset(data[,c(1,15:25)], agecat == '75p'),
        plot(date, tmax)
       )
      with(subset(data[,c(1,4,15:25)], agecat == '75p'),
              plot(date, cvd, type ='l', col = 'grey')
              )
      with(subset(data[,c(1,4,15:25)], agecat == '75p'),
              lines(lowess(date, cvd, f = 0.015))
              )
      # I am worried about that outlier
      data$date[which(data$cvd > 100)]
      # [1] "1995-07-15" "1995-07-16"
       
      ######################################################
      # do standard NMMAPS timeseries poisson GAM model
      numYears<-length(names(table(data$yy)))
      df <- subset(data, agecat == '75p')
      df$time <- as.numeric(df$date)
      fit <- gam(cvd ~ s(pm10tmean) + s(tmax) + s(dptp) + s(time, k= 7*numYears, fx=T), data = df, family = poisson)
      # plot of response functions
      par(mfrow=c(2,2))
      plot(fit)
      dev.off()
       
      ######################################################
      # some diagnostics
      summary(fit)
      # note the R-sq.(adj) =   0.21
      gam.check(fit)
      # note the lack of a leverage plot.  for that we need glm
       
      ######################################################
      # do same model as glm
      fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = poisson)
      # plot responses
      par(mfrow=c(2,2))
      termplot(fit2, se =T)
      dev.off()
       
      # plot prediction
      df$predictedCvd <- predict(fit2, df, 'response')
      # baseline is given by the intercept
      fit3 <- glm(cvd ~ 1, data = df, family = poisson)
      df$baseline <-  predict(fit3, df, 'response')
      with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
       plot(date, cvd, type ='l', col = 'grey')
              )
      with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
              lines(date,predictedCvd)
              )
      with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
       lines(date,baseline)
              )
      ######################################################
      # some diagnostics
      # need to load a function to calculate poisson adjusted R squared
      # original S code from
      # The formula for pseudo-R^2 is taken from G. S. Maddalla,
      # Limited-dependent and Qualitative Variables in Econometrics, Cambridge:Cambridge Univ. Press, 1983. page 40, equation 2.50.
      RsquaredGlm <- function(o) {
       n <- length(o$residuals)
       ll <- logLik(o)[1]
       ll_0 <- logLik(update(o,~1))[1]
       R2 <- (1 - exp(((-2*ll) - (-2*ll_0))/n))/(1 - exp( - (-2*ll_0)/n))
       names(R2) <- 'pseudo.Rsquared'
       R2
       }
      RsquaredGlm(fit2)
      # 0.51
      # the difference is presumably due to the arguments about how to account for unexplainable variance in the poisson distribution?
       
      # significance of spline terms
      drop1(fit2, test='Chisq')
      # also note AIC. best model includes all of these terms
      # BIC can be computed instead (but still labelled AIC) using
      drop1(fit2, test='Chisq', k = log(nrow(data)))
       
      # diagnostic plots
      par(mfrow=c(2,2))
      plot(fit2)
      dev.off()
      # note high leverage plus residuals points are labelled
      # leverage doesn't seem to be too high though which is good
      # NB the numbers refer to the row.names attribute which still refer to the original dataset, not this subset
      df[row.names(df) %in% c(9354,9356),]$date
      # as suspected [1] "1995-07-15" "1995-07-16"
       
      ######################################################
      # so lets re run without these obs
      df2 <- df[!row.names(df) %in% c(9354,9356),]
      # to avoid duplicating code just re run fit2, replacing data=df with df2
      # tmax still significant but not so extreme
      # check diagnostic plots again
      par(mfrow=c(2,2))
      plot(fit2)
      dev.off()
      # looks like a well behaved model now.
       
      # if we were still worried about any high leverage values we could identify these with
      df3 <- na.omit(df2[,c('cvd','pm10tmean','tmax','dptp','time')])
      df3$hatvalue <- hatvalues(fit2)
      df3$res <- residuals(fit2, 'pearson')
      with(df3, plot(hatvalue, res))
      # this is the same as the fourth default glm diagnostic plot, which they label x-axis as leverage
      summary(df3$hatvalue)
      # gives us an idea of the distribution of hat values
      # decide on a threshold and look at it
      hatThreshold <- 0.1
      with(subset(df3, hatvalue > hatThreshold), points(hatvalue, res, col = 'red', pch = 16))
      abline(0,0)
      segments(hatThreshold,-2,hatThreshold,15)
      dev.off()
       
      fit3 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = subset(df3, hatvalue < hatThreshold), family = poisson)
      par(mfrow=c(2,2))
      termplot(fit3, se = T)
      # same same
      plot(fit3)
      # no better
       
      # or we could go nuts with a whole number of ways of estimating influence
      # check all influential observations
      infl <- influence.measures(fit2)
      # which observations 'are' influential
      inflk <- which(apply(infl$is.inf, 1, any))
      length(inflk)
       
       
      ######################################################
      # now what about serial autocorrelation in the residuals?
       
      par(mfrow = c(2,1))
      with(df3, acf(res))
      with(df3, pacf(res))
      dev.off()
       
       
       
      ######################################################
      # just check for overdispersion
      fit <- gam(cvd ~ s(pm10tmean) + s(tmax) + s(dptp) + s(time, k= 7*numYears, fx=T), data = df, family = quasipoisson)
      summary(fit)
      # note the Scale est. = 1.1627
      # alternatively check the glm
      fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = quasipoisson)
      summary(fit2)
      # (Dispersion parameter for quasipoisson family taken to be 1.222640)
      # this is probably near enough to support a standard poisson model...
       
      # if we have overdispersion we can use QAIC (A quasi- mode does not have a likelihood and so does not have an AIC,  by definition)
      # we can use the poisson model and calculate the overdispersion
      fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = poisson)
      1- pchisq(deviance(fit2), df.residual(fit2))
       
      # QAIC, c is the variance inflation factor, the ratio of the residual deviance of the global (most complicated) model to the residual degrees of freedom
      c=deviance(fit2)/df.residual(fit2)
      QAIC.1=-2*logLik(fit2)/c + 2*(length(coef(fit2)) + 1)
      QAIC.1
       
      # Actually lets use QAICc which is more conservative about parameters,
      QAICc.1=-2*logLik(fit2)/c + 2*(length(coef(fit2)) + 1) + 2*(length(coef(fit2)) + 1)*(length(coef(fit2)) + 1 + 1)/(nrow(na.omit(df[,c('cvd','pm10tmean','tmax','dptp','time')]))- (length(coef(fit2))+1)-1)
      QAICc.1
       
       
      ######################################################
      # the following is old work, some may be interesting
      # such as the use of sinusoidal wave instead of smooth function of time
       
       
      # # sine wave
      # timevar <- as.data.frame(names(table(df$date)))
      # index <- 1:length(names(table(df$date)))
      # timevar$time2 <- index / (length(index) / (length(index)/365.25))
      # names(timevar) <- c('date','timevar')
      # timevar$date <- as.Date(timevar$date)
      # df <- merge(df,timevar)
       
      # fit <- gam(cvd ~ s(tmax) + s(dptp) + sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
      # summary(fit)
      # par(mfrow=c(3,2))
      # plot(fit, all.terms = T)
      # dev.off()
       
      # # now just explore the season fit
      # fit <- gam(cvd ~ sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
      # yhat <- predict(fit)
      # head(yhat)
       
      # with(df, plot(date,cvd,type = 'l',col='grey', ylim = c(15,55)))
      # lines(df[,'date'],exp(yhat),col='red')
       
       
      # # drop1(fit, test= 'Chisq')
       
      # # drop1 only works in glm?
      # # fit with weather variables, use degrees of freedom estimated by gam
      # fit <- glm(cvd ~ ns(tmax,8) + ns(dptp,2) + sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
      # drop1(fit, test= 'Chisq')
      # # use plot.glm for diagnostics
      # par(mfrow=c(2,2))
      # plot(fit)
      # par(mfrow=c(3,2))
      # termplot(fit, se=T)
      # dev.off()
       
      # # cyclic spline, overlay on prior sinusoidal
      # with(df, plot(date,cvd,type = 'l',col='grey', ylim = c(0,55)))
      # lines(df[,'date'],exp(yhat),col='red')
       
      # df$daynum <- as.numeric(format(df$date, "%j"))
      # df[360:370,c('date','daynum')]
      # fit <- gam(cvd ~ s(daynum, k=3, fx=T, bs = 'cp') +  s(time, k = numYears, fx = T), data = df, family = poisson)
      # yhat2 <- predict(fit)
      # head(yhat2)
       
      # lines(df[,'date'],exp(yhat2),col='blue')
       
       
      # par(mfrow=c(1,2))
      # plot(fit)
       
       
      # # fit weather with season
      # fit <- gam(cvd ~ s(tmax) + s(dptp) +
        # s(daynum, k=3, fx=T, bs = 'cp') +  s(time, k = numYears, fx = T), data = df, family = poisson)
      # par(mfrow=c(2,2))
      # plot(fit)
       
      # summary(fit)
  
#+end_src
** COMMENT 2013-10-16-spatially-structured-time-series-with-nmmaps
#+name:spatially-structured-time-series-with-nmmaps-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-10-16-spatially-structured-time-series-with-nmmaps.md :exports none :eval no :padline no
  ---
  name: spatially-structured-time-series-with-nmmaps
  layout: post
  title: spatially-structured-time-series-with-nmmaps
  date: 2013-10-16
  categories:
  - spatial dependence
  ---
  
  I will use the NMMAPSlite datasets for a simple example of what I
  describe as "Spatially Structured Timeseries" as opposed to
  "Spatio-Temporal" which I think more explicitly includes spatial
  structure in the model.  [See This Report](http://ivanhanigan.github.io/spatiotemporal-regression-models/) for all the gory details.
  
  # R Codes
  
  <!-- <?xml version="1.0" encoding="utf-8"?> -->
  <!-- <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" -->
  <!--                "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"> -->
  <!-- <html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"> -->
  <head>
  <title>Spatiotemporal Regression Modelling</title>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
  <meta name="title" content="Spatiotemporal Regression Modelling"/>
  <meta name="generator" content="Org-mode"/>
  <meta name="generated" content="2013-10-16T15:17+1100"/>
  <meta name="author" content="Ivan Hanigan"/>
  <meta name="description" content=""/>
  <meta name="keywords" content=""/>
  <style type="text/css">
   <!--/*--><![CDATA[/*><!--*/
    html { font-family: Times, serif; font-size: 12pt; }
    .title  { text-align: center; }
    .todo   { color: red; }
    .done   { color: green; }
    .tag    { background-color: #add8e6; font-weight:normal }
    .target { }
    .timestamp { color: #bebebe; }
    .timestamp-kwd { color: #5f9ea0; }
    .right  {margin-left:auto; margin-right:0px;  text-align:right;}
    .left   {margin-left:0px;  margin-right:auto; text-align:left;}
    .center {margin-left:auto; margin-right:auto; text-align:center;}
    p.verse { margin-left: 3% }
    pre {
      border: 1pt solid #AEBDCC;
      background-color: #F3F5F7;
      padding: 5pt;
      font-family: courier, monospace;
          font-size: 90%;
          overflow:auto;
    }
    table { border-collapse: collapse; }
    td, th { vertical-align: top;  }
    th.right  { text-align:center;  }
    th.left   { text-align:center;   }
    th.center { text-align:center; }
    td.right  { text-align:right;  }
    td.left   { text-align:left;   }
    td.center { text-align:center; }
    dt { font-weight: bold; }
    div.figure { padding: 0.5em; }
    div.figure p { text-align: center; }
    div.inlinetask {
      padding:10px;
      border:2px solid gray;
      margin:10px;
      background: #ffffcc;
    }
    textarea { overflow-x: auto; }
    .linenr { font-size:smaller }
    .code-highlighted {background-color:#ffff00;}
    .org-info-js_info-navigation { border-style:none; }
    #org-info-js_console-label { font-size:10px; font-weight:bold;
                                 white-space:nowrap; }
    .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                   font-weight:bold; }
    /*]]>*/-->
  </style>
  <script type="text/javascript">
  /*
  @licstart  The following is the entire license notice for the
  JavaScript code in this tag.
  
  Copyright (C) 2012-2013 Free Software Foundation, Inc.
  
  The JavaScript code in this tag is free software: you can
  redistribute it and/or modify it under the terms of the GNU
  General Public License (GNU GPL) as published by the Free Software
  Foundation, either version 3 of the License, or (at your option)
  any later version.  The code is distributed WITHOUT ANY WARRANTY;
  without even the implied warranty of MERCHANTABILITY or FITNESS
  FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.
  
  As additional permission under GNU GPL version 3 section 7, you
  may distribute non-source (e.g., minimized or compacted) forms of
  that code without the copy of the GNU GPL normally required by
  section 4, provided you include this license notice and a URL
  through which recipients can access the Corresponding Source.
  
  
  @licend  The above is the entire license notice
  for the JavaScript code in this tag.
  ,*/
  <!--/*--><![CDATA[/*><!--*/
   function CodeHighlightOn(elem, id)
   {
     var target = document.getElementById(id);
     if(null != target) {
       elem.cacheClassElem = elem.className;
       elem.cacheClassTarget = target.className;
       target.className = "code-highlighted";
       elem.className   = "code-highlighted";
     }
   }
   function CodeHighlightOff(elem, id)
   {
     var target = document.getElementById(id);
     if(elem.cacheClassElem)
       elem.className = elem.cacheClassElem;
     if(elem.cacheClassTarget)
       target.className = elem.cacheClassTarget;
   }
  /*]]>*///-->
  </script>
  
  </head>
  <body>
  
  <div id="preamble">
  
  </div>
  
  <div id="content">
  <h1 class="title">Spatiotemporal Regression Modelling</h1>
  
  
  <div id="table-of-contents">
  <h2>Table of Contents</h2>
  <div id="text-table-of-contents">
  <ul>
  <li><a href="#sec-1">1 Core Model</a></li>
  <li><a href="#sec-2">2 Core Model Plots</a></li>
  </ul>
  </div>
  </div>
  
  <div id="outline-container-1" class="outline-4">
  <h4 id="sec-1"><span class="section-number-4">1</span> Core Model</h4>
  <div class="outline-text-4" id="text-1">
  
  
  
  
  <pre class="src src-R"><span style="color: #5F7F5F;">################################################################</span>
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">name:core</span>
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">func</span>
  setwd(<span style="color: #CC9393;">"~/projects/spatiotemporal-regression-models/NMMAPS-example"</span>)
  <span style="color: #BFEBBF; font-weight: bold;">require</span>(mgcv)
  <span style="color: #BFEBBF; font-weight: bold;">require</span>(splines)
  
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">load</span>
  analyte <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> read.csv(<span style="color: #CC9393;">"analyte.csv"</span>)
  
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">clean</span>
  analyte$yy <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> substr(analyte$date,1,4)
  numYears<span style="color: #BFEBBF; font-weight: bold;">&lt;-</span>length(names(table(analyte$yy)))
  analyte$date <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> as.Date(analyte$date)
  analyte$time <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> as.numeric(analyte$date)
  analyte$agecat <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> factor(analyte$agecat,
                            levels = c(<span style="color: #CC9393;">"under65"</span>,
                                <span style="color: #CC9393;">"65to74"</span>, <span style="color: #CC9393;">"75p"</span>),
                            ordered = <span style="color: #7CB8BB;">TRUE</span>
                            )
  
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">do</span>
  fit <span style="color: #BFEBBF; font-weight: bold;">&lt;-</span> gam(cvd ~ s(tmax) + s(dptp) +
             city + agecat +
             s(time, k= 7*numYears, fx=T) +
             offset(log(pop)),
             data = analyte, family = poisson
             )
  
  <span style="color: #5F7F5F;"># </span><span style="color: #7F9F7F;">plot of response functions</span>
  png(<span style="color: #CC9393;">"images/nmmaps-eg-core.png"</span>, width = 1000, height = 750, res = 150)
  par(mfrow=c(2,3))
  plot(fit, all.terms = <span style="color: #7CB8BB;">TRUE</span>)
  dev.off()
  
  
  </pre>
  
  </div>
  
  </div>
  
  <div id="outline-container-2" class="outline-4">
  <h4 id="sec-2"><span class="section-number-4">2</span> Core Model Plots</h4>
  <div class="outline-text-4" id="text-2">
  
  <p><img src="/images/nmmaps-eg-core.png"  alt="/images/nmmaps-eg-core.png" />
  </p></div>
  </div>
  </div>
  
  </body>
  </html>
  
  
#+end_src
# ![nmmaps-eg-core.png](/images/nmmaps-eg-core.png)
* Case Study 1: Spatially Structured Timeseries with NMMAPS
I will use the NMMAPSlite datasets for a simple example of what I describe as "Spatially Structured Timeseries" as opposed to "Spatio-Temporal" which I think more explicitly includes spatial structure in the model.
** Outcome Data
*** Original Data
# NB The original data are not available anymore
#+name:md
#+begin_src R :tangle no :exports reports :eval no :tangle NMMAPS-example/NMMAPS-original-data.r 
"
http://cran.r-project.org/web/packages/NMMAPSlite/index.html
Package NMMAPSlite was removed from the CRAN repository.
Formerly available versions can be obtained from the archive.
Archived on 2013-05-11 at the request of the maintainer.

The Archived versions do not seem to work either.

That is such a shame, lucky I saved some of the data using the following code:
"
#+end_src




#+begin_src R :session *R* :tangle NMMAPS-example/NMMAPS-original-data.r :exports reports :eval no
   
      #### Code: get nmmaps data
      # func
      if(!require(NMMAPSlite)) install.packages('NMMAPSlite');require(NMMAPSlite)

      ######################################################
      # load  
      setwd('data')
      initDB('data/NMMAPS') # this requires that we connect to the web,
                            # so lets get local copies
      setwd('..')
      cities <- getMetaData('cities')
      head(cities)
      citieslist <- cities$cityname
      # write out a few cities for access later
      for(city_i in citieslist[sample(1:nrow(cities), 9)])
      {
       city <- subset(cities, cityname == city_i)$city
       data <- readCity(city)
       write.table(data, file.path('data', paste(city_i, '.csv',sep='')),
       row.names = F, sep = ',')
      }
      # these are all tiny, go some big ones
      for(city_i in c('New York', 'Los Angeles', 'Madison', 'Boston'))
      {
       city <- subset(cities, cityname == city_i)$city
       data <- readCity(city)
       write.table(data, file.path('data', paste(city_i, '.csv',sep='')),
       row.names = F, sep = ',')
      }
#+end_src
*** Pooled Dataset
#+name:Pooled Dataset
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:Pooled Dataset
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")

  flist <- dir("data", full.names=T)
  
  flist <-    flist[which(basename(flist) %in%
                          c("Baton Rouge.csv",
                            "Los Angeles.csv",
                            "Tucson.csv",
                            "Denver.csv")
                          )
                    ]
  flist
  for(f_i in 1:length(flist))
      {
          #f_i <- 2
          fi <- flist[f_i]
          df <- read.csv(fi)
          df <- df[,c("city","date", "agecat",
                      "cvd", "resp", "tmax",
                      "tmin", "dptp")]
          # str(df)
          write.table(df,
                      "outcome.csv", sep = ",",
                      row.names = F, append = f_i > 1,
                      col.names = f_i ==1
                      )
      }
  
#+end_src

** Exposure Data
** Zones
*** Map Shapefile Code
#+name:zones
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:zones
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
  #require(devtools)
  #install_github("gisviz", "ivanhanigan")
  require(gisviz)
  
  # load
  flist <- dir("data")
  
  # do    
  ## geocode
  flist <- gsub(".csv", "", flist)    
  flist_geo <- gGeoCode2(flist)
  flist_geo
  
  ## plot
  png("images/nmmaps-eg-cities.png")
  plotMyMap(
      flist_geo[,c("long","lat")],
      xl = c(-130,-60), yl = c(25,50)
      )
  text(flist_geo$long, flist_geo$lat, flist_geo$address, pos=3)
  dev.off()
     
  # load city names
  flist2 <- dir("data")
  flist2
  city_codes <- matrix(NA, nrow = 0, ncol = 2)
  for(fi in 1:length(flist2))
      {
          # fi <- 1
          fname <- flist2[fi]
          print(fi); print(fname);
          df <- read.csv(
              file.path("data", fname),
              stringsAsFactors = F, nrow = 1)
          city_codes <- rbind(city_codes,
              c(gsub(".csv","",fname), df$city)
              )
      }
  city_codes <- as.data.frame(city_codes)
  names(city_codes) <- c("address","city")
  city_codes 
  flist_geo2 <- merge(flist_geo, city_codes, by = "address")
  flist_geo2
  
  ## make shapefile
  epsg <- make_EPSG()
  prj_code <- epsg[grep("WGS 84$", epsg$note),]
  prj_code
  shp <- SpatialPointsDataFrame(cbind(flist_geo2$long,flist_geo2$lat),flist_geo2,
                                proj4string = CRS(
                                    epsg$prj4[which(epsg$code ==prj_code$code)]
                                    )
                                )
  writeOGR(shp, 'cities.shp', 'cities', driver='ESRI Shapefile')

#+end_src
*** Map Shapefile Output
[[file:NMMAPS-example/images/nmmaps-eg-cities.png]]

** Population Data
*** Population data 
**** A Code Skeleton
The process for getting the population data from the websites is a bit of a pain, with repeated copy and paste operations.  To make sure this is recorded I set up a skeleton to paste into.
**** TODO better to use gsub() to remove the ","
#+name:population-data
#+begin_src R :session *shell* :tangle no :exports reports :eval no
  ################################################################
  # name:population-data
  
  # func
  setwd("~/projects/spatiotemporal-regression-models")
  require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  cityname <- "SELECTED CITY"
  
  # do
  # save url
  # xxx
  population_input <- read.table(textConnection(
  "age: population
  xxx
  "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src


**** Baton Rouge
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval yes
  ################################################################
  # name:population-data
  # first city, remove population data.frame
  rm(population)
  
  # func
  setwd("~/projects/spatiotemporal-regression-models")
  require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  cityname <- "Baton Rouge"
  
  # do
  # save url
  # http://www.city-data.com/us-cities/The-South/Baton-Rouge-Population-Profile.html
  population_input <- read.table(textConnection(
  "age: population
  Population Under 5 years: 15502
  Population 5 to 9 years: 15609
  Population 10 to 14 years: 15248
  Population 15 to 19 years: 21954
  Population 20 to 24 years: 27230
  Population 25 to 34 years: 31719
  Population 35 to 44 years: 30343
  Population 45 to 54 years: 27166
  Population 55 to 59 years: 9495
  Population 60 to 64 years: 7490
  Population 65 to 74 years: 13312
  Population 75 to 84 years: 9611
  Population 85 years and over: 3139
  "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src

**** Los Angeles
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:population-data
  
  # func
  #setwd("~/projects/spatiotemporal-regression-models")
  #require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  cityname <- "Los Angeles"
  
  # do
  # save url
  # http://www.city-data.com/us-cities/The-West/Los-Angeles-Population-Profile.html
  population_input <- read.table(textConnection(
  "age: population
  Population under 5 years old: 285976
  Population 5 to 9 years old: 297837
  Population 10 to 14 years old: 255604
  Population 15 to 19 years old: 251632
  Population 20 to 24 years old: 299906
  Population 25 to 34 years old: 674098
  Population 35 to 44 years old: 584036
  Population 45 to 54 years old: 428974
  Population 55 to 59 years old: 143965
  Population 60 to 64 years old: 115663
  Population 65 to 74 years old: 18711
  Population 75 to 84 years old: 125829
  Population 85 years and over: 44189
  "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src


**** Tucson
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:population-data
  
  # func
  #setwd("~/projects/spatiotemporal-regression-models")
  #require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  cityname <- "Tucson"
  
  # do
  # save url
  # http://www.city-data.com/us-cities/The-West/Tucson-Population-Profile.html
  population_input <- read.table(textConnection(
  "age: population
  Population under 5 years old: 35201
  Population 5 to 9 years old: 34189
  Population 10 to 14 years old: 31939
  Population 15 to 19 years old: 38170
  Population 20 to 24 years old: 47428
  Population 25 to 34 years old: 76394
  Population 35 to 44 years old: 72289
  Population 45 to 54 years old: 57608
  Population 55 to 59 years old: 19597
  Population 60 to 64 years old: 16056
  Population 65 to 74 years old: 29117
  Population 75 to 84 years old: 21394
  Population 85 years and older: 7317
  "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src

**** Denver
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:population-data
  
  # func
  #setwd("~/projects/spatiotemporal-regression-models")
  #require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  cityname <- "Denver"
  
  # do
  # save url
  # http://www.city-data.com/us-cities/The-West/Denver-Population-Profile.html
  
  population_input <- read.table(textConnection(
  "age: population
  Population under 5 years old: 37769
  Population 5 to 9 years old: 34473
  Population 10 to 14 years old: 31315
  Population 15 to 19 years old: 32259
  Population 20 to 24 years old: 45534
  Population 25 to 34 years old: 113676
  Population 35 to 44 years old: 86420
  Population 45 to 54 years old: 71000
  Population 55 to 59 years old: 22573
  Population 60 to 64 years old: 17191
  Population 65 to 74 years old: 30643
  Population 75 to 84 years old: 23369
  Population 85 years and over: 8414
  "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src
**** Louisville
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:population-data
  
  # func
  setwd("~/projects/spatiotemporal-regression-models")
  require(gisviz)
  
  # load
  ## citycodes
  shp <- readOGR("cities.shp", "cities")
  shp@data
  
  cityname <- "Louisville"
  
  # do
  # save url
  #http://www.city-data.com/us-cities/The-South/Louisville-Population-Profile.html
  
  ## NB error in table, 75-84 was missing.  used same from baton rouge
  population_input <- read.table(textConnection(
    "age: population
    Population under 5 years old: 16926
    Population 5 to 9 years old: 17359
    Population 10 to 14 years old: 16627
    Population 15 to 19 years old: 17362
    Population 20 to 24 years old: 18923
    Population 25 to 34 years old: 37541
    Population 35 to 44 years old: 40354
    Population 45 to 54 years old: 33755
    Population 55 to 59 years old: 10716
    Population 60 to 64 years old: 9211
    Population 65 to 74 years old: 18577
    Population 75 to 84 years: 9611
    Population 85 years and older: 5075
    "), sep = ":", header = TRUE)
  
  ## agecats
  ## 65to74     75p under65  
  population_input$agecat <- c(rep("under65", 10),
                               rep("65to74"),
                               rep("75p",2)
                               )
  
  citycode <- subset(shp@data, address == cityname,
                     select = city)
  
  population_input$city <- rep(
      as.character(citycode[1,1])        
      , 13
      )
  
  population_input
  if(exists("population"))
      {
          population <- rbind(population,
                              population_input
                              )        
      } else {
          population <- population_input
      }
  
#+end_src


**** Save Population Dataset
#+name:save-population
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:save-population
  write.csv(population, "population.csv",
            row.names = FALSE
            )
#+end_src

*** Population Summary 
#+name:Population Summary
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:Population Summary
  # func
  require(plyr)
  
  # load
  population <- read.csv("population.csv")
  str(population)
  population <- population[,c("city", "agecat", "population")]
  # do
  population_summary <- ddply(population,
                             .variables = c("city",
                                 "agecat"),
                             .fun = summarise,
                             sum(population)
                             )
  
  names(population_summary) <- c("city","agecat","pop")
  
  write.csv(population_summary,
            "population_summary.csv",
            row.names = FALSE
            )
#+end_src

** Merge

#+name:merge
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:merge
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
  
  # load
  outcome <- read.csv("outcome.csv")
  str(outcome)
  outcome$date <- as.Date(outcome$date)
  
  population <- read.csv("population_summary.csv")
  str(population)
  population
  
  # do
  analyte <- merge(outcome, population, by = c("city", "agecat"))
  analyte <- arrange(analyte, city, date, agecat)
  # check
  subset(analyte, date == as.Date("1990-01-01"))
  
  # save
  write.csv(analyte, "analyte.csv", row.names = FALSE)
#+end_src

** Data Checking
*** Check Assumption of Proportional Hazards
#+begin_src R :session *R* :tangle NMMAPS-example/case-study-1-data-checking.r :exports reports :eval no
  # Checking the proportional hazards assumption for Indirect Age Standardisation
  
  # Background
  ## Indirect SMRs from different index/study populations are not strictly comparable
  ## because they are calculated using different weighting schemes that
  ## depend upon the age structures of the index/study populations
  ## (see http://www.statsdirect.com/webhelp/#rates/smr.htm).
  
  ## Indirect SMRs can be compared if you make the assumption that the
  ## ratio of rates between index and reference populations is constant;
  ## this is similar to the assumption of proportional hazards in Cox
  ## regression (Armitage and Berry, 1994).
  
  ## So we need to check if the rate ratio of the study population(s)
  ## compared to the standard population varies substantially with age.
  ## If not the proportional hazards assumption holds for the standard
  ## rates compared with the observed rates and the Indirect SMRs are
  ## comparable.  To do this check we calculate the Annualised Age Specific
  ## Rates for our study areas and for our standard for several years at
  ## periodic timepoints across the study period, and then calculate the
  ## ratio of these at each timepoint we could reassure our selves that
  ## this assumption holds.
  
  ## An additional issue arises when there are non-negligible differences
  ## in the age distributions of the study population(s) and the standard
  ## population. In this situation, indirect standardisation produces
  ## biased results due to residual confounding by age
  
  ## Also see Australian Institute of Health and Welfare. (2011). Principles on
  ## the use of direct age-standardisation in administrative data
  ## collections For measuring the gap between Indigenous and
  ## non-Indigenous Australians. Data linkage series. Cat. no. CSI 12.
  ## Canberra: AIHW. Retrieved from
  ## http://www.aihw.gov.au/publication-detail/?id=10737420133
  
  ################################################################
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
  require(plyr)
  
  # load
  analyte <- read.csv("analyte.csv")
  
  # clean
  head(analyte)
  analyte$yy  <- substr(analyte$date, 1, 4)
    
  # do
  ## first we want to see if the age specific rates vary across the
  ## study sites
  disease_colname <- "cvd"
  pop_colname <- "pop"
  by_cols  <- c("city", "agecat", "yy")
  
  stdysites  <- ddply(analyte, by_cols,
                      function(df) return(
                        c(observed = sum(df[,disease_colname]),
                          pop = mean(df[,pop_colname]),
                          crude.rate = sum(df[,disease_colname])/mean(df[,pop_colname])
                          )
                        )
                      )
  ## check
  head(stdysites)
  subset(stdysites, yy == 1987 & city == "batr")
  
  ## define the subset we will use just the deaths and pop in 2000
  stdy <- subset(stdysites, yy == 2000)
  stdy
  
  ## now define the standard population as the entire country
  standard  <- ddply(stdy, c("agecat"),
                       function(df) return(
                         c(observed = sum(df[,"observed"]),
                           pop = sum(df[,"pop"]),
                           crude.rate = sum(df[,"observed"])/
                             sum(df[,"pop"])
                           )
                         )
                      )
  standard
  
  ## Merge the studysites and the standard
  stdyByStnd  <- merge(stdy, standard, by = "agecat")
  stdyByStnd
  
  ## plot the rate ratios
  png("images/ratio-stdy-by-stnd.png")
  mp <- barplot(stdyByStnd$crude.rate.x/stdyByStnd$crude.rate.y)
  text(mp, par("usr")[3],
       labels = paste(stdyByStnd$agecat,stdyByStnd$city),
       srt = 45, adj = c(1.1,1.1), xpd = TRUE
       )
  abline(1,0)
  dev.off()
  
  ## we can see that LA has an issue with the 65to74 agecat
  
  ## Second we will check if ratio of the proportions in each population
  ## agecat vary between study sites and the standard
  totals <- ddply(stdyByStnd, c("city"), summarise,
                  sum(pop.x)
                  )
  totals <- merge(stdyByStnd[,c("city","agecat","pop.x")], totals)
  totals$pop.wt  <- totals[,3] / totals[,4]
  totals <- arrange(totals, city, agecat)
  totals
  
  totalsStnd <- ddply(stdyByStnd, c("agecat"), summarise,
                  sum(pop.x)
                  )
  totalsStnd$totalPop <- sum(totalsStnd[,2])
  totalsStnd$pop.wt.total <- totalsStnd[,2]/totalsStnd[,3]
  totalsStnd
  
  ## merge these so we can look at the ratios
  totals2 <- merge(totals, totalsStnd, by = "agecat")
  totals2 <- arrange(totals2, agecat, city)
  
  ## now plot the ratios
  png("images/ratio-stdy-by-stnd-pops.png")
  mp <- barplot(totals2$pop.wt/totals2$pop.wt.total)
  text(mp, par("usr")[3],
       labels = paste(totals2$agecat,totals2$city),
       srt = 45, adj = c(1.1,1.1), xpd = TRUE
       )
  abline(1,0)
  dev.off()
  
  ## and we can see that there are far fewer 65to74 aged persons in LA
  ## than expected.
  
  
#+end_src

*** Incidence Rate Ratios Plot
[[file:NMMAPS-example/images/ratio-stdy-by-stnd.png]]
*** Population Rate Ratios Plot
[[file:NMMAPS-example/images/ratio-stdy-by-stnd-pops.png]]
    
** Exploratory Data Analyses
*** Time-series Plots Code
#+name:eda-tsplots
#+begin_src R :session *R* :tangle NMMAPS-example/case-study-1-load.r :exports reports :eval no
  ################################################################
  # name:eda-tsplots
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
    
  # load
  flist <- dir("data")
  fname <- flist[7]; print(fname)
  df <- read.csv(file.path("data", fname))
  
  # clean
  str(df)
  summary(df$tmax); summary(df$dptp)
  
  # do
  ## we will only consider cities with long periods temp and humidity observed
  png("images/nmmaps-eg-dateranges.png", width = 1000, height=500, res = 150)
  par(mfrow=c(6,5), mar=c(0,3,3,0), cex=.25)
  for(fi in 1:length(flist))
      {
          #fi <- 4
          fname <- flist[fi]
          df <- read.csv(file.path("data", fname))
          print(fi); print(fname);
          with(df, plot(as.Date(date), tmax, type = "l"))
          title(paste(fname, "tmax"))
          with(df, plot(as.Date(date), dptp, type = "l"))
          title(paste(fname, "dptp"))
      }
  dev.off()
  
#+end_src
*** Time-series Plots Output
[[file:NMMAPS-example/images/nmmaps-eg-dateranges.png]]

** Main Analyses
*** Core Model
#+name:core
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-1-do.r :exports reports :eval no
  ################################################################
  # name:core
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
  require(mgcv)
  require(splines)
  
  # load
  analyte <- read.csv("analyte.csv")
  
  # clean
  analyte$yy <- substr(analyte$date,1,4)
  numYears<-length(names(table(analyte$yy)))
  analyte$date <- as.Date(analyte$date)
  analyte$time <- as.numeric(analyte$date)
  analyte$agecat <- factor(analyte$agecat,
                            levels = c("under65",
                                "65to74", "75p"),
                            ordered = TRUE
                            )
  
  # do
  fit <- gam(cvd ~ s(tmax) + s(dptp) +
             city + agecat +
             s(time, k= 7*numYears, fx=T) +
             offset(log(pop)),
             data = analyte, family = poisson
             )
  
  # plot of response functions
  png("images/nmmaps-eg-core.png", width = 1000, height = 750, res = 150)
  par(mfrow=c(2,3))
  plot(fit, all.terms = TRUE)
  dev.off()
  
  
#+end_src
*** Core Model Plots
[[file:NMMAPS-example/images/nmmaps-eg-core.png]]

*** Model Selection
#+begin_src R :session *R* :tangle NMMAPS-example/case-study-1-model-selection.r :exports reports :eval no
     
  # The following codes are just a dump of stuff I've found useful in the past.
  # This is all time-series methods.
  # TODO replace the city-specific model with a pooled analysis with city specific trend.
  
  # func
  require(mgcv)
  require(splines)
    
  ######################################################
  # load  
  dir("data")
  city <- "Chicago"
  data <- read.csv(sprintf("data/%s.csv", city), header=T)
  str(data)
  data$yy <- substr(data$date,1,4)
  data$date <- as.Date(data$date)
  ######################################################
  # check
  par(mfrow=c(2,1), mar=c(4,4,3,1))
  with(subset(data[,c(1,15:25)], agecat == '75p'),
    plot(date, tmax)
   )
  with(subset(data[,c(1,4,15:25)], agecat == '75p'),
          plot(date, cvd, type ='l', col = 'grey')
          )
  with(subset(data[,c(1,4,15:25)], agecat == '75p'),
          lines(lowess(date, cvd, f = 0.015))
          )
  # I am worried about that outlier
  data$date[which(data$cvd > 100)]
  # [1] "1995-07-15" "1995-07-16"
   
  ######################################################
  # do standard NMMAPS timeseries poisson GAM model
  numYears<-length(names(table(data$yy)))
  df <- subset(data, agecat == '75p')
  df$time <- as.numeric(df$date)
  fit <- gam(cvd ~ s(pm10tmean) + s(tmax) + s(dptp) + s(time, k= 7*numYears, fx=T), data = df, family = poisson)
  # plot of response functions
  par(mfrow=c(2,2))
  plot(fit)
  dev.off()
   
  ######################################################
  # some diagnostics
  summary(fit)
  # note the R-sq.(adj) =   0.21
  gam.check(fit)
  # note the lack of a leverage plot.  for that we need glm
   
  ######################################################
  # do same model as glm
  fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = poisson)
  # plot responses
  par(mfrow=c(2,2))
  termplot(fit2, se =T)
  dev.off()
   
  # plot prediction
  df$predictedCvd <- predict(fit2, df, 'response')
  # baseline is given by the intercept
  fit3 <- glm(cvd ~ 1, data = df, family = poisson)
  df$baseline <-  predict(fit3, df, 'response')
  with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
   plot(date, cvd, type ='l', col = 'grey')
          )
  with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
          lines(date,predictedCvd)
          )
  with(subset(df, date>=as.Date('1995-01-01') & date <= as.Date('1995-07-31')),
   lines(date,baseline)
          )
  ######################################################
  # some diagnostics
  # need to load a function to calculate poisson adjusted R squared
  # original S code from
  # The formula for pseudo-R^2 is taken from G. S. Maddalla,
  # Limited-dependent and Qualitative Variables in Econometrics, Cambridge:Cambridge Univ. Press, 1983. page 40, equation 2.50.
  RsquaredGlm <- function(o) {
   n <- length(o$residuals)
   ll <- logLik(o)[1]
   ll_0 <- logLik(update(o,~1))[1]
   R2 <- (1 - exp(((-2*ll) - (-2*ll_0))/n))/(1 - exp( - (-2*ll_0)/n))
   names(R2) <- 'pseudo.Rsquared'
   R2
   }
  RsquaredGlm(fit2)
  # 0.51
  # the difference is presumably due to the arguments about how to account for unexplainable variance in the poisson distribution?
   
  # significance of spline terms
  drop1(fit2, test='Chisq')
  # also note AIC. best model includes all of these terms
  # BIC can be computed instead (but still labelled AIC) using
  drop1(fit2, test='Chisq', k = log(nrow(data)))
   
  # diagnostic plots
  par(mfrow=c(2,2))
  plot(fit2)
  dev.off()
  # note high leverage plus residuals points are labelled
  # leverage doesn't seem to be too high though which is good
  # NB the numbers refer to the row.names attribute which still refer to the original dataset, not this subset
  df[row.names(df) %in% c(9354,9356),]$date
  # as suspected [1] "1995-07-15" "1995-07-16"
   
  ######################################################
  # so lets re run without these obs
  df2 <- df[!row.names(df) %in% c(9354,9356),]
  # to avoid duplicating code just re run fit2, replacing data=df with df2
  # tmax still significant but not so extreme
  # check diagnostic plots again
  par(mfrow=c(2,2))
  plot(fit2)
  dev.off()
  # looks like a well behaved model now.
   
  # if we were still worried about any high leverage values we could identify these with
  df3 <- na.omit(df2[,c('cvd','pm10tmean','tmax','dptp','time')])
  df3$hatvalue <- hatvalues(fit2)
  df3$res <- residuals(fit2, 'pearson')
  with(df3, plot(hatvalue, res))
  # this is the same as the fourth default glm diagnostic plot, which they label x-axis as leverage
  summary(df3$hatvalue)
  # gives us an idea of the distribution of hat values
  # decide on a threshold and look at it
  hatThreshold <- 0.1
  with(subset(df3, hatvalue > hatThreshold), points(hatvalue, res, col = 'red', pch = 16))
  abline(0,0)
  segments(hatThreshold,-2,hatThreshold,15)
  dev.off()
   
  fit3 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = subset(df3, hatvalue < hatThreshold), family = poisson)
  par(mfrow=c(2,2))
  termplot(fit3, se = T)
  # same same
  plot(fit3)
  # no better
   
  # or we could go nuts with a whole number of ways of estimating influence
  # check all influential observations
  infl <- influence.measures(fit2)
  # which observations 'are' influential
  inflk <- which(apply(infl$is.inf, 1, any))
  length(inflk)
   
   
  ######################################################
  # now what about serial autocorrelation in the residuals?
   
  par(mfrow = c(2,1))
  with(df3, acf(res))
  with(df3, pacf(res))
  dev.off()
   
   
   
  ######################################################
  # just check for overdispersion
  fit <- gam(cvd ~ s(pm10tmean) + s(tmax) + s(dptp) + s(time, k= 7*numYears, fx=T), data = df, family = quasipoisson)
  summary(fit)
  # note the Scale est. = 1.1627
  # alternatively check the glm
  fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = quasipoisson)
  summary(fit2)
  # (Dispersion parameter for quasipoisson family taken to be 1.222640)
  # this is probably near enough to support a standard poisson model...
   
  # if we have overdispersion we can use QAIC (A quasi- mode does not have a likelihood and so does not have an AIC,  by definition)
  # we can use the poisson model and calculate the overdispersion
  fit2 <- glm(cvd ~ pm10tmean + ns(tmax, df = 8) + ns(dptp, df = 4) + ns(time, df = 7*numYears), data = df, family = poisson)
  1- pchisq(deviance(fit2), df.residual(fit2))
   
  # QAIC, c is the variance inflation factor, the ratio of the residual deviance of the global (most complicated) model to the residual degrees of freedom
  c=deviance(fit2)/df.residual(fit2)
  QAIC.1=-2*logLik(fit2)/c + 2*(length(coef(fit2)) + 1)
  QAIC.1
   
  # Actually lets use QAICc which is more conservative about parameters,
  QAICc.1=-2*logLik(fit2)/c + 2*(length(coef(fit2)) + 1) + 2*(length(coef(fit2)) + 1)*(length(coef(fit2)) + 1 + 1)/(nrow(na.omit(df[,c('cvd','pm10tmean','tmax','dptp','time')]))- (length(coef(fit2))+1)-1)
  QAICc.1
   
   
  ######################################################
  # the following is old work, some may be interesting
  # such as the use of sinusoidal wave instead of smooth function of time
   
   
  # # sine wave
  # timevar <- as.data.frame(names(table(df$date)))
  # index <- 1:length(names(table(df$date)))
  # timevar$time2 <- index / (length(index) / (length(index)/365.25))
  # names(timevar) <- c('date','timevar')
  # timevar$date <- as.Date(timevar$date)
  # df <- merge(df,timevar)
   
  # fit <- gam(cvd ~ s(tmax) + s(dptp) + sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
  # summary(fit)
  # par(mfrow=c(3,2))
  # plot(fit, all.terms = T)
  # dev.off()
   
  # # now just explore the season fit
  # fit <- gam(cvd ~ sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
  # yhat <- predict(fit)
  # head(yhat)
   
  # with(df, plot(date,cvd,type = 'l',col='grey', ylim = c(15,55)))
  # lines(df[,'date'],exp(yhat),col='red')
   
   
  # # drop1(fit, test= 'Chisq')
   
  # # drop1 only works in glm?
  # # fit with weather variables, use degrees of freedom estimated by gam
  # fit <- glm(cvd ~ ns(tmax,8) + ns(dptp,2) + sin(timevar * 2 * pi) + cos(timevar * 2 * pi) + ns(time, df = numYears), data = df, family = poisson)
  # drop1(fit, test= 'Chisq')
  # # use plot.glm for diagnostics
  # par(mfrow=c(2,2))
  # plot(fit)
  # par(mfrow=c(3,2))
  # termplot(fit, se=T)
  # dev.off()
   
  # # cyclic spline, overlay on prior sinusoidal
  # with(df, plot(date,cvd,type = 'l',col='grey', ylim = c(0,55)))
  # lines(df[,'date'],exp(yhat),col='red')
   
  # df$daynum <- as.numeric(format(df$date, "%j"))
  # df[360:370,c('date','daynum')]
  # fit <- gam(cvd ~ s(daynum, k=3, fx=T, bs = 'cp') +  s(time, k = numYears, fx = T), data = df, family = poisson)
  # yhat2 <- predict(fit)
  # head(yhat2)
   
  # lines(df[,'date'],exp(yhat2),col='blue')
   
   
  # par(mfrow=c(1,2))
  # plot(fit)
   
   
  # # fit weather with season
  # fit <- gam(cvd ~ s(tmax) + s(dptp) +
    # s(daynum, k=3, fx=T, bs = 'cp') +  s(time, k = numYears, fx = T), data = df, family = poisson)
  # par(mfrow=c(2,2))
  # plot(fit)
   
  # summary(fit)
    
#+end_src

** COMMENT SCP files across
#+name:scp-files
#+begin_src sh :session *shell* :tangle no :exports none :eval no
cd images
scp ivan_hanigan@130.56.102.53:projects/spatiotemporal-regression-models/NMMAPS-example/images/nmmaps-eg-core.png nmmaps-eg-core.png
cd ..
cd NMMAPS-example
scp ivan_hanigan@130.56.102.53:projects/spatiotemporal-regression-models/NMMAPS-example/analyte.csv analyte.csv
cd ..
#+end_src
* Case Study 2: Spatial Lag and Timeseries with NMMAPS
I will use the same NMMAPSlite to show how I'd approach a simple "Spatio-Temporal" model.
**** TODO The following is a stub of an idea.  For further development
** Data
*** Zones
**** Spatial Neighbours Code
#+name:spatwat
#+begin_src R :session *shell* :tangle NMMAPS-example/case-study-2-load.r :exports reports :eval no
  ################################################################
  # name:spatwat
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
  require(gisviz)
  
  # load
  dir()
  shp <- readOGR("cities.shp", "cities")
  
  # clean
  head(shp@data)
  
  # do
  ## I will use nearest neighbour within a distance threshold, but
  ## usually polygon datasets would use poly2nb
  nb <- dnearneigh(shp, d1 = 1, d2 = 1000)                      
  head(nb)
  shp[[1]][1]
  shp[[1]][nb[[1]]]
  # map
  nudge <- 10
  png("images/nmmaps-eg-neighbourhood.png")
  plotMyMap(
      shp@coords, xl = c(min(shp@coords[,1])-nudge, max(shp@coords[,1])+nudge),
      yl = c(min(shp@coords[,2])-nudge, max(shp@coords[,2])+nudge)
      )
  plot(nb, shp@coords, add=TRUE)
  text(shp@data$long, shp@data$lat, shp@data$address, pos=3)
  points(shp@coords[nb[[1]],], col = 'green', pch = 16)
  points(shp@data[1,c("long","lat")],col = 'blue', pch = 16)
  dev.off()
  
  
#+end_src

**** Spatial Neighbours Output
[[file:NMMAPS-example/images/nmmaps-eg-neighbourhood.png]]
*** Outcome
#+name:load-suicide
#+begin_src R :session *R* :tangle no :exports reports :eval no
  # func
  setwd("~/projects/spatiotemporal-regression-models/NMMAPS-example")
  require(gisviz)
  require(plyr)
  
  # load
  analyte <- read.csv("analyte.csv")
  shp <- readOGR("cities.shp", "cities")
  
  # clean
  analyte$yy <- substr(analyte$date,1,4)
  numYears<-length(names(table(analyte$yy)))
  analyte$date <- as.Date(analyte$date)
  analyte$time <- as.numeric(analyte$date)
  analyte$agecat <- factor(analyte$agecat,
                            levels = c("under65",
                                "65to74", "75p"),
                            ordered = TRUE
                            )
   
  names(analyte)
  analyte[1,]
  table(analyte$city)
  # do
  study <- analyte[, c("city","date", "agecat", "cvd", "pop")]
  subset(study, city == "tucs" & date == as.Date("1991-01-01"))
  
  nb <- dnearneigh(shp, d1 = 1, d2 = 1000)                      
  head(nb)
  head(shp@data)
  adj <- adjacency_df(NB = nb, shp = shp, zone_id = 'city')
  subset(adj, V1 == "tucs")
  
  neighbours <- merge(study, adj, by.x = "city", by.y = "V1")
  subset(neighbours, city == "tucs" & date == as.Date("1991-01-01"))
  
  xvars <- c("V2", "date","agecat")
  yvars <- c("city", "date", "agecat")
  neighbours <- merge(neighbours[,c(xvars, "city")],
                      analyte[,c(yvars, "cvd", "pop")],
                      by.x = xvars,
                      by.y = yvars)
  names(neighbours)
  subset(neighbours, city == "tucs" & date == as.Date("1991-01-01"))
  
  neighbours$asr  <- (neighbours$cvd / neighbours$pop) * 1000
  
  
  neighbours2  <- ddply(neighbours, c("city", "date", "agecat"), summarise,
                       NeighboursYij  = mean(asr)
                       )
  subset(neighbours2, city == "tucs" & date == as.Date("1991-01-01"))
  table(neighbours2$city)
  head(analyte)
  
  analyte  <- merge(analyte, neighbours2, by = c("city", "agecat", "date"))
  analyte <- arrange(analyte, city, date, agecat)
  head(analyte)
  
  
#+end_src


** Analysis
*** Model with Spatial Lag
#+begin_src R :session *R* :tangle no :exports reports :eval no
  # func
  require(splines)
  
  # load
  # assumes the prior code chunks have been run
  
  # do
  fit <- glm(cvd ~ ns(tmax, df = 3) + ns(dptp, df = 3) +
             NeighboursYij + 
             city + agecat +
             ns(time, df = 7*numYears) +
             offset(log(pop)),
             data = analyte, family = poisson
             )
  
  # plot of response functions
  png("images/nmmaps-eg-sp-lag.png", width = 1000, height = 750, res = 150)
  par(mfrow=c(2,3))
  termplot(fit, terms = attr(terms(fit),'term.labels') [c(1:2,6)], se = TRUE, ylim =c(-.2,.2), col.term = 'black', col.se = 'black')
  termplot(fit, terms = attr(terms(fit),'term.labels') [c(4,5)], se = TRUE, ylim =c(-3,1), col.term = 'black', col.se = 'black')
  termplot(fit, terms = attr(terms(fit),'term.labels') [3], se = TRUE, ylim =c(-1,1), col.term = 'black', col.se = 'black')
  dev.off()
  
  
#+end_src
*** Spatial Lag Model Plots
[[file:NMMAPS-example/images/nmmaps-eg-sp-lag.png]]

